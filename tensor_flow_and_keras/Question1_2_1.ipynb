{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question1_2_1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMjNmQPEaNc+TphlhkA/dQB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fkXNzQO1D-cH","executionInfo":{"status":"ok","timestamp":1618765736868,"user_tz":-60,"elapsed":440562,"user":{"displayName":"Shane Quinn","photoUrl":"","userId":"04299873785165069898"}},"outputId":"9e64b196-f606-4b15-8e24-6804f122fd85"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Name: Shane Quinn\n","Student Number: R00144107\n","Email: shane.quinn1@mycit.ie\n","Course: MSc Artificial Intelligence\n","Module: Deep Learning\n","Date: 03/04/2021\n","\"\"\"\n","\n","import tensorflow as tf\n","from keras.utils import np_utils\n","from tensorflow.python.client import device_lib\n","import matplotlib.pyplot as plt\n","import functools\n","import time\n","import numpy as np\n","\n","\n","def forward_pass(X, w1, w2, w3, b1, b2, b3, dop=0.1):\n","    \"\"\"\n","    Push feature data through neural network. Returns 10 class probabilities for all feature instances\n","\n","    Parameters\n","    ----------\n","    X : tf.Variable\n","        Pre-processed input data.\n","    w1 : tf.Variable\n","        Layer 1 learnable weights.\n","    w2 : tf.Variable\n","        Layer 2 learnable weights.\n","    w3 : tf.Variable\n","        Layer 3 learnable weights.\n","    b1 : tf.Variable\n","        Layer 1 bias.\n","    b2 : tf.Variable\n","        Layer 2 bias.\n","    b3 : tf.Variable\n","        Layer 3 bias.\n","    dop : Float32, optional\n","        Drop out probability, probability of each neuron to be dropped. The default is 0.1.\n","\n","    Returns\n","    -------\n","    H : tf.Variable\n","        Softmax layer output predicted probability of each class.\n","    \"\"\"\n","  \n","    #Layer 1- 300 Relu Neruons\n","    A = tf.matmul(w1, tf.transpose(X)) + b1                     #A1 = w1.X + b1\n","    H = tf.keras.activations.relu(A)                            #H1 = act(A1)\n","    #Layer 2 - Dropout Layer\n","    datatype = w1.dtype                                         #Get datatype used (float32/64)\n","    probThres = 1-dop                                           #Threshold of neurons to be dropped\n","    #Create matrix of shape H with boolean 1s in probThresh locations, and 0s in dop locations\n","    dropMat = np.random.rand(H.shape[0], H.shape[1]) < probThres \n","    #Cast to tf variable for interacting with H\n","    dropMat = tf.cast(dropMat, datatype)\n","    #Elementwise multiply mask matrix by H, 'dropping' neurons\n","    dropResult = tf.math.multiply(H, dropMat)  \n","    #Scale up values in H to compensate for dropped values\n","    H = tf.math.divide(dropResult, probThres)    \n","    #Layer 3 - 100 Relu Neurons\n","    A = tf.matmul(w2, H) + b2                                   #A2 = w2.H1 + b2\n","    H = tf.keras.activations.relu(A)                            #H2 = act(A2)\n","    #Layer 4 - Softmax = (e^A)/sum(A)\n","    A = tf.matmul(w3, H)+b3                                     #A3 = w3.H2 + b3\n","    H = tf.exp(A) / tf.reduce_sum(tf.exp(A), axis=0)            #H3 = act(A3)\n","    \n","    return H\n","    \n","\n","def cross_entropy(pred_y, y):\n","    \"\"\"\n","    Take in softmax probabilities (output of forward_pass) and true class labels and calculate cross entropy loss\n","\n","    Parameters\n","    ----------\n","    pred_y : tf.Variable\n","        Predictions (Output of softmax layer in forward_pass()).\n","    y : tf.Variable\n","        One-hot encoded true class labels.\n","\n","    Returns\n","    -------\n","    cross_ent : Cross entropy loss\n","        tf.Variable.\n","    \"\"\"\n","        \n","    #Cross entropy loss per class = -sum((True class encoded values)*log(predicted probabilities))\n","    a = -tf.reduce_sum(y * tf.math.log(pred_y), axis=0)\n","    #Cross entropy loss = mean of all losses calculated above.\n","    cross_ent = tf.reduce_mean(a, axis=0)\n","\n","    return cross_ent\n","\n","\n","def calculate_accuracy(pred_y, y):\n","    \"\"\"\n","    Calculate the model accuracy given predicted probabilities and true class labels\n","\n","    Parameters\n","    ----------\n","    pred_y : tf.Variable\n","        Predicted class probabilities, output of forward pass/softmax layer.\n","    y : tf.Variable\n","        True class values.\n","    datatype : tf.float32/tf.float64, optional\n","        One of the above tf datatypes. The default is tf.float32.\n","\n","    Returns\n","    -------\n","    accuracy : float32\n","        Model Accuracy.\n","    \"\"\"\n","\n","    # Convert predicted probabilities to 0 or 1\n","    pred_y = tf.round(pred_y)\n","    # Boolean True (1) if prediction is correct\n","    predictions = tf.cast(tf.equal(pred_y, y), tf.float32)\n","    #Mean value of correct predictions\n","    accuracy = tf.reduce_mean(predictions)\n","    \n","    return accuracy\n","\n","\n","def exec_time(func):\n","    \"\"\"\n","    Generic Execution time recorder, pass in function. Records execution time using decorators\n","\n","    Parameters\n","    ----------\n","    func : FUNCTION\n","        Function .\n","    \"\"\"\n","  \n","    @functools.wraps(func)\n","    def record_exec_time(*args, **kwargs):\n","        start_time = time.perf_counter()\n","        mn = func(*args, **kwargs)\n","        execution_time = time.perf_counter() - start_time\n","        print(\"Execution Time: \", execution_time)\n","        return mn\n","\n","    return record_exec_time\n","\n","@exec_time \n","def main():\n","      \n","    X, y, X_val, y_val = pre_process() \n","\n","    #Initialise Learning rate and iterations.\n","    learning_rate = 0.05\n","    iterations = 1000\n","    datatype = tf.float64\n","    dop = 0.75              #Drop out probability\n","    \n","    #Initialise lists for saving accuracies/loss\n","    te_acc = []\n","    tr_acc = []\n","    te_loss = []\n","    tr_loss = []\n","    \n","    # Create tf variables from data\n","    X = tf.cast(X, datatype)\n","    y = tf.cast(y, datatype)\n","    X_val = tf.cast(X_val, datatype)\n","    y_val = tf.cast(y_val, datatype)\n","    \n","    #Initialise Adam Optimizer\n","    adam = tf.keras.optimizers.Adam()\n","    \n","    #Initialise weights and bias\n","    zeros = tf.zeros_initializer()\n","    layer1_weights = tf.Variable(tf.random.normal([300,784], stddev=0.05, dtype=datatype))\n","    layer2_weights = tf.Variable(tf.random.normal([100, 300], stddev=0.05, dtype=datatype))\n","    layer3_weights = tf.Variable(tf.random.normal([10, 100], stddev=0.05, dtype=datatype))\n","    layer1_bias = tf.Variable(0, dtype=datatype)\n","    layer2_bias = tf.Variable(0, dtype=datatype)\n","    layer3_bias = tf.Variable(0, dtype=datatype)\n","    \n","    #Repeat gradient descent loop 'iterations' times\n","    for i in range(iterations):\n","        \n","        with tf.GradientTape() as tape:\n","            #Create instance of gradient tape to record forward pass and calculate gradients for learnable weights and biases\n","            pred_y = forward_pass(X, layer1_weights, layer2_weights, layer3_weights, layer1_bias, layer2_bias, layer3_bias, dop)\n","            loss = cross_entropy(pred_y, y)\n","            \n","        tr_loss.append(loss)\n","        #Calculate Gradients using gradient tape\n","        gradients = tape.gradient(loss, [layer1_weights, layer2_weights, layer3_weights, layer1_bias, layer2_bias, layer3_bias])\n","        accuracy = calculate_accuracy(pred_y, y)\n","        tr_acc.append(accuracy)\n","        print(\"Iteration {}: Training Loss = {} Training Accuracy = {}\".format(i, loss.numpy(), accuracy.numpy()))\n","        \n","        #Apply gradients using adaptive movement estimation, see accompanied report for more details\n","        adam.apply_gradients(zip(gradients, [layer1_weights, layer2_weights, layer3_weights, layer1_bias, layer2_bias, layer3_bias]))\n","        \n","        #Test model on validation data\n","        test_pred_y = forward_pass(X_val, layer1_weights, layer2_weights, layer3_weights, layer1_bias, layer2_bias, layer3_bias, dop)\n","        test_loss = cross_entropy(test_pred_y, y_val)\n","        te_loss.append(test_loss)\n","        te_acc.append(calculate_accuracy(test_pred_y, y_val))\n","    \n","    print(\"Dropout Probability: \", dop)\n","    plt.title(\"Question 1_2_1\")\n","    plt.plot(te_loss, label=\"Validation Loss\")\n","    plt.plot(tr_loss, label=\"Train Loss\")\n","    plt.plot(te_acc, label=\"Validation Accuracy\")\n","    plt.plot(tr_acc, label=\"Train Accuracy\")\n","    plt.ylim((0,1))\n","    plt.legend()\n","    plt.show()\n","    \n","\n","def pre_process():\n","    \"\"\"\n","    Supplied Code for pre-processing Fashion MNIST dataset. Returns target class values and training data for training and validation\n","\n","    Returns\n","    -------\n","    tr_x : NUMPY N-D ARRAY\n","        X Training Data.\n","    tr_y : NUMPY N-D ARRAY\n","        y target class values 1 hot encoded (training data).\n","    te_x : NUMPY N-D ARRAY\n","        X Test Data.\n","    te_y : NUMPY N-D ARRAY\n","        y test target class values 1 hot encoded (test data).\n","\n","    \"\"\"\n","    \n","    fashion_mnist = tf.keras.datasets.fashion_mnist \n","    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n","    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n","    te_x = te_x.reshape(te_x.shape[0], 784)\n","    tr_x = tr_x / 255.0\n","    te_x = te_x / 255.0\n","    tr_y = np_utils.to_categorical(tr_y,10)\n","    tr_y = tr_y.T\n","    te_y = np_utils.to_categorical(te_y,10)\n","    te_y = te_y.T\n","\n","    return tr_x, tr_y, te_x, te_y\n","\n","\n","if __name__ == '__main__':\n","    print(\"Local Devices: \\n\", device_lib.list_local_devices())\n","    main()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Local Devices: \n"," [name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 15000846141296553024\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 14674281152\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 3121463717337573242\n","physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n","]\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","Iteration 0: Training Loss = 2.3244133819816377 Training Accuracy = 0.8999983072280884\n","Iteration 1: Training Loss = 2.2237532755887464 Training Accuracy = 0.8999999761581421\n","Iteration 2: Training Loss = 2.1428759197532585 Training Accuracy = 0.8999999761581421\n","Iteration 3: Training Loss = 2.0628233387295873 Training Accuracy = 0.8999999761581421\n","Iteration 4: Training Loss = 1.98051967469396 Training Accuracy = 0.9000633358955383\n","Iteration 5: Training Loss = 1.8956188470260793 Training Accuracy = 0.900303304195404\n","Iteration 6: Training Loss = 1.8063274428764684 Training Accuracy = 0.9011533260345459\n","Iteration 7: Training Loss = 1.7183750531135369 Training Accuracy = 0.9027000069618225\n","Iteration 8: Training Loss = 1.6240290282285035 Training Accuracy = 0.904936671257019\n","Iteration 9: Training Loss = 1.5328873780102357 Training Accuracy = 0.9078599810600281\n","Iteration 10: Training Loss = 1.4515517963877287 Training Accuracy = 0.9111466407775879\n","Iteration 11: Training Loss = 1.377509688479144 Training Accuracy = 0.9145633578300476\n","Iteration 12: Training Loss = 1.3118306811480778 Training Accuracy = 0.9175783395767212\n","Iteration 13: Training Loss = 1.2438015877299806 Training Accuracy = 0.9203649759292603\n","Iteration 14: Training Loss = 1.19125461532482 Training Accuracy = 0.9225416779518127\n","Iteration 15: Training Loss = 1.1406291184834407 Training Accuracy = 0.9244199991226196\n","Iteration 16: Training Loss = 1.0977278452879111 Training Accuracy = 0.9259833097457886\n","Iteration 17: Training Loss = 1.0615043205558303 Training Accuracy = 0.927798330783844\n","Iteration 18: Training Loss = 1.0253748693120972 Training Accuracy = 0.9292600154876709\n","Iteration 19: Training Loss = 1.0007114092436642 Training Accuracy = 0.9308800101280212\n","Iteration 20: Training Loss = 0.9730545501668656 Training Accuracy = 0.9320233464241028\n","Iteration 21: Training Loss = 0.9485352556664272 Training Accuracy = 0.9333133101463318\n","Iteration 22: Training Loss = 0.9219604058991798 Training Accuracy = 0.9346266388893127\n","Iteration 23: Training Loss = 0.9033251841555573 Training Accuracy = 0.9362199902534485\n","Iteration 24: Training Loss = 0.883740145099931 Training Accuracy = 0.9371633529663086\n","Iteration 25: Training Loss = 0.8649980381893644 Training Accuracy = 0.9387483596801758\n","Iteration 26: Training Loss = 0.8461585896247711 Training Accuracy = 0.939454972743988\n","Iteration 27: Training Loss = 0.8328474453276052 Training Accuracy = 0.9405633211135864\n","Iteration 28: Training Loss = 0.8223634619713315 Training Accuracy = 0.9413666725158691\n","Iteration 29: Training Loss = 0.8057118663451404 Training Accuracy = 0.9423566460609436\n","Iteration 30: Training Loss = 0.7924532114822768 Training Accuracy = 0.943008303642273\n","Iteration 31: Training Loss = 0.7810089693895803 Training Accuracy = 0.9442333579063416\n","Iteration 32: Training Loss = 0.7703452771598054 Training Accuracy = 0.944694995880127\n","Iteration 33: Training Loss = 0.7568529686244726 Training Accuracy = 0.9456316828727722\n","Iteration 34: Training Loss = 0.7476687521825045 Training Accuracy = 0.9465033411979675\n","Iteration 35: Training Loss = 0.7398943060195935 Training Accuracy = 0.9470016956329346\n","Iteration 36: Training Loss = 0.7299240374852117 Training Accuracy = 0.9480400085449219\n","Iteration 37: Training Loss = 0.7213857246146043 Training Accuracy = 0.9485250115394592\n","Iteration 38: Training Loss = 0.7122792600228034 Training Accuracy = 0.9491733312606812\n","Iteration 39: Training Loss = 0.7025623945070189 Training Accuracy = 0.9498066902160645\n","Iteration 40: Training Loss = 0.6941954400994162 Training Accuracy = 0.9508900046348572\n","Iteration 41: Training Loss = 0.6857478875588512 Training Accuracy = 0.951533317565918\n","Iteration 42: Training Loss = 0.6805157195897449 Training Accuracy = 0.9519433379173279\n","Iteration 43: Training Loss = 0.6721843143512625 Training Accuracy = 0.9524366855621338\n","Iteration 44: Training Loss = 0.6636952786591057 Training Accuracy = 0.9532033205032349\n","Iteration 45: Training Loss = 0.6540679908764667 Training Accuracy = 0.954021692276001\n","Iteration 46: Training Loss = 0.6471098583513458 Training Accuracy = 0.9544550180435181\n","Iteration 47: Training Loss = 0.6442946410183783 Training Accuracy = 0.9550250172615051\n","Iteration 48: Training Loss = 0.6370928149220527 Training Accuracy = 0.9551833271980286\n","Iteration 49: Training Loss = 0.6307311015552702 Training Accuracy = 0.9558449983596802\n","Iteration 50: Training Loss = 0.624247227651735 Training Accuracy = 0.9560816884040833\n","Iteration 51: Training Loss = 0.6167394593844988 Training Accuracy = 0.9567483067512512\n","Iteration 52: Training Loss = 0.6104803717091538 Training Accuracy = 0.9574316740036011\n","Iteration 53: Training Loss = 0.6069648669555033 Training Accuracy = 0.9577850103378296\n","Iteration 54: Training Loss = 0.5991502212675014 Training Accuracy = 0.9582816958427429\n","Iteration 55: Training Loss = 0.5981469755507021 Training Accuracy = 0.9584866762161255\n","Iteration 56: Training Loss = 0.5878029649457657 Training Accuracy = 0.9590766429901123\n","Iteration 57: Training Loss = 0.5846190387143698 Training Accuracy = 0.9597650170326233\n","Iteration 58: Training Loss = 0.581539969924977 Training Accuracy = 0.959713339805603\n","Iteration 59: Training Loss = 0.5776112135231022 Training Accuracy = 0.9599916934967041\n","Iteration 60: Training Loss = 0.5726180660525153 Training Accuracy = 0.9603449702262878\n","Iteration 61: Training Loss = 0.5689125180316774 Training Accuracy = 0.9608983397483826\n","Iteration 62: Training Loss = 0.562996915363908 Training Accuracy = 0.9613800048828125\n","Iteration 63: Training Loss = 0.5597965621445238 Training Accuracy = 0.9613950252532959\n","Iteration 64: Training Loss = 0.5531438457510802 Training Accuracy = 0.961965024471283\n","Iteration 65: Training Loss = 0.5503060721378007 Training Accuracy = 0.9620749950408936\n","Iteration 66: Training Loss = 0.5501131396644158 Training Accuracy = 0.9623500108718872\n","Iteration 67: Training Loss = 0.5409335164354463 Training Accuracy = 0.9629300236701965\n","Iteration 68: Training Loss = 0.540259159983099 Training Accuracy = 0.9629666805267334\n","Iteration 69: Training Loss = 0.5380497106664226 Training Accuracy = 0.962933361530304\n","Iteration 70: Training Loss = 0.5325354492506775 Training Accuracy = 0.9634416699409485\n","Iteration 71: Training Loss = 0.5286613447919953 Training Accuracy = 0.9635266661643982\n","Iteration 72: Training Loss = 0.5272088644275106 Training Accuracy = 0.9640216827392578\n","Iteration 73: Training Loss = 0.5221064152501151 Training Accuracy = 0.9637633562088013\n","Iteration 74: Training Loss = 0.5228516519575572 Training Accuracy = 0.9644166827201843\n","Iteration 75: Training Loss = 0.5172457676783102 Training Accuracy = 0.9645466804504395\n","Iteration 76: Training Loss = 0.5145980113506089 Training Accuracy = 0.9647716879844666\n","Iteration 77: Training Loss = 0.5117308494899472 Training Accuracy = 0.96500164270401\n","Iteration 78: Training Loss = 0.508346912432657 Training Accuracy = 0.9653416872024536\n","Iteration 79: Training Loss = 0.5095091989039545 Training Accuracy = 0.9651650190353394\n","Iteration 80: Training Loss = 0.503355956915676 Training Accuracy = 0.9653266668319702\n","Iteration 81: Training Loss = 0.5003078162972393 Training Accuracy = 0.965624988079071\n","Iteration 82: Training Loss = 0.49896382160264935 Training Accuracy = 0.9659166932106018\n","Iteration 83: Training Loss = 0.49769990727248964 Training Accuracy = 0.9658650159835815\n","Iteration 84: Training Loss = 0.4929069894129399 Training Accuracy = 0.9661933183670044\n","Iteration 85: Training Loss = 0.49213730626673874 Training Accuracy = 0.9663450121879578\n","Iteration 86: Training Loss = 0.4915313042987176 Training Accuracy = 0.966646671295166\n","Iteration 87: Training Loss = 0.48623729908559793 Training Accuracy = 0.9666866660118103\n","Iteration 88: Training Loss = 0.4865187873687398 Training Accuracy = 0.9667083621025085\n","Iteration 89: Training Loss = 0.4835069611444514 Training Accuracy = 0.9669066667556763\n","Iteration 90: Training Loss = 0.480614503950169 Training Accuracy = 0.9669666886329651\n","Iteration 91: Training Loss = 0.47996864410532275 Training Accuracy = 0.9671499729156494\n","Iteration 92: Training Loss = 0.4792158709228359 Training Accuracy = 0.9675266742706299\n","Iteration 93: Training Loss = 0.47545595467556806 Training Accuracy = 0.9673516750335693\n","Iteration 94: Training Loss = 0.4748932366864358 Training Accuracy = 0.9677366614341736\n","Iteration 95: Training Loss = 0.47223034624126725 Training Accuracy = 0.9673133492469788\n","Iteration 96: Training Loss = 0.47122440144692795 Training Accuracy = 0.9679033160209656\n","Iteration 97: Training Loss = 0.46918409469149375 Training Accuracy = 0.9679316878318787\n","Iteration 98: Training Loss = 0.4651255605643602 Training Accuracy = 0.9680650234222412\n","Iteration 99: Training Loss = 0.4664368358107173 Training Accuracy = 0.9679399728775024\n","Iteration 100: Training Loss = 0.46299861402320946 Training Accuracy = 0.9682583212852478\n","Iteration 101: Training Loss = 0.45977474491368875 Training Accuracy = 0.9684249758720398\n","Iteration 102: Training Loss = 0.46125937170253106 Training Accuracy = 0.9684266448020935\n","Iteration 103: Training Loss = 0.45831522635718003 Training Accuracy = 0.9683766961097717\n","Iteration 104: Training Loss = 0.45637743064005487 Training Accuracy = 0.9686449766159058\n","Iteration 105: Training Loss = 0.45503854617465095 Training Accuracy = 0.9687749743461609\n","Iteration 106: Training Loss = 0.4538297060776087 Training Accuracy = 0.9689466953277588\n","Iteration 107: Training Loss = 0.4523646983638673 Training Accuracy = 0.9689683318138123\n","Iteration 108: Training Loss = 0.45211218903685924 Training Accuracy = 0.9693283438682556\n","Iteration 109: Training Loss = 0.4506623824606427 Training Accuracy = 0.9692566394805908\n","Iteration 110: Training Loss = 0.44833387546859677 Training Accuracy = 0.9694316387176514\n","Iteration 111: Training Loss = 0.44640310651519505 Training Accuracy = 0.9695450067520142\n","Iteration 112: Training Loss = 0.44292118258456975 Training Accuracy = 0.969635009765625\n","Iteration 113: Training Loss = 0.44426458579655287 Training Accuracy = 0.9694899916648865\n","Iteration 114: Training Loss = 0.4437510034484175 Training Accuracy = 0.969676673412323\n","Iteration 115: Training Loss = 0.4424254522182346 Training Accuracy = 0.969653308391571\n","Iteration 116: Training Loss = 0.4392322636779768 Training Accuracy = 0.969789981842041\n","Iteration 117: Training Loss = 0.4392785545321529 Training Accuracy = 0.9697816371917725\n","Iteration 118: Training Loss = 0.43667286078818707 Training Accuracy = 0.9698916673660278\n","Iteration 119: Training Loss = 0.4385868292987199 Training Accuracy = 0.9702200293540955\n","Iteration 120: Training Loss = 0.43443155270972617 Training Accuracy = 0.970104992389679\n","Iteration 121: Training Loss = 0.43248111117184435 Training Accuracy = 0.9706000089645386\n","Iteration 122: Training Loss = 0.4359893897335501 Training Accuracy = 0.9701216816902161\n","Iteration 123: Training Loss = 0.43335106018161124 Training Accuracy = 0.9702033400535583\n","Iteration 124: Training Loss = 0.4291815288400428 Training Accuracy = 0.9705849885940552\n","Iteration 125: Training Loss = 0.4277950101926645 Training Accuracy = 0.9705950021743774\n","Iteration 126: Training Loss = 0.42919682349494503 Training Accuracy = 0.970478355884552\n","Iteration 127: Training Loss = 0.4288079441178824 Training Accuracy = 0.9706483483314514\n","Iteration 128: Training Loss = 0.4269557018323404 Training Accuracy = 0.9706916809082031\n","Iteration 129: Training Loss = 0.42400776650522787 Training Accuracy = 0.9711133241653442\n","Iteration 130: Training Loss = 0.4252657963565981 Training Accuracy = 0.9707949757575989\n","Iteration 131: Training Loss = 0.42500005187704204 Training Accuracy = 0.9709016680717468\n","Iteration 132: Training Loss = 0.4221721450395523 Training Accuracy = 0.9710133075714111\n","Iteration 133: Training Loss = 0.4238507575723675 Training Accuracy = 0.9708983302116394\n","Iteration 134: Training Loss = 0.4179088580030814 Training Accuracy = 0.9714366793632507\n","Iteration 135: Training Loss = 0.4209649191916024 Training Accuracy = 0.9711116552352905\n","Iteration 136: Training Loss = 0.4182736828998146 Training Accuracy = 0.9711300134658813\n","Iteration 137: Training Loss = 0.4177437814002168 Training Accuracy = 0.9714683294296265\n","Iteration 138: Training Loss = 0.41875476336426026 Training Accuracy = 0.9712916612625122\n","Iteration 139: Training Loss = 0.41618335435950304 Training Accuracy = 0.9714366793632507\n","Iteration 140: Training Loss = 0.41493788473609966 Training Accuracy = 0.971548318862915\n","Iteration 141: Training Loss = 0.4141958991503526 Training Accuracy = 0.9716283082962036\n","Iteration 142: Training Loss = 0.4136737453568335 Training Accuracy = 0.9715883135795593\n","Iteration 143: Training Loss = 0.41172386101041875 Training Accuracy = 0.9718266725540161\n","Iteration 144: Training Loss = 0.410499478621379 Training Accuracy = 0.9719949960708618\n","Iteration 145: Training Loss = 0.41041462981818283 Training Accuracy = 0.9719316959381104\n","Iteration 146: Training Loss = 0.4076599004214 Training Accuracy = 0.9719600081443787\n","Iteration 147: Training Loss = 0.40979502284204034 Training Accuracy = 0.9717766642570496\n","Iteration 148: Training Loss = 0.40786094187155764 Training Accuracy = 0.9719933271408081\n","Iteration 149: Training Loss = 0.40728350151871684 Training Accuracy = 0.9720683097839355\n","Iteration 150: Training Loss = 0.4061464136950949 Training Accuracy = 0.9723299741744995\n","Iteration 151: Training Loss = 0.40382074710903504 Training Accuracy = 0.9720500111579895\n","Iteration 152: Training Loss = 0.4068660869859228 Training Accuracy = 0.9719833135604858\n","Iteration 153: Training Loss = 0.4040721995481879 Training Accuracy = 0.9723349809646606\n","Iteration 154: Training Loss = 0.40435777218558194 Training Accuracy = 0.9722283482551575\n","Iteration 155: Training Loss = 0.40355889839580583 Training Accuracy = 0.9720649719238281\n","Iteration 156: Training Loss = 0.4022904865887072 Training Accuracy = 0.9722833037376404\n","Iteration 157: Training Loss = 0.40028754178982345 Training Accuracy = 0.9726250171661377\n","Iteration 158: Training Loss = 0.40124114636241975 Training Accuracy = 0.9723433256149292\n","Iteration 159: Training Loss = 0.3985019811398706 Training Accuracy = 0.9726866483688354\n","Iteration 160: Training Loss = 0.39586702694738546 Training Accuracy = 0.9730700254440308\n","Iteration 161: Training Loss = 0.39920054248278597 Training Accuracy = 0.9724933505058289\n","Iteration 162: Training Loss = 0.39655938321272305 Training Accuracy = 0.9728166460990906\n","Iteration 163: Training Loss = 0.39591704727989746 Training Accuracy = 0.9728083610534668\n","Iteration 164: Training Loss = 0.39703274666082955 Training Accuracy = 0.97270667552948\n","Iteration 165: Training Loss = 0.39576480339585896 Training Accuracy = 0.9728516936302185\n","Iteration 166: Training Loss = 0.39471145834812693 Training Accuracy = 0.9731249809265137\n","Iteration 167: Training Loss = 0.39401907933033775 Training Accuracy = 0.9724966883659363\n","Iteration 168: Training Loss = 0.3932533956210624 Training Accuracy = 0.9728816747665405\n","Iteration 169: Training Loss = 0.39283263986055206 Training Accuracy = 0.9728383421897888\n","Iteration 170: Training Loss = 0.39155429618018883 Training Accuracy = 0.97298663854599\n","Iteration 171: Training Loss = 0.39179523402828137 Training Accuracy = 0.9729716777801514\n","Iteration 172: Training Loss = 0.3913099248885405 Training Accuracy = 0.9729499816894531\n","Iteration 173: Training Loss = 0.38843612525142024 Training Accuracy = 0.9733350276947021\n","Iteration 174: Training Loss = 0.3898364771827299 Training Accuracy = 0.9731366634368896\n","Iteration 175: Training Loss = 0.3883794631237031 Training Accuracy = 0.9731749892234802\n","Iteration 176: Training Loss = 0.3878179077514002 Training Accuracy = 0.9734816551208496\n","Iteration 177: Training Loss = 0.38553736783043047 Training Accuracy = 0.9735766649246216\n","Iteration 178: Training Loss = 0.3881017141599318 Training Accuracy = 0.973496675491333\n","Iteration 179: Training Loss = 0.3861948460928911 Training Accuracy = 0.9734333157539368\n","Iteration 180: Training Loss = 0.38425105389038383 Training Accuracy = 0.9733899831771851\n","Iteration 181: Training Loss = 0.38391504674556914 Training Accuracy = 0.9735749959945679\n","Iteration 182: Training Loss = 0.3818844061385791 Training Accuracy = 0.973663330078125\n","Iteration 183: Training Loss = 0.38121598682009494 Training Accuracy = 0.9736833572387695\n","Iteration 184: Training Loss = 0.3801430069010865 Training Accuracy = 0.9739199876785278\n","Iteration 185: Training Loss = 0.383592470388003 Training Accuracy = 0.9737200140953064\n","Iteration 186: Training Loss = 0.3815207953841112 Training Accuracy = 0.9736049771308899\n","Iteration 187: Training Loss = 0.37968581450712263 Training Accuracy = 0.9739716649055481\n","Iteration 188: Training Loss = 0.37876907220032374 Training Accuracy = 0.9737033247947693\n","Iteration 189: Training Loss = 0.3783732170261092 Training Accuracy = 0.9738083481788635\n","Iteration 190: Training Loss = 0.38201236524767446 Training Accuracy = 0.9736899733543396\n","Iteration 191: Training Loss = 0.37802038113816305 Training Accuracy = 0.9738516807556152\n","Iteration 192: Training Loss = 0.37830953532475564 Training Accuracy = 0.9738649725914001\n","Iteration 193: Training Loss = 0.37626730689612203 Training Accuracy = 0.9740983247756958\n","Iteration 194: Training Loss = 0.37805066885020416 Training Accuracy = 0.9738550186157227\n","Iteration 195: Training Loss = 0.3765106683825446 Training Accuracy = 0.9743066430091858\n","Iteration 196: Training Loss = 0.377031848382745 Training Accuracy = 0.973871648311615\n","Iteration 197: Training Loss = 0.3762390213628716 Training Accuracy = 0.9739750027656555\n","Iteration 198: Training Loss = 0.37478576404873165 Training Accuracy = 0.9742616415023804\n","Iteration 199: Training Loss = 0.37519143444810116 Training Accuracy = 0.9741466641426086\n","Iteration 200: Training Loss = 0.37318763751526934 Training Accuracy = 0.974328339099884\n","Iteration 201: Training Loss = 0.3736976916680901 Training Accuracy = 0.9744899868965149\n","Iteration 202: Training Loss = 0.37203063927066554 Training Accuracy = 0.9744600057601929\n","Iteration 203: Training Loss = 0.37347843012694104 Training Accuracy = 0.9742699861526489\n","Iteration 204: Training Loss = 0.37058298817899643 Training Accuracy = 0.9742533564567566\n","Iteration 205: Training Loss = 0.373130172888029 Training Accuracy = 0.9743316769599915\n","Iteration 206: Training Loss = 0.37194246677771303 Training Accuracy = 0.9743250012397766\n","Iteration 207: Training Loss = 0.3705084752687315 Training Accuracy = 0.9745566844940186\n","Iteration 208: Training Loss = 0.3684101166688296 Training Accuracy = 0.9745933413505554\n","Iteration 209: Training Loss = 0.3698872706480105 Training Accuracy = 0.9744916558265686\n","Iteration 210: Training Loss = 0.3678862020954791 Training Accuracy = 0.9745466709136963\n","Iteration 211: Training Loss = 0.36859473502814255 Training Accuracy = 0.9748299717903137\n","Iteration 212: Training Loss = 0.36682842742751326 Training Accuracy = 0.9742950201034546\n","Iteration 213: Training Loss = 0.3689658446483029 Training Accuracy = 0.9745633602142334\n","Iteration 214: Training Loss = 0.3673404969552821 Training Accuracy = 0.974590003490448\n","Iteration 215: Training Loss = 0.365485406939656 Training Accuracy = 0.9745766520500183\n","Iteration 216: Training Loss = 0.36560928961768807 Training Accuracy = 0.9747400283813477\n","Iteration 217: Training Loss = 0.36501483574196125 Training Accuracy = 0.9747883081436157\n","Iteration 218: Training Loss = 0.3638730447377537 Training Accuracy = 0.9748200178146362\n","Iteration 219: Training Loss = 0.3674655216503643 Training Accuracy = 0.9746766686439514\n","Iteration 220: Training Loss = 0.36568177817846376 Training Accuracy = 0.9747916460037231\n","Iteration 221: Training Loss = 0.3628726594388417 Training Accuracy = 0.9748550057411194\n","Iteration 222: Training Loss = 0.363788904185576 Training Accuracy = 0.9747750163078308\n","Iteration 223: Training Loss = 0.3626684848538727 Training Accuracy = 0.9749249815940857\n","Iteration 224: Training Loss = 0.362431261390615 Training Accuracy = 0.9751916527748108\n","Iteration 225: Training Loss = 0.3625164023977171 Training Accuracy = 0.9747933149337769\n","Iteration 226: Training Loss = 0.36171133650326337 Training Accuracy = 0.9750833511352539\n","Iteration 227: Training Loss = 0.3599318397672617 Training Accuracy = 0.9749749898910522\n","Iteration 228: Training Loss = 0.35869887648100374 Training Accuracy = 0.9750783443450928\n","Iteration 229: Training Loss = 0.3616892866370764 Training Accuracy = 0.9750099778175354\n","Iteration 230: Training Loss = 0.3603537040180489 Training Accuracy = 0.9753599762916565\n","Iteration 231: Training Loss = 0.3584853477113752 Training Accuracy = 0.9751516580581665\n","Iteration 232: Training Loss = 0.35895625155727695 Training Accuracy = 0.9751466512680054\n","Iteration 233: Training Loss = 0.3576690586055954 Training Accuracy = 0.9752783179283142\n","Iteration 234: Training Loss = 0.35918071341832947 Training Accuracy = 0.9751033186912537\n","Iteration 235: Training Loss = 0.36014117029632725 Training Accuracy = 0.9750816822052002\n","Iteration 236: Training Loss = 0.3581734095657665 Training Accuracy = 0.9753450155258179\n","Iteration 237: Training Loss = 0.35560723210761663 Training Accuracy = 0.9753766655921936\n","Iteration 238: Training Loss = 0.3572021780372508 Training Accuracy = 0.9751466512680054\n","Iteration 239: Training Loss = 0.3551286397590988 Training Accuracy = 0.9754800200462341\n","Iteration 240: Training Loss = 0.3580111908579256 Training Accuracy = 0.9753666520118713\n","Iteration 241: Training Loss = 0.35604580659587637 Training Accuracy = 0.9753233194351196\n","Iteration 242: Training Loss = 0.3538420349195288 Training Accuracy = 0.9756066799163818\n","Iteration 243: Training Loss = 0.3528853471360464 Training Accuracy = 0.9756766557693481\n","Iteration 244: Training Loss = 0.35367758979259306 Training Accuracy = 0.9756100177764893\n","Iteration 245: Training Loss = 0.35211715761805235 Training Accuracy = 0.975534975528717\n","Iteration 246: Training Loss = 0.3515847945249198 Training Accuracy = 0.9757533073425293\n","Iteration 247: Training Loss = 0.3545605782791914 Training Accuracy = 0.975350022315979\n","Iteration 248: Training Loss = 0.35285862890282843 Training Accuracy = 0.9756349921226501\n","Iteration 249: Training Loss = 0.35104639755487177 Training Accuracy = 0.9757633209228516\n","Iteration 250: Training Loss = 0.35258608526281227 Training Accuracy = 0.9755499958992004\n","Iteration 251: Training Loss = 0.3493033653141081 Training Accuracy = 0.9757750034332275\n","Iteration 252: Training Loss = 0.35142759509083854 Training Accuracy = 0.9756983518600464\n","Iteration 253: Training Loss = 0.35224122629110705 Training Accuracy = 0.9756183624267578\n","Iteration 254: Training Loss = 0.3513213885433871 Training Accuracy = 0.975754976272583\n","Iteration 255: Training Loss = 0.34810245364971765 Training Accuracy = 0.9758016467094421\n","Iteration 256: Training Loss = 0.3481097410039742 Training Accuracy = 0.9760950207710266\n","Iteration 257: Training Loss = 0.34923942619843484 Training Accuracy = 0.975695013999939\n","Iteration 258: Training Loss = 0.34839355098763164 Training Accuracy = 0.9759899973869324\n","Iteration 259: Training Loss = 0.350459514648984 Training Accuracy = 0.9757099747657776\n","Iteration 260: Training Loss = 0.3481823168339664 Training Accuracy = 0.9759316444396973\n","Iteration 261: Training Loss = 0.346835996592968 Training Accuracy = 0.9759983420372009\n","Iteration 262: Training Loss = 0.3479949795222342 Training Accuracy = 0.9760933518409729\n","Iteration 263: Training Loss = 0.345027309367272 Training Accuracy = 0.9759833216667175\n","Iteration 264: Training Loss = 0.3470658605696044 Training Accuracy = 0.9759133458137512\n","Iteration 265: Training Loss = 0.3455144316796695 Training Accuracy = 0.9760716557502747\n","Iteration 266: Training Loss = 0.3439895946374466 Training Accuracy = 0.9761416912078857\n","Iteration 267: Training Loss = 0.34603132625362937 Training Accuracy = 0.9760866761207581\n","Iteration 268: Training Loss = 0.3454031796218052 Training Accuracy = 0.9760533571243286\n","Iteration 269: Training Loss = 0.34324671102170445 Training Accuracy = 0.9761899709701538\n","Iteration 270: Training Loss = 0.34242531696342376 Training Accuracy = 0.9760633111000061\n","Iteration 271: Training Loss = 0.3445863604601093 Training Accuracy = 0.9761350154876709\n","Iteration 272: Training Loss = 0.3384897947245242 Training Accuracy = 0.9763633608818054\n","Iteration 273: Training Loss = 0.34401299292483944 Training Accuracy = 0.9758599996566772\n","Iteration 274: Training Loss = 0.3418247310467627 Training Accuracy = 0.9761199951171875\n","Iteration 275: Training Loss = 0.34182928329459633 Training Accuracy = 0.9765083193778992\n","Iteration 276: Training Loss = 0.34328570314634677 Training Accuracy = 0.9760366678237915\n","Iteration 277: Training Loss = 0.3417685677882546 Training Accuracy = 0.9760299921035767\n","Iteration 278: Training Loss = 0.3449189268855017 Training Accuracy = 0.9763233065605164\n","Iteration 279: Training Loss = 0.339334946559361 Training Accuracy = 0.976473331451416\n","Iteration 280: Training Loss = 0.33966333078437333 Training Accuracy = 0.9766299724578857\n","Iteration 281: Training Loss = 0.3409753063420492 Training Accuracy = 0.9762549996376038\n","Iteration 282: Training Loss = 0.33726826547318717 Training Accuracy = 0.9764999747276306\n","Iteration 283: Training Loss = 0.3399829764660154 Training Accuracy = 0.9763200283050537\n","Iteration 284: Training Loss = 0.338190904377015 Training Accuracy = 0.9764966368675232\n","Iteration 285: Training Loss = 0.3395241954024885 Training Accuracy = 0.976170003414154\n","Iteration 286: Training Loss = 0.3378326257114544 Training Accuracy = 0.9766299724578857\n","Iteration 287: Training Loss = 0.33757432060841036 Training Accuracy = 0.9767299890518188\n","Iteration 288: Training Loss = 0.33578959485625814 Training Accuracy = 0.9762883186340332\n","Iteration 289: Training Loss = 0.3358346306891581 Training Accuracy = 0.9764866828918457\n","Iteration 290: Training Loss = 0.3346186006097624 Training Accuracy = 0.976746678352356\n","Iteration 291: Training Loss = 0.337349238675387 Training Accuracy = 0.9765066504478455\n","Iteration 292: Training Loss = 0.33654761900975366 Training Accuracy = 0.9765633344650269\n","Iteration 293: Training Loss = 0.33406983840934024 Training Accuracy = 0.976848304271698\n","Iteration 294: Training Loss = 0.33350221738652863 Training Accuracy = 0.9766849875450134\n","Iteration 295: Training Loss = 0.3330668856409962 Training Accuracy = 0.9768083095550537\n","Iteration 296: Training Loss = 0.33370822732437594 Training Accuracy = 0.9767699837684631\n","Iteration 297: Training Loss = 0.3352714782222191 Training Accuracy = 0.9768400192260742\n","Iteration 298: Training Loss = 0.3347379803890729 Training Accuracy = 0.9765049815177917\n","Iteration 299: Training Loss = 0.33231128273252514 Training Accuracy = 0.9768683314323425\n","Iteration 300: Training Loss = 0.3340477940116925 Training Accuracy = 0.9766849875450134\n","Iteration 301: Training Loss = 0.33104060561828036 Training Accuracy = 0.9771216511726379\n","Iteration 302: Training Loss = 0.33405023129884026 Training Accuracy = 0.9767566919326782\n","Iteration 303: Training Loss = 0.3306195306992013 Training Accuracy = 0.9769899845123291\n","Iteration 304: Training Loss = 0.32961718043464194 Training Accuracy = 0.9768700003623962\n","Iteration 305: Training Loss = 0.3293622306653906 Training Accuracy = 0.976913332939148\n","Iteration 306: Training Loss = 0.33061590967381876 Training Accuracy = 0.9769166707992554\n","Iteration 307: Training Loss = 0.3309259932880614 Training Accuracy = 0.9769416451454163\n","Iteration 308: Training Loss = 0.32939722708934527 Training Accuracy = 0.9770416617393494\n","Iteration 309: Training Loss = 0.3318446945433814 Training Accuracy = 0.9767683148384094\n","Iteration 310: Training Loss = 0.3291888958098081 Training Accuracy = 0.9770449995994568\n","Iteration 311: Training Loss = 0.3289608113832614 Training Accuracy = 0.9770100116729736\n","Iteration 312: Training Loss = 0.3301144566863346 Training Accuracy = 0.9767799973487854\n","Iteration 313: Training Loss = 0.3289575148570627 Training Accuracy = 0.9770950078964233\n","Iteration 314: Training Loss = 0.3240440183600414 Training Accuracy = 0.977066695690155\n","Iteration 315: Training Loss = 0.33001856508763655 Training Accuracy = 0.9771100282669067\n","Iteration 316: Training Loss = 0.33115625859642567 Training Accuracy = 0.9769350290298462\n","Iteration 317: Training Loss = 0.3273899906644812 Training Accuracy = 0.9770783185958862\n","Iteration 318: Training Loss = 0.3282473099636139 Training Accuracy = 0.9772133231163025\n","Iteration 319: Training Loss = 0.32649613523495674 Training Accuracy = 0.9772650003433228\n","Iteration 320: Training Loss = 0.3254786298932709 Training Accuracy = 0.9772616624832153\n","Iteration 321: Training Loss = 0.3265022782675363 Training Accuracy = 0.977316677570343\n","Iteration 322: Training Loss = 0.3287271561457481 Training Accuracy = 0.9771516919136047\n","Iteration 323: Training Loss = 0.32674751278500924 Training Accuracy = 0.9775866866111755\n","Iteration 324: Training Loss = 0.3234263662381963 Training Accuracy = 0.9772999882698059\n","Iteration 325: Training Loss = 0.32424351396822776 Training Accuracy = 0.9772300124168396\n","Iteration 326: Training Loss = 0.32547409974283453 Training Accuracy = 0.9772666692733765\n","Iteration 327: Training Loss = 0.3235907915920624 Training Accuracy = 0.9776049852371216\n","Iteration 328: Training Loss = 0.32511501476022064 Training Accuracy = 0.9771466851234436\n","Iteration 329: Training Loss = 0.32413984977923477 Training Accuracy = 0.9772466421127319\n","Iteration 330: Training Loss = 0.32363732939145834 Training Accuracy = 0.9773100018501282\n","Iteration 331: Training Loss = 0.3235923494473652 Training Accuracy = 0.9774433374404907\n","Iteration 332: Training Loss = 0.32350481651742996 Training Accuracy = 0.9774699807167053\n","Iteration 333: Training Loss = 0.3240867507826445 Training Accuracy = 0.9775566458702087\n","Iteration 334: Training Loss = 0.3204866514006492 Training Accuracy = 0.977620005607605\n","Iteration 335: Training Loss = 0.3242486093613877 Training Accuracy = 0.9774050116539001\n","Iteration 336: Training Loss = 0.32150336332837515 Training Accuracy = 0.9775733351707458\n","Iteration 337: Training Loss = 0.3235972306918672 Training Accuracy = 0.9772766828536987\n","Iteration 338: Training Loss = 0.31977618122812584 Training Accuracy = 0.977596640586853\n","Iteration 339: Training Loss = 0.32187870564939114 Training Accuracy = 0.9776166677474976\n","Iteration 340: Training Loss = 0.323794353440347 Training Accuracy = 0.9774866700172424\n","Iteration 341: Training Loss = 0.3188121919396918 Training Accuracy = 0.9776350259780884\n","Iteration 342: Training Loss = 0.32063264966265975 Training Accuracy = 0.9776866436004639\n","Iteration 343: Training Loss = 0.32075368091744666 Training Accuracy = 0.9775516390800476\n","Iteration 344: Training Loss = 0.3183019449312077 Training Accuracy = 0.9776666760444641\n","Iteration 345: Training Loss = 0.31940503967184164 Training Accuracy = 0.9774933457374573\n","Iteration 346: Training Loss = 0.31744819615492276 Training Accuracy = 0.9777716398239136\n","Iteration 347: Training Loss = 0.3178846099651448 Training Accuracy = 0.9776433110237122\n","Iteration 348: Training Loss = 0.3185477910337539 Training Accuracy = 0.9777349829673767\n","Iteration 349: Training Loss = 0.3183759372834712 Training Accuracy = 0.9776600003242493\n","Iteration 350: Training Loss = 0.3181772205420537 Training Accuracy = 0.9777533411979675\n","Iteration 351: Training Loss = 0.3198341159167484 Training Accuracy = 0.9777950048446655\n","Iteration 352: Training Loss = 0.31876915102658093 Training Accuracy = 0.9777466654777527\n","Iteration 353: Training Loss = 0.31655609704332016 Training Accuracy = 0.9780516624450684\n","Iteration 354: Training Loss = 0.31655993719169756 Training Accuracy = 0.9778483510017395\n","Iteration 355: Training Loss = 0.3158604278522962 Training Accuracy = 0.9778149724006653\n","Iteration 356: Training Loss = 0.3183123853913358 Training Accuracy = 0.977596640586853\n","Iteration 357: Training Loss = 0.3171790706177018 Training Accuracy = 0.9778866767883301\n","Iteration 358: Training Loss = 0.3151730662703781 Training Accuracy = 0.9777366518974304\n","Iteration 359: Training Loss = 0.3175121635467228 Training Accuracy = 0.9776216745376587\n","Iteration 360: Training Loss = 0.3153126144246989 Training Accuracy = 0.9778450131416321\n","Iteration 361: Training Loss = 0.31235455035807363 Training Accuracy = 0.978106677532196\n","Iteration 362: Training Loss = 0.3113079576915792 Training Accuracy = 0.9780133366584778\n","Iteration 363: Training Loss = 0.31359379090501377 Training Accuracy = 0.9781133532524109\n","Iteration 364: Training Loss = 0.31320600569273793 Training Accuracy = 0.978201687335968\n","Iteration 365: Training Loss = 0.31202029419573557 Training Accuracy = 0.9782000184059143\n","Iteration 366: Training Loss = 0.3136121630667343 Training Accuracy = 0.9779733419418335\n","Iteration 367: Training Loss = 0.31404400641462027 Training Accuracy = 0.9778299927711487\n","Iteration 368: Training Loss = 0.3093108170120008 Training Accuracy = 0.9782516956329346\n","Iteration 369: Training Loss = 0.3121372238447664 Training Accuracy = 0.9782183170318604\n","Iteration 370: Training Loss = 0.31128434945122613 Training Accuracy = 0.9779199957847595\n","Iteration 371: Training Loss = 0.3083280802067842 Training Accuracy = 0.9784033298492432\n","Iteration 372: Training Loss = 0.30789456756108585 Training Accuracy = 0.9784116744995117\n","Iteration 373: Training Loss = 0.3119622795458592 Training Accuracy = 0.9781466722488403\n","Iteration 374: Training Loss = 0.3118095626573744 Training Accuracy = 0.9782183170318604\n","Iteration 375: Training Loss = 0.3102972283139916 Training Accuracy = 0.9781366586685181\n","Iteration 376: Training Loss = 0.3114948897845217 Training Accuracy = 0.9780383110046387\n","Iteration 377: Training Loss = 0.3103158044515124 Training Accuracy = 0.9781299829483032\n","Iteration 378: Training Loss = 0.31078340138435406 Training Accuracy = 0.978338360786438\n","Iteration 379: Training Loss = 0.3087416946417423 Training Accuracy = 0.9785116910934448\n","Iteration 380: Training Loss = 0.30836602389271567 Training Accuracy = 0.978338360786438\n","Iteration 381: Training Loss = 0.30730896132872026 Training Accuracy = 0.9783733487129211\n","Iteration 382: Training Loss = 0.30842591950842313 Training Accuracy = 0.978386640548706\n","Iteration 383: Training Loss = 0.3097118535053379 Training Accuracy = 0.9781866669654846\n","Iteration 384: Training Loss = 0.3089823282611833 Training Accuracy = 0.9784133434295654\n","Iteration 385: Training Loss = 0.30786232845752576 Training Accuracy = 0.9781233072280884\n","Iteration 386: Training Loss = 0.30769698923268474 Training Accuracy = 0.9783316850662231\n","Iteration 387: Training Loss = 0.3078621594709296 Training Accuracy = 0.978338360786438\n","Iteration 388: Training Loss = 0.3060810110645599 Training Accuracy = 0.978451669216156\n","Iteration 389: Training Loss = 0.3065454861910335 Training Accuracy = 0.9785749912261963\n","Iteration 390: Training Loss = 0.3068551605579382 Training Accuracy = 0.9783516526222229\n","Iteration 391: Training Loss = 0.3054407434810979 Training Accuracy = 0.978380024433136\n","Iteration 392: Training Loss = 0.305957930803748 Training Accuracy = 0.9784716963768005\n","Iteration 393: Training Loss = 0.3054146548840699 Training Accuracy = 0.978600025177002\n","Iteration 394: Training Loss = 0.3079559401378674 Training Accuracy = 0.9785716533660889\n","Iteration 395: Training Loss = 0.30527782982191015 Training Accuracy = 0.9783816933631897\n","Iteration 396: Training Loss = 0.30398537008205156 Training Accuracy = 0.9787449836730957\n","Iteration 397: Training Loss = 0.30370609702366663 Training Accuracy = 0.9788216948509216\n","Iteration 398: Training Loss = 0.3045090020639776 Training Accuracy = 0.9786233305931091\n","Iteration 399: Training Loss = 0.30336302514994445 Training Accuracy = 0.9786733388900757\n","Iteration 400: Training Loss = 0.3059420438546142 Training Accuracy = 0.9784083366394043\n","Iteration 401: Training Loss = 0.3032705399592391 Training Accuracy = 0.9786083102226257\n","Iteration 402: Training Loss = 0.3059990109145088 Training Accuracy = 0.9784500002861023\n","Iteration 403: Training Loss = 0.3041140547155245 Training Accuracy = 0.978879988193512\n","Iteration 404: Training Loss = 0.3040042968298691 Training Accuracy = 0.9786916375160217\n","Iteration 405: Training Loss = 0.3037600603181656 Training Accuracy = 0.9787766933441162\n","Iteration 406: Training Loss = 0.30249055508999023 Training Accuracy = 0.9788516759872437\n","Iteration 407: Training Loss = 0.30310588505769337 Training Accuracy = 0.978659987449646\n","Iteration 408: Training Loss = 0.3010841131944623 Training Accuracy = 0.9786783456802368\n","Iteration 409: Training Loss = 0.2996252771007397 Training Accuracy = 0.9788749814033508\n","Iteration 410: Training Loss = 0.30063239960213095 Training Accuracy = 0.9787316918373108\n","Iteration 411: Training Loss = 0.3019906379370109 Training Accuracy = 0.9788733124732971\n","Iteration 412: Training Loss = 0.2997530450372545 Training Accuracy = 0.9788050055503845\n","Iteration 413: Training Loss = 0.30354537697404405 Training Accuracy = 0.9788983464241028\n","Iteration 414: Training Loss = 0.2992522674374959 Training Accuracy = 0.9790233373641968\n","Iteration 415: Training Loss = 0.3005929171016668 Training Accuracy = 0.9788566827774048\n","Iteration 416: Training Loss = 0.3006340514004915 Training Accuracy = 0.9789299964904785\n","Iteration 417: Training Loss = 0.3034059255671263 Training Accuracy = 0.9785383343696594\n","Iteration 418: Training Loss = 0.30220224325511663 Training Accuracy = 0.9789050221443176\n","Iteration 419: Training Loss = 0.29861711380437905 Training Accuracy = 0.9789816737174988\n","Iteration 420: Training Loss = 0.29742797365359425 Training Accuracy = 0.9790549874305725\n","Iteration 421: Training Loss = 0.3006436876012796 Training Accuracy = 0.9788399934768677\n","Iteration 422: Training Loss = 0.2993548815233414 Training Accuracy = 0.9787916541099548\n","Iteration 423: Training Loss = 0.2965851705239481 Training Accuracy = 0.9791200160980225\n","Iteration 424: Training Loss = 0.3020444289486699 Training Accuracy = 0.9787650108337402\n","Iteration 425: Training Loss = 0.29831933860945 Training Accuracy = 0.9790133237838745\n","Iteration 426: Training Loss = 0.2995088632626172 Training Accuracy = 0.9789649844169617\n","Iteration 427: Training Loss = 0.29946353318705793 Training Accuracy = 0.9788316488265991\n","Iteration 428: Training Loss = 0.3007409544970015 Training Accuracy = 0.9786466956138611\n","Iteration 429: Training Loss = 0.2979012952576729 Training Accuracy = 0.9790733456611633\n","Iteration 430: Training Loss = 0.29693511262334604 Training Accuracy = 0.979170024394989\n","Iteration 431: Training Loss = 0.29795110795338986 Training Accuracy = 0.9789749979972839\n","Iteration 432: Training Loss = 0.29659873640486223 Training Accuracy = 0.9791499972343445\n","Iteration 433: Training Loss = 0.298786727111101 Training Accuracy = 0.9789350032806396\n","Iteration 434: Training Loss = 0.29635250062330143 Training Accuracy = 0.9791250228881836\n","Iteration 435: Training Loss = 0.29596748040304893 Training Accuracy = 0.9790033102035522\n","Iteration 436: Training Loss = 0.29670082136962206 Training Accuracy = 0.9789783358573914\n","Iteration 437: Training Loss = 0.29667860012068226 Training Accuracy = 0.9790449738502502\n","Iteration 438: Training Loss = 0.29619708801848327 Training Accuracy = 0.9792199730873108\n","Iteration 439: Training Loss = 0.29662873284427876 Training Accuracy = 0.9792916774749756\n","Iteration 440: Training Loss = 0.29685566144066977 Training Accuracy = 0.9790549874305725\n","Iteration 441: Training Loss = 0.2958231209725055 Training Accuracy = 0.9790949821472168\n","Iteration 442: Training Loss = 0.2940056066912018 Training Accuracy = 0.9794416427612305\n","Iteration 443: Training Loss = 0.2944567579037804 Training Accuracy = 0.9792733192443848\n","Iteration 444: Training Loss = 0.2929995788250257 Training Accuracy = 0.979348361492157\n","Iteration 445: Training Loss = 0.2959371365700755 Training Accuracy = 0.9790683388710022\n","Iteration 446: Training Loss = 0.2931725769137765 Training Accuracy = 0.9793533086776733\n","Iteration 447: Training Loss = 0.294705746751928 Training Accuracy = 0.9791433215141296\n","Iteration 448: Training Loss = 0.29282144372717206 Training Accuracy = 0.9792683124542236\n","Iteration 449: Training Loss = 0.2905761544714271 Training Accuracy = 0.9793450236320496\n","Iteration 450: Training Loss = 0.29095286554964983 Training Accuracy = 0.9792816638946533\n","Iteration 451: Training Loss = 0.2934096528520435 Training Accuracy = 0.9794433116912842\n","Iteration 452: Training Loss = 0.2911080553672763 Training Accuracy = 0.9794966578483582\n","Iteration 453: Training Loss = 0.2921219496127092 Training Accuracy = 0.9791333079338074\n","Iteration 454: Training Loss = 0.2933191280035871 Training Accuracy = 0.9792633056640625\n","Iteration 455: Training Loss = 0.2928864211680998 Training Accuracy = 0.9793649911880493\n","Iteration 456: Training Loss = 0.292079885555938 Training Accuracy = 0.9794750213623047\n","Iteration 457: Training Loss = 0.2933158709927071 Training Accuracy = 0.9793099761009216\n","Iteration 458: Training Loss = 0.29001702921439454 Training Accuracy = 0.979515016078949\n","Iteration 459: Training Loss = 0.2895914289330771 Training Accuracy = 0.9796000123023987\n","Iteration 460: Training Loss = 0.28979615094405653 Training Accuracy = 0.9796649813652039\n","Iteration 461: Training Loss = 0.2894137054846718 Training Accuracy = 0.9796883463859558\n","Iteration 462: Training Loss = 0.2908790616704957 Training Accuracy = 0.9794716835021973\n","Iteration 463: Training Loss = 0.2875856454225582 Training Accuracy = 0.9795950055122375\n","Iteration 464: Training Loss = 0.29285968351401426 Training Accuracy = 0.9792400002479553\n","Iteration 465: Training Loss = 0.2904288294727966 Training Accuracy = 0.9795849919319153\n","Iteration 466: Training Loss = 0.2906843822211647 Training Accuracy = 0.9795416593551636\n","Iteration 467: Training Loss = 0.29134398266123285 Training Accuracy = 0.9793750047683716\n","Iteration 468: Training Loss = 0.2898649637398823 Training Accuracy = 0.9795916676521301\n","Iteration 469: Training Loss = 0.2864227357322996 Training Accuracy = 0.9794999957084656\n","Iteration 470: Training Loss = 0.2891977609853277 Training Accuracy = 0.9796333312988281\n","Iteration 471: Training Loss = 0.2881549923694549 Training Accuracy = 0.9797066450119019\n","Iteration 472: Training Loss = 0.28691194032543094 Training Accuracy = 0.9794366955757141\n","Iteration 473: Training Loss = 0.28665156369886885 Training Accuracy = 0.9798250198364258\n","Iteration 474: Training Loss = 0.28861314821424233 Training Accuracy = 0.9793866872787476\n","Iteration 475: Training Loss = 0.28775462427627146 Training Accuracy = 0.9795516729354858\n","Iteration 476: Training Loss = 0.2866530469506271 Training Accuracy = 0.9797899723052979\n","Iteration 477: Training Loss = 0.28935797793466966 Training Accuracy = 0.9796433448791504\n","Iteration 478: Training Loss = 0.2889674482247803 Training Accuracy = 0.9794350266456604\n","Iteration 479: Training Loss = 0.2856588730300272 Training Accuracy = 0.9799050092697144\n","Iteration 480: Training Loss = 0.28651289945884045 Training Accuracy = 0.9796966910362244\n","Iteration 481: Training Loss = 0.28706995163073756 Training Accuracy = 0.9795833230018616\n","Iteration 482: Training Loss = 0.28568685254357784 Training Accuracy = 0.9797400236129761\n","Iteration 483: Training Loss = 0.281596196909092 Training Accuracy = 0.9799466729164124\n","Iteration 484: Training Loss = 0.2855204742976992 Training Accuracy = 0.9796183109283447\n","Iteration 485: Training Loss = 0.28527948790642527 Training Accuracy = 0.9799683094024658\n","Iteration 486: Training Loss = 0.2866366560602566 Training Accuracy = 0.9798349738121033\n","Iteration 487: Training Loss = 0.28649596488285256 Training Accuracy = 0.9795883297920227\n","Iteration 488: Training Loss = 0.284572810268084 Training Accuracy = 0.9801366925239563\n","Iteration 489: Training Loss = 0.284284192203461 Training Accuracy = 0.979669988155365\n","Iteration 490: Training Loss = 0.2852547515102796 Training Accuracy = 0.9796183109283447\n","Iteration 491: Training Loss = 0.28356297476513587 Training Accuracy = 0.9799649715423584\n","Iteration 492: Training Loss = 0.281404712381601 Training Accuracy = 0.9800216555595398\n","Iteration 493: Training Loss = 0.2853555336405437 Training Accuracy = 0.9798499941825867\n","Iteration 494: Training Loss = 0.2853870507245401 Training Accuracy = 0.9800416827201843\n","Iteration 495: Training Loss = 0.2848922056619385 Training Accuracy = 0.9798616766929626\n","Iteration 496: Training Loss = 0.2860029444822846 Training Accuracy = 0.9798499941825867\n","Iteration 497: Training Loss = 0.2842251744212286 Training Accuracy = 0.9798233509063721\n","Iteration 498: Training Loss = 0.28318819960140335 Training Accuracy = 0.980014979839325\n","Iteration 499: Training Loss = 0.28210112290271594 Training Accuracy = 0.9799666404724121\n","Iteration 500: Training Loss = 0.28332658072831435 Training Accuracy = 0.9796983599662781\n","Iteration 501: Training Loss = 0.28239796623020236 Training Accuracy = 0.9799116849899292\n","Iteration 502: Training Loss = 0.282646808834191 Training Accuracy = 0.9801433086395264\n","Iteration 503: Training Loss = 0.2823827342480512 Training Accuracy = 0.9798583388328552\n","Iteration 504: Training Loss = 0.2837467905272618 Training Accuracy = 0.9798150062561035\n","Iteration 505: Training Loss = 0.2834678048268019 Training Accuracy = 0.9798849821090698\n","Iteration 506: Training Loss = 0.2815491341362794 Training Accuracy = 0.9800383448600769\n","Iteration 507: Training Loss = 0.28049603817392194 Training Accuracy = 0.980014979839325\n","Iteration 508: Training Loss = 0.2789575066981016 Training Accuracy = 0.9802583456039429\n","Iteration 509: Training Loss = 0.28190301239343973 Training Accuracy = 0.9801216721534729\n","Iteration 510: Training Loss = 0.2792120947013323 Training Accuracy = 0.9803033471107483\n","Iteration 511: Training Loss = 0.2849991053301652 Training Accuracy = 0.9797099828720093\n","Iteration 512: Training Loss = 0.28112006282992696 Training Accuracy = 0.980358362197876\n","Iteration 513: Training Loss = 0.2816864207223635 Training Accuracy = 0.9800716638565063\n","Iteration 514: Training Loss = 0.279648581519478 Training Accuracy = 0.9800616502761841\n","Iteration 515: Training Loss = 0.2805003917517041 Training Accuracy = 0.9804966449737549\n","Iteration 516: Training Loss = 0.27855096126092227 Training Accuracy = 0.9803666472434998\n","Iteration 517: Training Loss = 0.2776347693793992 Training Accuracy = 0.9800549745559692\n","Iteration 518: Training Loss = 0.2783628459809972 Training Accuracy = 0.9801966547966003\n","Iteration 519: Training Loss = 0.2777548497598277 Training Accuracy = 0.9804400205612183\n","Iteration 520: Training Loss = 0.28058593065580656 Training Accuracy = 0.9803033471107483\n","Iteration 521: Training Loss = 0.2781928700540387 Training Accuracy = 0.9803383350372314\n","Iteration 522: Training Loss = 0.27927505171565425 Training Accuracy = 0.980263352394104\n","Iteration 523: Training Loss = 0.27520351877137655 Training Accuracy = 0.9804733395576477\n","Iteration 524: Training Loss = 0.27980761058826464 Training Accuracy = 0.9800900220870972\n","Iteration 525: Training Loss = 0.2793990604305537 Training Accuracy = 0.9804333448410034\n","Iteration 526: Training Loss = 0.2758497447166081 Training Accuracy = 0.9806650280952454\n","Iteration 527: Training Loss = 0.2765723540655238 Training Accuracy = 0.9803100228309631\n","Iteration 528: Training Loss = 0.27612529895123183 Training Accuracy = 0.9801700115203857\n","Iteration 529: Training Loss = 0.2783861348983354 Training Accuracy = 0.9805033206939697\n","Iteration 530: Training Loss = 0.2755616380214359 Training Accuracy = 0.980448305606842\n","Iteration 531: Training Loss = 0.2764135568545812 Training Accuracy = 0.9804350137710571\n","Iteration 532: Training Loss = 0.2770611942287154 Training Accuracy = 0.98021000623703\n","Iteration 533: Training Loss = 0.27842589325528305 Training Accuracy = 0.9803850054740906\n","Iteration 534: Training Loss = 0.2774057931908723 Training Accuracy = 0.9803366661071777\n","Iteration 535: Training Loss = 0.2758470632678065 Training Accuracy = 0.9805600047111511\n","Iteration 536: Training Loss = 0.27465669393409464 Training Accuracy = 0.9806216955184937\n","Iteration 537: Training Loss = 0.27447709151412963 Training Accuracy = 0.9804766774177551\n","Iteration 538: Training Loss = 0.2758842159560069 Training Accuracy = 0.9801899790763855\n","Iteration 539: Training Loss = 0.2741358562516359 Training Accuracy = 0.9806466698646545\n","Iteration 540: Training Loss = 0.27376109912350927 Training Accuracy = 0.980501651763916\n","Iteration 541: Training Loss = 0.2740483071275948 Training Accuracy = 0.9804850220680237\n","Iteration 542: Training Loss = 0.2706798187525562 Training Accuracy = 0.9807699918746948\n","Iteration 543: Training Loss = 0.27514126471140854 Training Accuracy = 0.980364978313446\n","Iteration 544: Training Loss = 0.2731561351963242 Training Accuracy = 0.9805700182914734\n","Iteration 545: Training Loss = 0.2727438861140097 Training Accuracy = 0.9807366728782654\n","Iteration 546: Training Loss = 0.27595494630300393 Training Accuracy = 0.9804783463478088\n","Iteration 547: Training Loss = 0.27219065940686454 Training Accuracy = 0.9808283448219299\n","Iteration 548: Training Loss = 0.2737052552812794 Training Accuracy = 0.9804966449737549\n","Iteration 549: Training Loss = 0.2715868117633058 Training Accuracy = 0.980513334274292\n","Iteration 550: Training Loss = 0.2737599554952651 Training Accuracy = 0.980376660823822\n","Iteration 551: Training Loss = 0.2728118795749754 Training Accuracy = 0.9806749820709229\n","Iteration 552: Training Loss = 0.2722250420633728 Training Accuracy = 0.9804566502571106\n","Iteration 553: Training Loss = 0.27238364795314596 Training Accuracy = 0.9806616902351379\n","Iteration 554: Training Loss = 0.2696752265270313 Training Accuracy = 0.9808350205421448\n","Iteration 555: Training Loss = 0.27222105739629615 Training Accuracy = 0.9808983206748962\n","Iteration 556: Training Loss = 0.27108266131819325 Training Accuracy = 0.9806883335113525\n","Iteration 557: Training Loss = 0.27117895678732534 Training Accuracy = 0.9808099865913391\n","Iteration 558: Training Loss = 0.26965785050013913 Training Accuracy = 0.9806366562843323\n","Iteration 559: Training Loss = 0.27202408449223686 Training Accuracy = 0.9804800152778625\n","Iteration 560: Training Loss = 0.271674773876599 Training Accuracy = 0.9806283116340637\n","Iteration 561: Training Loss = 0.2710866681547051 Training Accuracy = 0.9805949926376343\n","Iteration 562: Training Loss = 0.2715835069917267 Training Accuracy = 0.9806966781616211\n","Iteration 563: Training Loss = 0.2740262638583526 Training Accuracy = 0.9805949926376343\n","Iteration 564: Training Loss = 0.2716904762960461 Training Accuracy = 0.980638325214386\n","Iteration 565: Training Loss = 0.2731892393910566 Training Accuracy = 0.9806966781616211\n","Iteration 566: Training Loss = 0.2716656123322356 Training Accuracy = 0.9807850122451782\n","Iteration 567: Training Loss = 0.26982699635599133 Training Accuracy = 0.9806883335113525\n","Iteration 568: Training Loss = 0.27070307077145056 Training Accuracy = 0.9808250069618225\n","Iteration 569: Training Loss = 0.2737571606365151 Training Accuracy = 0.9806333184242249\n","Iteration 570: Training Loss = 0.2678265235759725 Training Accuracy = 0.981041669845581\n","Iteration 571: Training Loss = 0.2685119524660829 Training Accuracy = 0.9808099865913391\n","Iteration 572: Training Loss = 0.2703094172951811 Training Accuracy = 0.9806399941444397\n","Iteration 573: Training Loss = 0.27152087281275006 Training Accuracy = 0.9806249737739563\n","Iteration 574: Training Loss = 0.27096404169288746 Training Accuracy = 0.9807933568954468\n","Iteration 575: Training Loss = 0.2683413896588694 Training Accuracy = 0.981011688709259\n","Iteration 576: Training Loss = 0.2664472662885767 Training Accuracy = 0.9808750152587891\n","Iteration 577: Training Loss = 0.26699852311122857 Training Accuracy = 0.9810199737548828\n","Iteration 578: Training Loss = 0.2680712852725382 Training Accuracy = 0.9810733199119568\n","Iteration 579: Training Loss = 0.2711743612483329 Training Accuracy = 0.9808866381645203\n","Iteration 580: Training Loss = 0.2688171395116603 Training Accuracy = 0.980888307094574\n","Iteration 581: Training Loss = 0.2675342990546444 Training Accuracy = 0.9810266494750977\n","Iteration 582: Training Loss = 0.2680260280171446 Training Accuracy = 0.9807683229446411\n","Iteration 583: Training Loss = 0.2673262598661185 Training Accuracy = 0.9809150099754333\n","Iteration 584: Training Loss = 0.26715272708542287 Training Accuracy = 0.9807733297348022\n","Iteration 585: Training Loss = 0.2670083966950114 Training Accuracy = 0.9809216856956482\n","Iteration 586: Training Loss = 0.26599967548234904 Training Accuracy = 0.9809583425521851\n","Iteration 587: Training Loss = 0.2654665067394082 Training Accuracy = 0.9811233282089233\n","Iteration 588: Training Loss = 0.26714646608996034 Training Accuracy = 0.9811516404151917\n","Iteration 589: Training Loss = 0.2660268384873746 Training Accuracy = 0.9810400009155273\n","Iteration 590: Training Loss = 0.2650034255632348 Training Accuracy = 0.9810450077056885\n","Iteration 591: Training Loss = 0.26623039253173436 Training Accuracy = 0.9809566736221313\n","Iteration 592: Training Loss = 0.26615996604315856 Training Accuracy = 0.9810366630554199\n","Iteration 593: Training Loss = 0.2646976965138943 Training Accuracy = 0.9808433055877686\n","Iteration 594: Training Loss = 0.26488572631508 Training Accuracy = 0.9810283184051514\n","Iteration 595: Training Loss = 0.2662415206029015 Training Accuracy = 0.9809200167655945\n","Iteration 596: Training Loss = 0.2665406843965145 Training Accuracy = 0.9811733365058899\n","Iteration 597: Training Loss = 0.2665901069641597 Training Accuracy = 0.9807733297348022\n","Iteration 598: Training Loss = 0.2675806563234302 Training Accuracy = 0.9809849858283997\n","Iteration 599: Training Loss = 0.2654978215981312 Training Accuracy = 0.9810666441917419\n","Iteration 600: Training Loss = 0.26309759249811904 Training Accuracy = 0.9811716675758362\n","Iteration 601: Training Loss = 0.25893451323551037 Training Accuracy = 0.9815716743469238\n","Iteration 602: Training Loss = 0.265514779115001 Training Accuracy = 0.9810933470726013\n","Iteration 603: Training Loss = 0.2670564536173463 Training Accuracy = 0.980888307094574\n","Iteration 604: Training Loss = 0.2627738510160436 Training Accuracy = 0.9813433289527893\n","Iteration 605: Training Loss = 0.2641661697471308 Training Accuracy = 0.9810683131217957\n","Iteration 606: Training Loss = 0.2646245758746292 Training Accuracy = 0.9812999963760376\n","Iteration 607: Training Loss = 0.26340831341436033 Training Accuracy = 0.9812716841697693\n","Iteration 608: Training Loss = 0.26171989293856 Training Accuracy = 0.9811549782752991\n","Iteration 609: Training Loss = 0.26051551307794074 Training Accuracy = 0.9816233515739441\n","Iteration 610: Training Loss = 0.2631809551420725 Training Accuracy = 0.9811766743659973\n","Iteration 611: Training Loss = 0.26359475005896416 Training Accuracy = 0.9811433553695679\n","Iteration 612: Training Loss = 0.26219628122116617 Training Accuracy = 0.9814383387565613\n","Iteration 613: Training Loss = 0.2620156860728891 Training Accuracy = 0.9814649820327759\n","Iteration 614: Training Loss = 0.2620110562325542 Training Accuracy = 0.9812883138656616\n","Iteration 615: Training Loss = 0.26145261091636846 Training Accuracy = 0.981291651725769\n","Iteration 616: Training Loss = 0.26147189759521067 Training Accuracy = 0.9815866947174072\n","Iteration 617: Training Loss = 0.2602791823532599 Training Accuracy = 0.9814566373825073\n","Iteration 618: Training Loss = 0.2617270260074436 Training Accuracy = 0.9811750054359436\n","Iteration 619: Training Loss = 0.26273039801575854 Training Accuracy = 0.9809733629226685\n","Iteration 620: Training Loss = 0.2596399108941844 Training Accuracy = 0.981398344039917\n","Iteration 621: Training Loss = 0.25725063039895923 Training Accuracy = 0.9818066954612732\n","Iteration 622: Training Loss = 0.25931306912673285 Training Accuracy = 0.9815383553504944\n","Iteration 623: Training Loss = 0.2597991943122192 Training Accuracy = 0.9814233183860779\n","Iteration 624: Training Loss = 0.2597421208870534 Training Accuracy = 0.9813383221626282\n","Iteration 625: Training Loss = 0.2602871501024673 Training Accuracy = 0.981386661529541\n","Iteration 626: Training Loss = 0.26185519743895197 Training Accuracy = 0.981136679649353\n","Iteration 627: Training Loss = 0.26292178227402957 Training Accuracy = 0.9812049865722656\n","Iteration 628: Training Loss = 0.2573302538426572 Training Accuracy = 0.9815566539764404\n","Iteration 629: Training Loss = 0.2578485888024614 Training Accuracy = 0.9816733598709106\n","Iteration 630: Training Loss = 0.258793407025953 Training Accuracy = 0.9812999963760376\n","Iteration 631: Training Loss = 0.2591413576401144 Training Accuracy = 0.9814649820327759\n","Iteration 632: Training Loss = 0.2578225973314844 Training Accuracy = 0.9815416932106018\n","Iteration 633: Training Loss = 0.25882153514698686 Training Accuracy = 0.9814983606338501\n","Iteration 634: Training Loss = 0.2578563591842471 Training Accuracy = 0.9815483093261719\n","Iteration 635: Training Loss = 0.2567484263748284 Training Accuracy = 0.9817649722099304\n","Iteration 636: Training Loss = 0.2561045355590108 Training Accuracy = 0.9816449880599976\n","Iteration 637: Training Loss = 0.2547021496959015 Training Accuracy = 0.9816416501998901\n","Iteration 638: Training Loss = 0.25985725641403323 Training Accuracy = 0.9815566539764404\n","Iteration 639: Training Loss = 0.25784336085510884 Training Accuracy = 0.9817133545875549\n","Iteration 640: Training Loss = 0.25989073818452074 Training Accuracy = 0.9815199971199036\n","Iteration 641: Training Loss = 0.2576084514455876 Training Accuracy = 0.9815383553504944\n","Iteration 642: Training Loss = 0.25468395644486996 Training Accuracy = 0.9818350076675415\n","Iteration 643: Training Loss = 0.25620506237374635 Training Accuracy = 0.9817550182342529\n","Iteration 644: Training Loss = 0.25787831100301595 Training Accuracy = 0.9815049767494202\n","Iteration 645: Training Loss = 0.2557039104162495 Training Accuracy = 0.9816633462905884\n","Iteration 646: Training Loss = 0.257124782425076 Training Accuracy = 0.9815883040428162\n","Iteration 647: Training Loss = 0.25555439896499127 Training Accuracy = 0.981856644153595\n","Iteration 648: Training Loss = 0.25999693494573706 Training Accuracy = 0.9813783168792725\n","Iteration 649: Training Loss = 0.25980371180059075 Training Accuracy = 0.9814366698265076\n","Iteration 650: Training Loss = 0.2554985081719678 Training Accuracy = 0.9817933440208435\n","Iteration 651: Training Loss = 0.2543854121278182 Training Accuracy = 0.981660008430481\n","Iteration 652: Training Loss = 0.25362686585879846 Training Accuracy = 0.9817866683006287\n","Iteration 653: Training Loss = 0.2578040741043695 Training Accuracy = 0.9814266562461853\n","Iteration 654: Training Loss = 0.2574595087941759 Training Accuracy = 0.9817000031471252\n","Iteration 655: Training Loss = 0.25416479497795463 Training Accuracy = 0.9817133545875549\n","Iteration 656: Training Loss = 0.25627713255748513 Training Accuracy = 0.9818900227546692\n","Iteration 657: Training Loss = 0.2522166031191967 Training Accuracy = 0.981939971446991\n","Iteration 658: Training Loss = 0.25446129008240453 Training Accuracy = 0.9817416667938232\n","Iteration 659: Training Loss = 0.25364708998479685 Training Accuracy = 0.9816233515739441\n","Iteration 660: Training Loss = 0.2544340229601883 Training Accuracy = 0.9819650053977966\n","Iteration 661: Training Loss = 0.2533178207921773 Training Accuracy = 0.9818333387374878\n","Iteration 662: Training Loss = 0.25682631596212824 Training Accuracy = 0.9816833138465881\n","Iteration 663: Training Loss = 0.2528266341301217 Training Accuracy = 0.9820166826248169\n","Iteration 664: Training Loss = 0.2545435580201617 Training Accuracy = 0.9820899963378906\n","Iteration 665: Training Loss = 0.2533158239270026 Training Accuracy = 0.9817933440208435\n","Iteration 666: Training Loss = 0.2523428629529795 Training Accuracy = 0.9817550182342529\n","Iteration 667: Training Loss = 0.2540546332780768 Training Accuracy = 0.9814883470535278\n","Iteration 668: Training Loss = 0.2540888039962914 Training Accuracy = 0.9820066690444946\n","Iteration 669: Training Loss = 0.25267338481101026 Training Accuracy = 0.9819200038909912\n","Iteration 670: Training Loss = 0.2513313387007823 Training Accuracy = 0.9820616841316223\n","Iteration 671: Training Loss = 0.2536698972482344 Training Accuracy = 0.9819083213806152\n","Iteration 672: Training Loss = 0.2515107904104594 Training Accuracy = 0.9822800159454346\n","Iteration 673: Training Loss = 0.24910619115860436 Training Accuracy = 0.9820600152015686\n","Iteration 674: Training Loss = 0.2509499011626744 Training Accuracy = 0.9818116426467896\n","Iteration 675: Training Loss = 0.2529862057297799 Training Accuracy = 0.9818249940872192\n","Iteration 676: Training Loss = 0.2518284991839809 Training Accuracy = 0.981909990310669\n","Iteration 677: Training Loss = 0.2529603951323754 Training Accuracy = 0.9817399978637695\n","Iteration 678: Training Loss = 0.2519670985541735 Training Accuracy = 0.9818866848945618\n","Iteration 679: Training Loss = 0.2516122884407455 Training Accuracy = 0.9820333123207092\n","Iteration 680: Training Loss = 0.25203734446132886 Training Accuracy = 0.9819700121879578\n","Iteration 681: Training Loss = 0.25263937873616016 Training Accuracy = 0.981856644153595\n","Iteration 682: Training Loss = 0.2517161442845457 Training Accuracy = 0.9819849729537964\n","Iteration 683: Training Loss = 0.24954936486086068 Training Accuracy = 0.98225998878479\n","Iteration 684: Training Loss = 0.2499399162157119 Training Accuracy = 0.982021689414978\n","Iteration 685: Training Loss = 0.2504831762013324 Training Accuracy = 0.9819616675376892\n","Iteration 686: Training Loss = 0.2514891117903907 Training Accuracy = 0.9818950295448303\n","Iteration 687: Training Loss = 0.24986444642562486 Training Accuracy = 0.9818883538246155\n","Iteration 688: Training Loss = 0.252041467944165 Training Accuracy = 0.9818333387374878\n","Iteration 689: Training Loss = 0.24772341787890514 Training Accuracy = 0.9822733402252197\n","Iteration 690: Training Loss = 0.24977950490072687 Training Accuracy = 0.982010006904602\n","Iteration 691: Training Loss = 0.2505548384265953 Training Accuracy = 0.9819783568382263\n","Iteration 692: Training Loss = 0.2491479682283637 Training Accuracy = 0.9821649789810181\n","Iteration 693: Training Loss = 0.25011928702631153 Training Accuracy = 0.982230007648468\n","Iteration 694: Training Loss = 0.24810573740183917 Training Accuracy = 0.9820433259010315\n","Iteration 695: Training Loss = 0.24693802633958956 Training Accuracy = 0.9823416471481323\n","Iteration 696: Training Loss = 0.24726414588167797 Training Accuracy = 0.9823766946792603\n","Iteration 697: Training Loss = 0.2504290946206211 Training Accuracy = 0.9820483326911926\n","Iteration 698: Training Loss = 0.24914112554543802 Training Accuracy = 0.9822149872779846\n","Iteration 699: Training Loss = 0.24729631201923222 Training Accuracy = 0.9822133183479309\n","Iteration 700: Training Loss = 0.250010121797625 Training Accuracy = 0.9821383357048035\n","Iteration 701: Training Loss = 0.24849330221094473 Training Accuracy = 0.9823066592216492\n","Iteration 702: Training Loss = 0.24705816461541508 Training Accuracy = 0.9821983575820923\n","Iteration 703: Training Loss = 0.24954879709334818 Training Accuracy = 0.982105016708374\n","Iteration 704: Training Loss = 0.24514034008975122 Training Accuracy = 0.9821916818618774\n","Iteration 705: Training Loss = 0.24936905410476645 Training Accuracy = 0.9821333289146423\n","Iteration 706: Training Loss = 0.24748750547539305 Training Accuracy = 0.9821799993515015\n","Iteration 707: Training Loss = 0.2471001748825487 Training Accuracy = 0.9822266697883606\n","Iteration 708: Training Loss = 0.25077999860803024 Training Accuracy = 0.9820233583450317\n","Iteration 709: Training Loss = 0.24758821647720694 Training Accuracy = 0.9822099804878235\n","Iteration 710: Training Loss = 0.24868277090827343 Training Accuracy = 0.9822116494178772\n","Iteration 711: Training Loss = 0.24592670471653824 Training Accuracy = 0.9820666909217834\n","Iteration 712: Training Loss = 0.2477431937015829 Training Accuracy = 0.9823683500289917\n","Iteration 713: Training Loss = 0.2490547128422514 Training Accuracy = 0.9822016954421997\n","Iteration 714: Training Loss = 0.2451052725450553 Training Accuracy = 0.9822483062744141\n","Iteration 715: Training Loss = 0.24647395923046006 Training Accuracy = 0.9824566841125488\n","Iteration 716: Training Loss = 0.24649191712645252 Training Accuracy = 0.9823499917984009\n","Iteration 717: Training Loss = 0.24613864980405584 Training Accuracy = 0.982271671295166\n","Iteration 718: Training Loss = 0.24474945566037815 Training Accuracy = 0.9822133183479309\n","Iteration 719: Training Loss = 0.24756769245504032 Training Accuracy = 0.9821983575820923\n","Iteration 720: Training Loss = 0.24495057553342095 Training Accuracy = 0.9823750257492065\n","Iteration 721: Training Loss = 0.24645303807313224 Training Accuracy = 0.9823949933052063\n","Iteration 722: Training Loss = 0.24316911084981158 Training Accuracy = 0.9825500249862671\n","Iteration 723: Training Loss = 0.2438951678552025 Training Accuracy = 0.9825599789619446\n","Iteration 724: Training Loss = 0.2456123525244245 Training Accuracy = 0.9824016690254211\n","Iteration 725: Training Loss = 0.2426220087403696 Training Accuracy = 0.9825316667556763\n","Iteration 726: Training Loss = 0.24253816790124838 Training Accuracy = 0.9824216961860657\n","Iteration 727: Training Loss = 0.2455933465885167 Training Accuracy = 0.9824050068855286\n","Iteration 728: Training Loss = 0.24444734271718316 Training Accuracy = 0.9824016690254211\n","Iteration 729: Training Loss = 0.24215243984304174 Training Accuracy = 0.9826449751853943\n","Iteration 730: Training Loss = 0.24476500570346302 Training Accuracy = 0.98225998878479\n","Iteration 731: Training Loss = 0.2410072210719511 Training Accuracy = 0.9825966954231262\n","Iteration 732: Training Loss = 0.24292378206673984 Training Accuracy = 0.9825816750526428\n","Iteration 733: Training Loss = 0.24436899719813476 Training Accuracy = 0.9825816750526428\n","Iteration 734: Training Loss = 0.24308641153550617 Training Accuracy = 0.982438325881958\n","Iteration 735: Training Loss = 0.24429251210007408 Training Accuracy = 0.9823916554450989\n","Iteration 736: Training Loss = 0.2441797073363596 Training Accuracy = 0.9823483228683472\n","Iteration 737: Training Loss = 0.2417449250871512 Training Accuracy = 0.9824533462524414\n","Iteration 738: Training Loss = 0.24338547412265796 Training Accuracy = 0.9824416637420654\n","Iteration 739: Training Loss = 0.24249874350494624 Training Accuracy = 0.9825283288955688\n","Iteration 740: Training Loss = 0.23981022958183523 Training Accuracy = 0.9826833605766296\n","Iteration 741: Training Loss = 0.24209499207787075 Training Accuracy = 0.9827799797058105\n","Iteration 742: Training Loss = 0.24316945256937061 Training Accuracy = 0.9827333092689514\n","Iteration 743: Training Loss = 0.2417769455485719 Training Accuracy = 0.9824483394622803\n","Iteration 744: Training Loss = 0.24191958178196518 Training Accuracy = 0.9826416373252869\n","Iteration 745: Training Loss = 0.24202046069456906 Training Accuracy = 0.9826499819755554\n","Iteration 746: Training Loss = 0.24096492402437675 Training Accuracy = 0.9826116561889648\n","Iteration 747: Training Loss = 0.24175027998583526 Training Accuracy = 0.9827666878700256\n","Iteration 748: Training Loss = 0.24315251199620214 Training Accuracy = 0.9823983311653137\n","Iteration 749: Training Loss = 0.23880394686115264 Training Accuracy = 0.9829833507537842\n","Iteration 750: Training Loss = 0.24158306046331282 Training Accuracy = 0.9826366901397705\n","Iteration 751: Training Loss = 0.23891609083619159 Training Accuracy = 0.9827666878700256\n","Iteration 752: Training Loss = 0.23989508545339522 Training Accuracy = 0.9826683402061462\n","Iteration 753: Training Loss = 0.24272909069009746 Training Accuracy = 0.9824366569519043\n","Iteration 754: Training Loss = 0.23872680461459633 Training Accuracy = 0.9826099872589111\n","Iteration 755: Training Loss = 0.24220191387392095 Training Accuracy = 0.9825066924095154\n","Iteration 756: Training Loss = 0.23987130466488438 Training Accuracy = 0.9826383590698242\n","Iteration 757: Training Loss = 0.2380457665835688 Training Accuracy = 0.9827383160591125\n","Iteration 758: Training Loss = 0.24168489018215739 Training Accuracy = 0.9825783371925354\n","Iteration 759: Training Loss = 0.2408225254762308 Training Accuracy = 0.9829866886138916\n","Iteration 760: Training Loss = 0.24143041379037847 Training Accuracy = 0.9825466871261597\n","Iteration 761: Training Loss = 0.2395530174850788 Training Accuracy = 0.9828283190727234\n","Iteration 762: Training Loss = 0.23880306778100496 Training Accuracy = 0.9828016757965088\n","Iteration 763: Training Loss = 0.24154315412077396 Training Accuracy = 0.9828150272369385\n","Iteration 764: Training Loss = 0.23804333932104751 Training Accuracy = 0.9826750159263611\n","Iteration 765: Training Loss = 0.2405967458622413 Training Accuracy = 0.9827283620834351\n","Iteration 766: Training Loss = 0.2385696279792054 Training Accuracy = 0.9828716516494751\n","Iteration 767: Training Loss = 0.23946760121241165 Training Accuracy = 0.9828616380691528\n","Iteration 768: Training Loss = 0.2428688736812227 Training Accuracy = 0.9824333190917969\n","Iteration 769: Training Loss = 0.23792092418342836 Training Accuracy = 0.9827650189399719\n","Iteration 770: Training Loss = 0.2399853177680129 Training Accuracy = 0.9827366471290588\n","Iteration 771: Training Loss = 0.23656942233802822 Training Accuracy = 0.9826933145523071\n","Iteration 772: Training Loss = 0.237176589369014 Training Accuracy = 0.9827550053596497\n","Iteration 773: Training Loss = 0.23613328620761373 Training Accuracy = 0.9830016493797302\n","Iteration 774: Training Loss = 0.238570364974176 Training Accuracy = 0.9827483296394348\n","Iteration 775: Training Loss = 0.23790926712707228 Training Accuracy = 0.9829049706459045\n","Iteration 776: Training Loss = 0.23727641173416292 Training Accuracy = 0.9827716946601868\n","Iteration 777: Training Loss = 0.2377195632881906 Training Accuracy = 0.9829099774360657\n","Iteration 778: Training Loss = 0.2374857182221618 Training Accuracy = 0.9829049706459045\n","Iteration 779: Training Loss = 0.23572360218095303 Training Accuracy = 0.9828733205795288\n","Iteration 780: Training Loss = 0.2376444438787472 Training Accuracy = 0.9828516840934753\n","Iteration 781: Training Loss = 0.23537678066509451 Training Accuracy = 0.9830483198165894\n","Iteration 782: Training Loss = 0.2374071520040857 Training Accuracy = 0.9828266501426697\n","Iteration 783: Training Loss = 0.2382609141269938 Training Accuracy = 0.9827866554260254\n","Iteration 784: Training Loss = 0.23811509027368558 Training Accuracy = 0.9829233288764954\n","Iteration 785: Training Loss = 0.23444748514001235 Training Accuracy = 0.9827983379364014\n","Iteration 786: Training Loss = 0.23534231289900162 Training Accuracy = 0.9829116463661194\n","Iteration 787: Training Loss = 0.23457712157189203 Training Accuracy = 0.9831783175468445\n","Iteration 788: Training Loss = 0.2363539136038784 Training Accuracy = 0.9831416606903076\n","Iteration 789: Training Loss = 0.23628625537546255 Training Accuracy = 0.9828016757965088\n","Iteration 790: Training Loss = 0.2344588223259447 Training Accuracy = 0.9830966591835022\n","Iteration 791: Training Loss = 0.2364160578768364 Training Accuracy = 0.9829933047294617\n","Iteration 792: Training Loss = 0.23336960219093258 Training Accuracy = 0.9831183552742004\n","Iteration 793: Training Loss = 0.23395774882329842 Training Accuracy = 0.9831716418266296\n","Iteration 794: Training Loss = 0.23462148675217626 Training Accuracy = 0.9830683469772339\n","Iteration 795: Training Loss = 0.23563134435987695 Training Accuracy = 0.9830516576766968\n","Iteration 796: Training Loss = 0.23332473497678427 Training Accuracy = 0.9832850098609924\n","Iteration 797: Training Loss = 0.23323388880854073 Training Accuracy = 0.9831200242042542\n","Iteration 798: Training Loss = 0.23515941834258575 Training Accuracy = 0.9830266833305359\n","Iteration 799: Training Loss = 0.2340328954595097 Training Accuracy = 0.9831900000572205\n","Iteration 800: Training Loss = 0.23558304220427823 Training Accuracy = 0.9830949902534485\n","Iteration 801: Training Loss = 0.23436417700736892 Training Accuracy = 0.9833066463470459\n","Iteration 802: Training Loss = 0.23506458932444238 Training Accuracy = 0.9829599857330322\n","Iteration 803: Training Loss = 0.2329846546764011 Training Accuracy = 0.9833049774169922\n","Iteration 804: Training Loss = 0.23401829965193288 Training Accuracy = 0.9830216765403748\n","Iteration 805: Training Loss = 0.23487689439517392 Training Accuracy = 0.983186662197113\n","Iteration 806: Training Loss = 0.23204271703048948 Training Accuracy = 0.983240008354187\n","Iteration 807: Training Loss = 0.2343672459682242 Training Accuracy = 0.9831183552742004\n","Iteration 808: Training Loss = 0.23450293963763366 Training Accuracy = 0.9830116629600525\n","Iteration 809: Training Loss = 0.23301062234893177 Training Accuracy = 0.9832183122634888\n","Iteration 810: Training Loss = 0.2325681607504782 Training Accuracy = 0.9831699728965759\n","Iteration 811: Training Loss = 0.23411952727752028 Training Accuracy = 0.9828600287437439\n","Iteration 812: Training Loss = 0.23307971819171483 Training Accuracy = 0.9834966659545898\n","Iteration 813: Training Loss = 0.23173541819479737 Training Accuracy = 0.983210027217865\n","Iteration 814: Training Loss = 0.23500467930848104 Training Accuracy = 0.9830716848373413\n","Iteration 815: Training Loss = 0.23388429210163506 Training Accuracy = 0.9829816818237305\n","Iteration 816: Training Loss = 0.2333039386981968 Training Accuracy = 0.9831116795539856\n","Iteration 817: Training Loss = 0.23146415587509359 Training Accuracy = 0.983353316783905\n","Iteration 818: Training Loss = 0.23250872643248624 Training Accuracy = 0.9830949902534485\n","Iteration 819: Training Loss = 0.23403975975213887 Training Accuracy = 0.9830583333969116\n","Iteration 820: Training Loss = 0.23168825816751237 Training Accuracy = 0.9833149909973145\n","Iteration 821: Training Loss = 0.23032133259679752 Training Accuracy = 0.9832649827003479\n","Iteration 822: Training Loss = 0.23162528148590003 Training Accuracy = 0.9832350015640259\n","Iteration 823: Training Loss = 0.23127741809021365 Training Accuracy = 0.9831416606903076\n","Iteration 824: Training Loss = 0.23266528242925516 Training Accuracy = 0.9833366870880127\n","Iteration 825: Training Loss = 0.23163525234500473 Training Accuracy = 0.983144998550415\n","Iteration 826: Training Loss = 0.23329569801798158 Training Accuracy = 0.9832666516304016\n","Iteration 827: Training Loss = 0.2303544380236846 Training Accuracy = 0.9832199811935425\n","Iteration 828: Training Loss = 0.23114606872508586 Training Accuracy = 0.9832783341407776\n","Iteration 829: Training Loss = 0.23052859238484877 Training Accuracy = 0.9833250045776367\n","Iteration 830: Training Loss = 0.22905823435331588 Training Accuracy = 0.983436644077301\n","Iteration 831: Training Loss = 0.23269009923751474 Training Accuracy = 0.9833700060844421\n","Iteration 832: Training Loss = 0.22990526289373545 Training Accuracy = 0.9832916855812073\n","Iteration 833: Training Loss = 0.23099366033492966 Training Accuracy = 0.9830783605575562\n","Iteration 834: Training Loss = 0.2337657907712167 Training Accuracy = 0.9832199811935425\n","Iteration 835: Training Loss = 0.23039628406751453 Training Accuracy = 0.983519971370697\n","Iteration 836: Training Loss = 0.22861411387424996 Training Accuracy = 0.9832983613014221\n","Iteration 837: Training Loss = 0.22973955190137094 Training Accuracy = 0.9835050106048584\n","Iteration 838: Training Loss = 0.22867581454903932 Training Accuracy = 0.9836016893386841\n","Iteration 839: Training Loss = 0.22993055398802983 Training Accuracy = 0.9833516478538513\n","Iteration 840: Training Loss = 0.22899475406703193 Training Accuracy = 0.9833250045776367\n","Iteration 841: Training Loss = 0.22764514515485115 Training Accuracy = 0.9836633205413818\n","Iteration 842: Training Loss = 0.2302747773429648 Training Accuracy = 0.9834116697311401\n","Iteration 843: Training Loss = 0.22917395986659936 Training Accuracy = 0.9832333326339722\n","Iteration 844: Training Loss = 0.22766687245557324 Training Accuracy = 0.9833983182907104\n","Iteration 845: Training Loss = 0.22834926362631577 Training Accuracy = 0.9833449721336365\n","Iteration 846: Training Loss = 0.2283369630265577 Training Accuracy = 0.9836549758911133\n","Iteration 847: Training Loss = 0.22811863829944215 Training Accuracy = 0.9836750030517578\n","Iteration 848: Training Loss = 0.22598951863225214 Training Accuracy = 0.9833966493606567\n","Iteration 849: Training Loss = 0.22695430397983704 Training Accuracy = 0.9835283160209656\n","Iteration 850: Training Loss = 0.22692945016781335 Training Accuracy = 0.9834250211715698\n","Iteration 851: Training Loss = 0.22812982401137588 Training Accuracy = 0.9835566878318787\n","Iteration 852: Training Loss = 0.22904063275967734 Training Accuracy = 0.9831799864768982\n","Iteration 853: Training Loss = 0.22634421128299964 Training Accuracy = 0.9834150075912476\n","Iteration 854: Training Loss = 0.22868665956796697 Training Accuracy = 0.9833616614341736\n","Iteration 855: Training Loss = 0.22738443818902715 Training Accuracy = 0.9837016463279724\n","Iteration 856: Training Loss = 0.22714985936393356 Training Accuracy = 0.9836783409118652\n","Iteration 857: Training Loss = 0.22676682138969778 Training Accuracy = 0.9834833145141602\n","Iteration 858: Training Loss = 0.22340388943584882 Training Accuracy = 0.9837083220481873\n","Iteration 859: Training Loss = 0.22600578935241775 Training Accuracy = 0.983543336391449\n","Iteration 860: Training Loss = 0.2289900992180926 Training Accuracy = 0.9833366870880127\n","Iteration 861: Training Loss = 0.2261291286828147 Training Accuracy = 0.9837433099746704\n","Iteration 862: Training Loss = 0.22599386635035348 Training Accuracy = 0.983543336391449\n","Iteration 863: Training Loss = 0.22453066165212573 Training Accuracy = 0.9838483333587646\n","Iteration 864: Training Loss = 0.22699562717696045 Training Accuracy = 0.9833966493606567\n","Iteration 865: Training Loss = 0.22729604023234343 Training Accuracy = 0.9835500121116638\n","Iteration 866: Training Loss = 0.2271086428783483 Training Accuracy = 0.9833583235740662\n","Iteration 867: Training Loss = 0.22448193372498743 Training Accuracy = 0.9837499856948853\n","Iteration 868: Training Loss = 0.22553069337106038 Training Accuracy = 0.9837233424186707\n","Iteration 869: Training Loss = 0.2266218553918382 Training Accuracy = 0.9834833145141602\n","Iteration 870: Training Loss = 0.22528858607970423 Training Accuracy = 0.983614981174469\n","Iteration 871: Training Loss = 0.22680376431154783 Training Accuracy = 0.9837549924850464\n","Iteration 872: Training Loss = 0.2251608799638296 Training Accuracy = 0.9835983514785767\n","Iteration 873: Training Loss = 0.22565800568449862 Training Accuracy = 0.9836016893386841\n","Iteration 874: Training Loss = 0.22321532314186218 Training Accuracy = 0.9837216734886169\n","Iteration 875: Training Loss = 0.22507755727034492 Training Accuracy = 0.9837666749954224\n","Iteration 876: Training Loss = 0.22139498873696903 Training Accuracy = 0.9839566946029663\n","Iteration 877: Training Loss = 0.22333658067971715 Training Accuracy = 0.9836199879646301\n","Iteration 878: Training Loss = 0.22627464313700335 Training Accuracy = 0.983513355255127\n","Iteration 879: Training Loss = 0.22685987883687334 Training Accuracy = 0.9836633205413818\n","Iteration 880: Training Loss = 0.22305807991449483 Training Accuracy = 0.9836833477020264\n","Iteration 881: Training Loss = 0.22593319785738544 Training Accuracy = 0.9835533499717712\n","Iteration 882: Training Loss = 0.2225863425320708 Training Accuracy = 0.9837316870689392\n","Iteration 883: Training Loss = 0.2245893783699364 Training Accuracy = 0.9836633205413818\n","Iteration 884: Training Loss = 0.22293127036189223 Training Accuracy = 0.984041690826416\n","Iteration 885: Training Loss = 0.22383263637845224 Training Accuracy = 0.9838500022888184\n","Iteration 886: Training Loss = 0.22373559927397277 Training Accuracy = 0.9837300181388855\n","Iteration 887: Training Loss = 0.22237701352076128 Training Accuracy = 0.9839900135993958\n","Iteration 888: Training Loss = 0.22195710977521302 Training Accuracy = 0.9837783575057983\n","Iteration 889: Training Loss = 0.2241689886767027 Training Accuracy = 0.9837750196456909\n","Iteration 890: Training Loss = 0.22508687243460684 Training Accuracy = 0.9836633205413818\n","Iteration 891: Training Loss = 0.22258893771483101 Training Accuracy = 0.9838250279426575\n","Iteration 892: Training Loss = 0.22233690894811317 Training Accuracy = 0.9840116500854492\n","Iteration 893: Training Loss = 0.2234246011600828 Training Accuracy = 0.9837683439254761\n","Iteration 894: Training Loss = 0.22234446700088795 Training Accuracy = 0.9839216470718384\n","Iteration 895: Training Loss = 0.22204887034015416 Training Accuracy = 0.9840283393859863\n","Iteration 896: Training Loss = 0.22260607865939136 Training Accuracy = 0.9838233590126038\n","Iteration 897: Training Loss = 0.22351272586387097 Training Accuracy = 0.9837700128555298\n","Iteration 898: Training Loss = 0.22234077132915592 Training Accuracy = 0.9840066432952881\n","Iteration 899: Training Loss = 0.2233847280471893 Training Accuracy = 0.9839416742324829\n","Iteration 900: Training Loss = 0.22169863933065675 Training Accuracy = 0.9838600158691406\n","Iteration 901: Training Loss = 0.2222515237756013 Training Accuracy = 0.9836733341217041\n","Iteration 902: Training Loss = 0.22268978548577756 Training Accuracy = 0.9839816689491272\n","Iteration 903: Training Loss = 0.2194925444536196 Training Accuracy = 0.9841766953468323\n","Iteration 904: Training Loss = 0.2185266637739575 Training Accuracy = 0.9839850068092346\n","Iteration 905: Training Loss = 0.22246174230990962 Training Accuracy = 0.9838666915893555\n","Iteration 906: Training Loss = 0.2207475812401274 Training Accuracy = 0.9840649962425232\n","Iteration 907: Training Loss = 0.22209271517060006 Training Accuracy = 0.9836333394050598\n","Iteration 908: Training Loss = 0.2226234707064704 Training Accuracy = 0.9836616516113281\n","Iteration 909: Training Loss = 0.21972384907808956 Training Accuracy = 0.9841333627700806\n","Iteration 910: Training Loss = 0.22294375453088994 Training Accuracy = 0.9839933514595032\n","Iteration 911: Training Loss = 0.22075711780125004 Training Accuracy = 0.9840533137321472\n","Iteration 912: Training Loss = 0.2209918860143397 Training Accuracy = 0.9839900135993958\n","Iteration 913: Training Loss = 0.21816847679858922 Training Accuracy = 0.9839733242988586\n","Iteration 914: Training Loss = 0.22011198036172863 Training Accuracy = 0.983918309211731\n","Iteration 915: Training Loss = 0.22186057092513156 Training Accuracy = 0.9840466380119324\n","Iteration 916: Training Loss = 0.22155242815144646 Training Accuracy = 0.9838749766349792\n","Iteration 917: Training Loss = 0.218474096860347 Training Accuracy = 0.9840483069419861\n","Iteration 918: Training Loss = 0.21841652608457585 Training Accuracy = 0.9841150045394897\n","Iteration 919: Training Loss = 0.22091460608830027 Training Accuracy = 0.9840899705886841\n","Iteration 920: Training Loss = 0.21895268499129816 Training Accuracy = 0.9841833114624023\n","Iteration 921: Training Loss = 0.2181132757158704 Training Accuracy = 0.9842166900634766\n","Iteration 922: Training Loss = 0.21879585881027933 Training Accuracy = 0.9841466546058655\n","Iteration 923: Training Loss = 0.21674365926010358 Training Accuracy = 0.9840666651725769\n","Iteration 924: Training Loss = 0.22066299155161592 Training Accuracy = 0.9839616417884827\n","Iteration 925: Training Loss = 0.21777447763432045 Training Accuracy = 0.9841266870498657\n","Iteration 926: Training Loss = 0.21768973823371843 Training Accuracy = 0.984083354473114\n","Iteration 927: Training Loss = 0.21973632806672608 Training Accuracy = 0.9839100241661072\n","Iteration 928: Training Loss = 0.216354277401054 Training Accuracy = 0.9842233061790466\n","Iteration 929: Training Loss = 0.21915503761040298 Training Accuracy = 0.9838983416557312\n","Iteration 930: Training Loss = 0.21960735510185017 Training Accuracy = 0.9840066432952881\n","Iteration 931: Training Loss = 0.2153604646685657 Training Accuracy = 0.9842166900634766\n","Iteration 932: Training Loss = 0.21812936446355807 Training Accuracy = 0.9840616583824158\n","Iteration 933: Training Loss = 0.2207516090140841 Training Accuracy = 0.9838166832923889\n","Iteration 934: Training Loss = 0.2181074692072926 Training Accuracy = 0.9841499924659729\n","Iteration 935: Training Loss = 0.21829469701253632 Training Accuracy = 0.9841333627700806\n","Iteration 936: Training Loss = 0.21773949546947274 Training Accuracy = 0.9840483069419861\n","Iteration 937: Training Loss = 0.21808844315078027 Training Accuracy = 0.9841566681861877\n","Iteration 938: Training Loss = 0.2155130060655862 Training Accuracy = 0.9842466711997986\n","Iteration 939: Training Loss = 0.21516753378274797 Training Accuracy = 0.9841483235359192\n","Iteration 940: Training Loss = 0.2159282823229207 Training Accuracy = 0.9842050075531006\n","Iteration 941: Training Loss = 0.21534528322694235 Training Accuracy = 0.984416663646698\n","Iteration 942: Training Loss = 0.21806238728482286 Training Accuracy = 0.9840733408927917\n","Iteration 943: Training Loss = 0.2176317650850259 Training Accuracy = 0.98430997133255\n","Iteration 944: Training Loss = 0.21429716309518357 Training Accuracy = 0.9842933416366577\n","Iteration 945: Training Loss = 0.21693697728114422 Training Accuracy = 0.984071671962738\n","Iteration 946: Training Loss = 0.21602093176351875 Training Accuracy = 0.9843266606330872\n","Iteration 947: Training Loss = 0.21416499202598163 Training Accuracy = 0.9844549894332886\n","Iteration 948: Training Loss = 0.21649770564666274 Training Accuracy = 0.9842883348464966\n","Iteration 949: Training Loss = 0.2165774584758233 Training Accuracy = 0.9840350151062012\n","Iteration 950: Training Loss = 0.2168585631943576 Training Accuracy = 0.9843299984931946\n","Iteration 951: Training Loss = 0.21660900215714188 Training Accuracy = 0.9843549728393555\n","Iteration 952: Training Loss = 0.2197542938309604 Training Accuracy = 0.9839866757392883\n","Iteration 953: Training Loss = 0.21779314687271356 Training Accuracy = 0.9841566681861877\n","Iteration 954: Training Loss = 0.21466011855143827 Training Accuracy = 0.9844183325767517\n","Iteration 955: Training Loss = 0.21726853642415087 Training Accuracy = 0.9840366840362549\n","Iteration 956: Training Loss = 0.2148041824310115 Training Accuracy = 0.9843350052833557\n","Iteration 957: Training Loss = 0.2119580902455957 Training Accuracy = 0.9845283627510071\n","Iteration 958: Training Loss = 0.21630840952552735 Training Accuracy = 0.9843599796295166\n","Iteration 959: Training Loss = 0.2155259963010035 Training Accuracy = 0.9842983484268188\n","Iteration 960: Training Loss = 0.21355288717111243 Training Accuracy = 0.984345018863678\n","Iteration 961: Training Loss = 0.21981928943289297 Training Accuracy = 0.9840850234031677\n","Iteration 962: Training Loss = 0.21350034321559877 Training Accuracy = 0.9842749834060669\n","Iteration 963: Training Loss = 0.2180737528387273 Training Accuracy = 0.9841899871826172\n","Iteration 964: Training Loss = 0.21254887336076664 Training Accuracy = 0.9843683242797852\n","Iteration 965: Training Loss = 0.2143187471471209 Training Accuracy = 0.9841316938400269\n","Iteration 966: Training Loss = 0.2160923093311707 Training Accuracy = 0.9843966960906982\n","Iteration 967: Training Loss = 0.21639310801636055 Training Accuracy = 0.9841866493225098\n","Iteration 968: Training Loss = 0.21593079959327577 Training Accuracy = 0.984250009059906\n","Iteration 969: Training Loss = 0.21426651687745005 Training Accuracy = 0.9844516515731812\n","Iteration 970: Training Loss = 0.21462960723904717 Training Accuracy = 0.984250009059906\n","Iteration 971: Training Loss = 0.2137233629171182 Training Accuracy = 0.9844216704368591\n","Iteration 972: Training Loss = 0.21156202718913245 Training Accuracy = 0.9846866726875305\n","Iteration 973: Training Loss = 0.21177010341946598 Training Accuracy = 0.9846466779708862\n","Iteration 974: Training Loss = 0.21410237766115123 Training Accuracy = 0.9843999743461609\n","Iteration 975: Training Loss = 0.21468151798336632 Training Accuracy = 0.9843483567237854\n","Iteration 976: Training Loss = 0.21464919482156955 Training Accuracy = 0.9842983484268188\n","Iteration 977: Training Loss = 0.21165150707505803 Training Accuracy = 0.984428346157074\n","Iteration 978: Training Loss = 0.21395324861869328 Training Accuracy = 0.9845566749572754\n","Iteration 979: Training Loss = 0.21483879465289277 Training Accuracy = 0.9841983318328857\n","Iteration 980: Training Loss = 0.21151831693012263 Training Accuracy = 0.9845799803733826\n","Iteration 981: Training Loss = 0.21081515500676687 Training Accuracy = 0.98471999168396\n","Iteration 982: Training Loss = 0.2126175291672804 Training Accuracy = 0.9846283197402954\n","Iteration 983: Training Loss = 0.21174960465753573 Training Accuracy = 0.984375\n","Iteration 984: Training Loss = 0.2153758921147791 Training Accuracy = 0.9840450286865234\n","Iteration 985: Training Loss = 0.21090410744068608 Training Accuracy = 0.9844949841499329\n","Iteration 986: Training Loss = 0.2128798276054489 Training Accuracy = 0.9844300150871277\n","Iteration 987: Training Loss = 0.20901232486177712 Training Accuracy = 0.9844899773597717\n","Iteration 988: Training Loss = 0.2133489680916578 Training Accuracy = 0.9844383597373962\n","Iteration 989: Training Loss = 0.21167324929470813 Training Accuracy = 0.9844716787338257\n","Iteration 990: Training Loss = 0.21276535502614752 Training Accuracy = 0.9844766855239868\n","Iteration 991: Training Loss = 0.2085797322928726 Training Accuracy = 0.9848600029945374\n","Iteration 992: Training Loss = 0.21021042595449682 Training Accuracy = 0.9846983551979065\n","Iteration 993: Training Loss = 0.21044380257260054 Training Accuracy = 0.9847033619880676\n","Iteration 994: Training Loss = 0.2122825288757304 Training Accuracy = 0.984220027923584\n","Iteration 995: Training Loss = 0.20956277720454947 Training Accuracy = 0.9846383333206177\n","Iteration 996: Training Loss = 0.21207662138861655 Training Accuracy = 0.9844650030136108\n","Iteration 997: Training Loss = 0.2108849623118921 Training Accuracy = 0.9845700263977051\n","Iteration 998: Training Loss = 0.21002761618636073 Training Accuracy = 0.9846583604812622\n","Iteration 999: Training Loss = 0.21186254775372468 Training Accuracy = 0.9847716689109802\n","Dropout Probability:  0.75\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Execution Time:  433.294130735\n"],"name":"stdout"}]}]}