{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Question1_3_1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOR6XKQTjPQXe0Dt6eoMqTm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2lPurQieHuNk","executionInfo":{"status":"ok","timestamp":1618763967637,"user_tz":-60,"elapsed":302327,"user":{"displayName":"Shane Quinn","photoUrl":"","userId":"04299873785165069898"}},"outputId":"8de32a77-4c9d-4963-fdc3-8361d0a6feb1"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Name: Shane Quinn\n","Student Number: R00144107\n","Email: shane.quinn1@mycit.ie\n","Course: MSc Artificial Intelligence\n","Module: Deep Learning\n","Date: 03/04/2021\n","\"\"\"\n","\n","import tensorflow as tf\n","from keras.utils import np_utils\n","from tensorflow.python.client import device_lib\n","from tensorflow.keras.datasets import fashion_mnist\n","import matplotlib.pyplot as plt\n","import functools\n","import time\n","import numpy as np\n","\n","\n","def forward_pass(X, w1, w2, w3, w4, w5, w6, b1, b2, b3, b4, b5, b6):\n","    \"\"\"\n","    Push feature data through neural network. Final sigmoid activation layer returns flattened encoded pixel values\n","    between 0 and 1 (black and white). \n","\n","    Parameters\n","    ----------\n","    X : tf.Variable\n","        Pre-processed input data.\n","    w1 : tf.Variable\n","        Layer 1 learnable weights.\n","    w2 : tf.Variable\n","        Layer 2 learnable weights.\n","    w3 : tf.Variable\n","        Layer 3 learnable weights.\n","    w4 : tf.Variable\n","        Layer 4 learnable weights.\n","    w5 : tf.Variable\n","        Layer 5 learnable weights.\n","    w6 : tf.Variable\n","        Layer 6 learnable weights.\n","    b1 : tf.Variable\n","        Layer 1 bias.\n","    b2 : tf.Variable\n","        Layer 2 bias.\n","    b3 : tf.Variable\n","        Layer 3 bias.\n","    b4 : tf.Variable\n","        Layer 4 bias.\n","    b5 : tf.Variable\n","        Layer 5 bias.\n","    b6 : tf.Variable\n","        Layer 6 bias.\n","\n","    Returns\n","    -------\n","    H : td.Variable\n","        Flattened autoencoded pixel values between 0 and 1.\n","\n","    \"\"\"    \n","\n","    #Layer 1: 128 Relu Neruons\n","    A = tf.matmul(w1, tf.transpose(X)) + b1                    #A1 = w1.X + b1\n","    H = tf.keras.activations.relu(A)                          #H1 = act(A1)\n","    #Layer 2: 64 Relu Neruons\n","    A = tf.matmul(w2, H) + b2                                 #A2 = w2.H1 + b2\n","    H = tf.keras.activations.relu(A)                          #H2 = act(A2)     \n","    #Layer 3: 32 Relu Neurons\n","    A = tf.matmul(w3, H) + b3                                 #A3 = w3.H2 + b3\n","    H = tf.keras.activations.relu(A)                          #H3 = act(A3)\n","    #Layer 4: 64 Relu Neurons\n","    A = tf.matmul(w4, H) + b4                                 #A4 = w2.H1 + b4\n","    H = tf.keras.activations.relu(A)                          #H4 = act(A4)\n","    #Layer 5: 128 Relu Neurons\n","    A = tf.matmul(w5, H) + b5                                 #A5 = w5.H1 + b5\n","    H = tf.keras.activations.relu(A)                          #H5 = act(A5) \n","    #Layer 6: 784 Sigmoid Neurons \n","    A = tf.matmul(w6, H) + b6                                   #A6 = w6.H5 + b6\n","    H = tf.sigmoid(A)                                           #H6 = sigmoid(A6)\n","\n","    H = tf.transpose(H)\n","    \n","    return H\n","    \n","\n","\n","\n","\n","\n","def mean_absolute_error(X_encoded, X_true):\n","    \"\"\"\n","    Returns mean absolute error given encoded image and true image data\n","\n","    Parameters\n","    ----------\n","    X_encoded : tf.Variable\n","        DESCRIPTION.\n","    X_true : tf.Variable\n","        DESCRIPTION.\n","\n","    Returns\n","    -------\n","    mae : float\n","        Mean absolute error.\n","    \"\"\"\n","    \n","    mae = tf.math.divide(tf.reduce_sum(abs(tf.math.subtract(X_true, X_encoded))), X_encoded.shape[0])\n","\n","    return mae\n","\n","\n","\n","def calculate_accuracy(X_encoded, X):\n","    \"\"\"\n","    Calculate the model accuracy given encoded and original images\n","\n","    Parameters\n","    ----------\n","    X_encoded : tf.Variable\n","        Encoded image (output of forward pass).\n","    X : tf.Variable\n","        Original Images.\n","\n","    Returns\n","    -------\n","    accuracy : float32\n","        Model Accuracy.\n","    \"\"\"\n","\n","    # Boolean True (1) if prediction is correct\n","    predictions = tf.cast(tf.equal(X_encoded, X), tf.float32)\n","    #Mean value of correct predictions\n","    accuracy = tf.reduce_mean(predictions)\n","    \n","    return accuracy\n","\n","\n","def exec_time(func):\n","    \"\"\"\n","    Generic Execution time recorder, pass in function. Records execution time using decorators\n","\n","    Parameters\n","    ----------\n","    func : FUNCTION\n","        Function .\n","\n","\n","    \"\"\"\n","    \n","    @functools.wraps(func)\n","    def record_exec_time(*args, **kwargs):\n","        start_time = time.perf_counter()\n","        mn = func(*args, **kwargs)\n","        execution_time = time.perf_counter() - start_time\n","        print(\"Execution Time: \", execution_time)\n","        return mn\n","\n","    return record_exec_time\n","\n","@exec_time \n","def main():\n","      \n","    X, nX, X_val, nX_val = pre_process() \n","\n","    #Initialise Learning rate and iterations.\n","    learning_rate = 0.05\n","    iterations = 5000\n","    datatype = tf.float32\n","\n","    \n","    #Initialise lists for saving accuracies/loss\n","    te_acc = []\n","    tr_acc = []\n","    te_loss = []\n","    tr_loss = []\n","    \n","    # Create tf variables from data\n","    X = tf.cast(X, datatype)\n","    nX = tf.cast(nX, datatype)\n","    X_val = tf.cast(X_val, datatype)\n","    nX_val = tf.cast(nX_val, datatype)\n","    \n","    #Initialise Adam Optimizer\n","    adam = tf.keras.optimizers.Adam()\n","    \n","    #Initialise weights and bias\n","    zeros = tf.zeros_initializer()\n","    layer1_weights = tf.Variable(tf.random.normal([128,784], stddev=0.05, dtype=datatype))\n","    layer2_weights = tf.Variable(tf.random.normal([64, 128], stddev=0.05, dtype=datatype))\n","    layer3_weights = tf.Variable(tf.random.normal([32, 64], stddev=0.05, dtype=datatype))\n","    layer4_weights = tf.Variable(tf.random.normal([64,32], stddev=0.05, dtype=datatype))\n","    layer5_weights = tf.Variable(tf.random.normal([128, 64], stddev=0.05, dtype=datatype))\n","    layer6_weights = tf.Variable(tf.random.normal([784, 128], stddev=0.05, dtype=datatype))\n","    layer1_bias = tf.Variable(0, dtype=datatype)\n","    layer2_bias = tf.Variable(0, dtype=datatype)\n","    layer3_bias = tf.Variable(0, dtype=datatype)\n","    layer4_bias = tf.Variable(0, dtype=datatype)\n","    layer5_bias = tf.Variable(0, dtype=datatype)\n","    layer6_bias = tf.Variable(0, dtype=datatype)\n","    \n","\n","    for i in range(iterations):\n","        \n","        with tf.GradientTape() as tape:\n","            #Create instance of gradient tape to record forward pass and calculate gradients for learnable weights and biases\n","            encoded_X = forward_pass(nX, layer1_weights, layer2_weights, layer3_weights,\n","                                  layer4_weights, layer5_weights, layer6_weights,\n","                                  layer1_bias, layer2_bias, layer3_bias, \n","                                  layer4_bias, layer5_bias, layer6_bias)\n","            loss = mean_absolute_error(encoded_X, X)\n","            \n","        tr_loss.append(loss)\n","        #Calculate Gradients using gradient tape\n","        gradients = tape.gradient(loss, [layer1_weights, layer2_weights, layer3_weights,\n","                                          layer4_weights, layer5_weights, layer6_weights,\n","                                          layer1_bias, layer2_bias, layer3_bias, \n","                                          layer4_bias, layer5_bias, layer6_bias])\n","        accuracy = calculate_accuracy(encoded_X, X)\n","        tr_acc.append(accuracy)\n","        print(\"Iteration {}: Training Loss = {} Training Accuracy = {}\".format(i, loss.numpy(), accuracy.numpy()))\n","        \n","        #Apply gradients using adaptive movement estimation, see accompanied report for more details\n","        adam.apply_gradients(zip(gradients, [layer1_weights, layer2_weights, layer3_weights,\n","                                              layer4_weights, layer5_weights, layer6_weights,\n","                                              layer1_bias, layer2_bias, layer3_bias, \n","                                              layer4_bias, layer5_bias, layer6_bias, ]))\n","        \n","        #Test model on validation data\n","        test_encoded_X = forward_pass(nX_val, layer1_weights, layer2_weights, layer3_weights,\n","                                  layer4_weights, layer5_weights, layer6_weights,\n","                                  layer1_bias, layer2_bias, layer3_bias, \n","                                  layer4_bias, layer5_bias, layer6_bias)\n","        test_loss = mean_absolute_error(test_encoded_X, X_val)\n","        te_loss.append(test_loss)\n","        te_acc.append(calculate_accuracy(test_encoded_X, X_val))\n","        \n","        \n","    \n","\n","    plt.title(\"Question 1_3_1\")\n","    plt.plot(te_loss, label=\"Validation Loss\")\n","    plt.plot(tr_loss, label=\"Train Loss\")\n","    plt.legend()\n","    plt.show()    \n","\n","    n=10\n","    plt.figure(figsize=(20, 4))\n","    \n","    for i in range(n):\n","        # Display 10 original validation images\n","        ax1 = plt.subplot(3, n, i + 1)\n","        plt.imshow(test_encoded_X[i].numpy().reshape(28, 28))\n","        plt.gray()\n","        ax1.get_xaxis().set_visible(False)\n","        ax1.get_yaxis().set_visible(False)\n","        \n","        \n","        # Display above images after noise added\n","        ax2 = plt.subplot(3, n, i + 1 + n)\n","        plt.imshow(nX_val[i].numpy().reshape(28, 28))\n","        plt.gray()\n","        ax2.get_xaxis().set_visible(False)\n","        ax2.get_yaxis().set_visible(False)\n","        \n","        \n","        # Display above images after processed through autoencoder\n","        ax3 = plt.subplot(3, n, i + 1 + n*2)\n","        plt.imshow(X_val[i].numpy().reshape(28, 28))\n","        plt.gray()\n","        ax3.get_xaxis().set_visible(False)\n","        ax3.get_yaxis().set_visible(False)\n","        \n","    ax1.title.set_text('Encoded Validation Images, Model Trained for {} iterations'.format(iterations)) \n","    ax2.title.set_text('Noisy Validation Images')\n","    ax3.title.set_text('Original Validation Images')\n","            \n","    plt.show()\n","\n"," \n","  \n","\n","def pre_process():\n","    \"\"\"\n","    Supplied code, from fashion MNIST dataset creates noisy images, returns noisy and original train and test data\n","\n","    Returns\n","    -------\n","    x_train : tf.Variable\n","        Original training images.\n","    x_train_noisy : tf.Variable\n","        Noisy training images.\n","    x_test : tf.Variable\n","        Original test images.\n","    x_test_noisy : tf.Variable\n","        Noisy test images.\n","    \"\"\"\n","\n","    (x_train, _), (x_test, _) = fashion_mnist.load_data()\n","    # Normalize train and test data\n","    x_train = x_train.astype('float32') / 255.\n","    x_test = x_test.astype('float32') / 255.\n","    \n","    # Reshape so that each instance is a linear array of 784 normalized pixel values\n","    x_train = x_train.reshape((len(x_train), 784))\n","    x_test = x_test.reshape((len(x_test), 784))\n","\n","    # Add random noise to the image\n","    noise_factor = 0.2\n","    x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape) \n","    x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape) \n","    \n","    # Clip the resulting values so that they don't fall outside the upper and lower normalized value of 0 and 1\n","    x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\n","    x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)\n","       \n","    return x_train, x_train_noisy, x_test, x_test_noisy\n","    \n","\n","if __name__ == '__main__':\n","    print(\"Local Devices: \\n\", device_lib.list_local_devices())\n","    main()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Iteration 0: Training Loss = 300.09967041015625 Training Accuracy = 0.0\n","Iteration 1: Training Loss = 300.00433349609375 Training Accuracy = 0.0\n","Iteration 2: Training Loss = 299.8702392578125 Training Accuracy = 0.0\n","Iteration 3: Training Loss = 299.6755676269531 Training Accuracy = 0.0\n","Iteration 4: Training Loss = 299.3922119140625 Training Accuracy = 0.0\n","Iteration 5: Training Loss = 298.9767150878906 Training Accuracy = 2.1258502513887834e-08\n","Iteration 6: Training Loss = 298.3673400878906 Training Accuracy = 2.1258502513887834e-08\n","Iteration 7: Training Loss = 297.47369384765625 Training Accuracy = 2.1258502513887834e-08\n","Iteration 8: Training Loss = 296.1731262207031 Training Accuracy = 2.1258502513887834e-08\n","Iteration 9: Training Loss = 294.3092041015625 Training Accuracy = 2.1258502513887834e-08\n","Iteration 10: Training Loss = 291.6784973144531 Training Accuracy = 0.0\n","Iteration 11: Training Loss = 288.00982666015625 Training Accuracy = 2.1258502513887834e-08\n","Iteration 12: Training Loss = 282.9976806640625 Training Accuracy = 6.377550931802034e-08\n","Iteration 13: Training Loss = 276.3295593261719 Training Accuracy = 1.0629251789850969e-07\n","Iteration 14: Training Loss = 267.7776794433594 Training Accuracy = 2.1258502513887834e-08\n","Iteration 15: Training Loss = 257.40679931640625 Training Accuracy = 2.1258502513887834e-08\n","Iteration 16: Training Loss = 245.77963256835938 Training Accuracy = 6.377550931802034e-08\n","Iteration 17: Training Loss = 233.94166564941406 Training Accuracy = 4.251700502777567e-08\n","Iteration 18: Training Loss = 223.06654357910156 Training Accuracy = 6.377550931802034e-08\n","Iteration 19: Training Loss = 214.07553100585938 Training Accuracy = 8.503401005555133e-08\n","Iteration 20: Training Loss = 207.4759521484375 Training Accuracy = 0.0\n","Iteration 21: Training Loss = 202.97796630859375 Training Accuracy = 4.251700502777567e-08\n","Iteration 22: Training Loss = 199.69979858398438 Training Accuracy = 0.0\n","Iteration 23: Training Loss = 196.71893310546875 Training Accuracy = 0.0\n","Iteration 24: Training Loss = 193.4528045654297 Training Accuracy = 2.1258502513887834e-08\n","Iteration 25: Training Loss = 189.7156524658203 Training Accuracy = 2.1258502513887834e-08\n","Iteration 26: Training Loss = 185.63247680664062 Training Accuracy = 0.0\n","Iteration 27: Training Loss = 181.49229431152344 Training Accuracy = 0.0\n","Iteration 28: Training Loss = 177.67373657226562 Training Accuracy = 2.1258502513887834e-08\n","Iteration 29: Training Loss = 174.58380126953125 Training Accuracy = 2.1258502513887834e-08\n","Iteration 30: Training Loss = 172.4341278076172 Training Accuracy = 2.1258502513887834e-08\n","Iteration 31: Training Loss = 171.20477294921875 Training Accuracy = 2.1258502513887834e-08\n","Iteration 32: Training Loss = 170.6766357421875 Training Accuracy = 0.0\n","Iteration 33: Training Loss = 170.40440368652344 Training Accuracy = 2.1258502513887834e-08\n","Iteration 34: Training Loss = 170.1155242919922 Training Accuracy = 4.251700502777567e-08\n","Iteration 35: Training Loss = 169.66287231445312 Training Accuracy = 0.0\n","Iteration 36: Training Loss = 168.95404052734375 Training Accuracy = 2.1258502513887834e-08\n","Iteration 37: Training Loss = 168.00608825683594 Training Accuracy = 0.0\n","Iteration 38: Training Loss = 166.94981384277344 Training Accuracy = 2.1258502513887834e-08\n","Iteration 39: Training Loss = 165.94422912597656 Training Accuracy = 2.1258502513887834e-08\n","Iteration 40: Training Loss = 165.10508728027344 Training Accuracy = 6.377550931802034e-08\n","Iteration 41: Training Loss = 164.48020935058594 Training Accuracy = 2.1258502513887834e-08\n","Iteration 42: Training Loss = 164.07298278808594 Training Accuracy = 0.0\n","Iteration 43: Training Loss = 163.8481903076172 Training Accuracy = 2.1258502513887834e-08\n","Iteration 44: Training Loss = 163.7232666015625 Training Accuracy = 4.251700502777567e-08\n","Iteration 45: Training Loss = 163.61219787597656 Training Accuracy = 2.1258502513887834e-08\n","Iteration 46: Training Loss = 163.4604034423828 Training Accuracy = 2.1258502513887834e-08\n","Iteration 47: Training Loss = 163.26840209960938 Training Accuracy = 6.377550931802034e-08\n","Iteration 48: Training Loss = 163.06573486328125 Training Accuracy = 0.0\n","Iteration 49: Training Loss = 162.8888702392578 Training Accuracy = 2.1258502513887834e-08\n","Iteration 50: Training Loss = 162.75985717773438 Training Accuracy = 2.1258502513887834e-08\n","Iteration 51: Training Loss = 162.67718505859375 Training Accuracy = 2.1258502513887834e-08\n","Iteration 52: Training Loss = 162.62123107910156 Training Accuracy = 2.1258502513887834e-08\n","Iteration 53: Training Loss = 162.56431579589844 Training Accuracy = 4.251700502777567e-08\n","Iteration 54: Training Loss = 162.4823760986328 Training Accuracy = 4.251700502777567e-08\n","Iteration 55: Training Loss = 162.36180114746094 Training Accuracy = 2.1258502513887834e-08\n","Iteration 56: Training Loss = 162.2023162841797 Training Accuracy = 0.0\n","Iteration 57: Training Loss = 162.01535034179688 Training Accuracy = 8.503401005555133e-08\n","Iteration 58: Training Loss = 161.8206329345703 Training Accuracy = 0.0\n","Iteration 59: Training Loss = 161.63841247558594 Training Accuracy = 0.0\n","Iteration 60: Training Loss = 161.4838104248047 Training Accuracy = 0.0\n","Iteration 61: Training Loss = 161.361572265625 Training Accuracy = 4.251700502777567e-08\n","Iteration 62: Training Loss = 161.2664031982422 Training Accuracy = 2.1258502513887834e-08\n","Iteration 63: Training Loss = 161.18630981445312 Training Accuracy = 2.1258502513887834e-08\n","Iteration 64: Training Loss = 161.1085662841797 Training Accuracy = 2.1258502513887834e-08\n","Iteration 65: Training Loss = 161.02572631835938 Training Accuracy = 4.251700502777567e-08\n","Iteration 66: Training Loss = 160.93814086914062 Training Accuracy = 4.251700502777567e-08\n","Iteration 67: Training Loss = 160.85098266601562 Training Accuracy = 2.1258502513887834e-08\n","Iteration 68: Training Loss = 160.76910400390625 Training Accuracy = 2.1258502513887834e-08\n","Iteration 69: Training Loss = 160.6940155029297 Training Accuracy = 0.0\n","Iteration 70: Training Loss = 160.62408447265625 Training Accuracy = 0.0\n","Iteration 71: Training Loss = 160.55726623535156 Training Accuracy = 0.0\n","Iteration 72: Training Loss = 160.49319458007812 Training Accuracy = 4.251700502777567e-08\n","Iteration 73: Training Loss = 160.4325714111328 Training Accuracy = 6.377550931802034e-08\n","Iteration 74: Training Loss = 160.37457275390625 Training Accuracy = 8.503401005555133e-08\n","Iteration 75: Training Loss = 160.3161163330078 Training Accuracy = 2.1258502513887834e-08\n","Iteration 76: Training Loss = 160.2549285888672 Training Accuracy = 8.503401005555133e-08\n","Iteration 77: Training Loss = 160.191650390625 Training Accuracy = 4.251700502777567e-08\n","Iteration 78: Training Loss = 160.1292266845703 Training Accuracy = 2.1258502513887834e-08\n","Iteration 79: Training Loss = 160.06932067871094 Training Accuracy = 0.0\n","Iteration 80: Training Loss = 160.0097198486328 Training Accuracy = 4.251700502777567e-08\n","Iteration 81: Training Loss = 159.9469451904297 Training Accuracy = 6.377550931802034e-08\n","Iteration 82: Training Loss = 159.87986755371094 Training Accuracy = 4.251700502777567e-08\n","Iteration 83: Training Loss = 159.8099822998047 Training Accuracy = 2.1258502513887834e-08\n","Iteration 84: Training Loss = 159.73963928222656 Training Accuracy = 6.377550931802034e-08\n","Iteration 85: Training Loss = 159.66835021972656 Training Accuracy = 6.377550931802034e-08\n","Iteration 86: Training Loss = 159.5932159423828 Training Accuracy = 4.251700502777567e-08\n","Iteration 87: Training Loss = 159.51133728027344 Training Accuracy = 4.251700502777567e-08\n","Iteration 88: Training Loss = 159.4230499267578 Training Accuracy = 8.503401005555133e-08\n","Iteration 89: Training Loss = 159.33033752441406 Training Accuracy = 8.503401005555133e-08\n","Iteration 90: Training Loss = 159.23394775390625 Training Accuracy = 8.503401005555133e-08\n","Iteration 91: Training Loss = 159.13119506835938 Training Accuracy = 2.1258502513887834e-08\n","Iteration 92: Training Loss = 159.01791381835938 Training Accuracy = 4.251700502777567e-08\n","Iteration 93: Training Loss = 158.89210510253906 Training Accuracy = 6.377550931802034e-08\n","Iteration 94: Training Loss = 158.74949645996094 Training Accuracy = 2.1258502513887834e-08\n","Iteration 95: Training Loss = 158.58694458007812 Training Accuracy = 4.251700502777567e-08\n","Iteration 96: Training Loss = 158.40280151367188 Training Accuracy = 6.377550931802034e-08\n","Iteration 97: Training Loss = 158.20899963378906 Training Accuracy = 2.1258502513887834e-08\n","Iteration 98: Training Loss = 157.99270629882812 Training Accuracy = 0.0\n","Iteration 99: Training Loss = 157.74688720703125 Training Accuracy = 0.0\n","Iteration 100: Training Loss = 157.46961975097656 Training Accuracy = 8.503401005555133e-08\n","Iteration 101: Training Loss = 157.15072631835938 Training Accuracy = 0.0\n","Iteration 102: Training Loss = 156.786865234375 Training Accuracy = 2.1258502513887834e-08\n","Iteration 103: Training Loss = 156.37062072753906 Training Accuracy = 2.1258502513887834e-08\n","Iteration 104: Training Loss = 155.89210510253906 Training Accuracy = 0.0\n","Iteration 105: Training Loss = 155.34176635742188 Training Accuracy = 0.0\n","Iteration 106: Training Loss = 154.71209716796875 Training Accuracy = 6.377550931802034e-08\n","Iteration 107: Training Loss = 154.00059509277344 Training Accuracy = 2.1258502513887834e-08\n","Iteration 108: Training Loss = 153.20870971679688 Training Accuracy = 2.1258502513887834e-08\n","Iteration 109: Training Loss = 152.34841918945312 Training Accuracy = 0.0\n","Iteration 110: Training Loss = 151.4446563720703 Training Accuracy = 4.251700502777567e-08\n","Iteration 111: Training Loss = 150.53536987304688 Training Accuracy = 0.0\n","Iteration 112: Training Loss = 149.66543579101562 Training Accuracy = 6.377550931802034e-08\n","Iteration 113: Training Loss = 148.87098693847656 Training Accuracy = 6.377550931802034e-08\n","Iteration 114: Training Loss = 148.1539306640625 Training Accuracy = 0.0\n","Iteration 115: Training Loss = 147.54104614257812 Training Accuracy = 0.0\n","Iteration 116: Training Loss = 147.01153564453125 Training Accuracy = 2.1258502513887834e-08\n","Iteration 117: Training Loss = 146.4945831298828 Training Accuracy = 2.1258502513887834e-08\n","Iteration 118: Training Loss = 145.98240661621094 Training Accuracy = 4.251700502777567e-08\n","Iteration 119: Training Loss = 145.4625701904297 Training Accuracy = 2.1258502513887834e-08\n","Iteration 120: Training Loss = 144.8679962158203 Training Accuracy = 4.251700502777567e-08\n","Iteration 121: Training Loss = 144.20326232910156 Training Accuracy = 0.0\n","Iteration 122: Training Loss = 143.52685546875 Training Accuracy = 6.377550931802034e-08\n","Iteration 123: Training Loss = 142.88388061523438 Training Accuracy = 4.251700502777567e-08\n","Iteration 124: Training Loss = 142.26853942871094 Training Accuracy = 0.0\n","Iteration 125: Training Loss = 141.66839599609375 Training Accuracy = 8.503401005555133e-08\n","Iteration 126: Training Loss = 141.09323120117188 Training Accuracy = 1.0629251789850969e-07\n","Iteration 127: Training Loss = 140.60072326660156 Training Accuracy = 2.1258502513887834e-08\n","Iteration 128: Training Loss = 140.19894409179688 Training Accuracy = 4.251700502777567e-08\n","Iteration 129: Training Loss = 139.8560028076172 Training Accuracy = 2.1258502513887834e-08\n","Iteration 130: Training Loss = 139.55020141601562 Training Accuracy = 2.1258502513887834e-08\n","Iteration 131: Training Loss = 139.2455596923828 Training Accuracy = 4.251700502777567e-08\n","Iteration 132: Training Loss = 138.98019409179688 Training Accuracy = 6.377550931802034e-08\n","Iteration 133: Training Loss = 138.80198669433594 Training Accuracy = 0.0\n","Iteration 134: Training Loss = 138.74794006347656 Training Accuracy = 6.377550931802034e-08\n","Iteration 135: Training Loss = 138.6483612060547 Training Accuracy = 2.1258502513887834e-08\n","Iteration 136: Training Loss = 137.9947967529297 Training Accuracy = 2.1258502513887834e-08\n","Iteration 137: Training Loss = 137.7351531982422 Training Accuracy = 2.1258502513887834e-08\n","Iteration 138: Training Loss = 137.78074645996094 Training Accuracy = 0.0\n","Iteration 139: Training Loss = 137.27101135253906 Training Accuracy = 0.0\n","Iteration 140: Training Loss = 136.85638427734375 Training Accuracy = 4.251700502777567e-08\n","Iteration 141: Training Loss = 136.78016662597656 Training Accuracy = 2.1258502513887834e-08\n","Iteration 142: Training Loss = 136.4990234375 Training Accuracy = 0.0\n","Iteration 143: Training Loss = 136.1346893310547 Training Accuracy = 4.251700502777567e-08\n","Iteration 144: Training Loss = 136.0179901123047 Training Accuracy = 4.251700502777567e-08\n","Iteration 145: Training Loss = 135.96673583984375 Training Accuracy = 2.1258502513887834e-08\n","Iteration 146: Training Loss = 135.78021240234375 Training Accuracy = 6.377550931802034e-08\n","Iteration 147: Training Loss = 135.58448791503906 Training Accuracy = 2.1258502513887834e-08\n","Iteration 148: Training Loss = 135.51559448242188 Training Accuracy = 2.1258502513887834e-08\n","Iteration 149: Training Loss = 135.4239959716797 Training Accuracy = 0.0\n","Iteration 150: Training Loss = 135.11380004882812 Training Accuracy = 6.377550931802034e-08\n","Iteration 151: Training Loss = 134.87461853027344 Training Accuracy = 6.377550931802034e-08\n","Iteration 152: Training Loss = 134.74609375 Training Accuracy = 2.1258502513887834e-08\n","Iteration 153: Training Loss = 134.55982971191406 Training Accuracy = 4.251700502777567e-08\n","Iteration 154: Training Loss = 134.3268280029297 Training Accuracy = 6.377550931802034e-08\n","Iteration 155: Training Loss = 134.1367645263672 Training Accuracy = 4.251700502777567e-08\n","Iteration 156: Training Loss = 134.02577209472656 Training Accuracy = 0.0\n","Iteration 157: Training Loss = 133.89517211914062 Training Accuracy = 4.251700502777567e-08\n","Iteration 158: Training Loss = 133.6975555419922 Training Accuracy = 6.377550931802034e-08\n","Iteration 159: Training Loss = 133.54336547851562 Training Accuracy = 4.251700502777567e-08\n","Iteration 160: Training Loss = 133.42868041992188 Training Accuracy = 0.0\n","Iteration 161: Training Loss = 133.27120971679688 Training Accuracy = 4.251700502777567e-08\n","Iteration 162: Training Loss = 133.0966796875 Training Accuracy = 0.0\n","Iteration 163: Training Loss = 132.95065307617188 Training Accuracy = 8.503401005555133e-08\n","Iteration 164: Training Loss = 132.8354949951172 Training Accuracy = 8.503401005555133e-08\n","Iteration 165: Training Loss = 132.7173614501953 Training Accuracy = 2.1258502513887834e-08\n","Iteration 166: Training Loss = 132.57481384277344 Training Accuracy = 2.1258502513887834e-08\n","Iteration 167: Training Loss = 132.438232421875 Training Accuracy = 1.0629251789850969e-07\n","Iteration 168: Training Loss = 132.3213348388672 Training Accuracy = 6.377550931802034e-08\n","Iteration 169: Training Loss = 132.2094268798828 Training Accuracy = 2.1258502513887834e-08\n","Iteration 170: Training Loss = 132.085205078125 Training Accuracy = 8.503401005555133e-08\n","Iteration 171: Training Loss = 131.9400177001953 Training Accuracy = 0.0\n","Iteration 172: Training Loss = 131.7946319580078 Training Accuracy = 8.503401005555133e-08\n","Iteration 173: Training Loss = 131.66224670410156 Training Accuracy = 2.1258502513887834e-08\n","Iteration 174: Training Loss = 131.5458221435547 Training Accuracy = 4.251700502777567e-08\n","Iteration 175: Training Loss = 131.43978881835938 Training Accuracy = 2.1258502513887834e-08\n","Iteration 176: Training Loss = 131.3362579345703 Training Accuracy = 2.1258502513887834e-08\n","Iteration 177: Training Loss = 131.23703002929688 Training Accuracy = 2.1258502513887834e-08\n","Iteration 178: Training Loss = 131.13351440429688 Training Accuracy = 6.377550931802034e-08\n","Iteration 179: Training Loss = 131.03460693359375 Training Accuracy = 2.1258502513887834e-08\n","Iteration 180: Training Loss = 130.93836975097656 Training Accuracy = 4.251700502777567e-08\n","Iteration 181: Training Loss = 130.85269165039062 Training Accuracy = 8.503401005555133e-08\n","Iteration 182: Training Loss = 130.77464294433594 Training Accuracy = 4.251700502777567e-08\n","Iteration 183: Training Loss = 130.72848510742188 Training Accuracy = 2.1258502513887834e-08\n","Iteration 184: Training Loss = 130.69058227539062 Training Accuracy = 6.377550931802034e-08\n","Iteration 185: Training Loss = 130.73423767089844 Training Accuracy = 0.0\n","Iteration 186: Training Loss = 130.6115264892578 Training Accuracy = 0.0\n","Iteration 187: Training Loss = 130.51058959960938 Training Accuracy = 2.1258502513887834e-08\n","Iteration 188: Training Loss = 130.2726287841797 Training Accuracy = 4.251700502777567e-08\n","Iteration 189: Training Loss = 130.14801025390625 Training Accuracy = 2.1258502513887834e-08\n","Iteration 190: Training Loss = 130.13442993164062 Training Accuracy = 2.1258502513887834e-08\n","Iteration 191: Training Loss = 130.1150360107422 Training Accuracy = 4.251700502777567e-08\n","Iteration 192: Training Loss = 130.05589294433594 Training Accuracy = 8.503401005555133e-08\n","Iteration 193: Training Loss = 129.87586975097656 Training Accuracy = 8.503401005555133e-08\n","Iteration 194: Training Loss = 129.75816345214844 Training Accuracy = 6.377550931802034e-08\n","Iteration 195: Training Loss = 129.71841430664062 Training Accuracy = 6.377550931802034e-08\n","Iteration 196: Training Loss = 129.67750549316406 Training Accuracy = 6.377550931802034e-08\n","Iteration 197: Training Loss = 129.60400390625 Training Accuracy = 6.377550931802034e-08\n","Iteration 198: Training Loss = 129.47000122070312 Training Accuracy = 8.503401005555133e-08\n","Iteration 199: Training Loss = 129.3795623779297 Training Accuracy = 4.251700502777567e-08\n","Iteration 200: Training Loss = 129.33807373046875 Training Accuracy = 2.1258502513887834e-08\n","Iteration 201: Training Loss = 129.2878875732422 Training Accuracy = 4.251700502777567e-08\n","Iteration 202: Training Loss = 129.21182250976562 Training Accuracy = 1.0629251789850969e-07\n","Iteration 203: Training Loss = 129.1114044189453 Training Accuracy = 0.0\n","Iteration 204: Training Loss = 129.0390625 Training Accuracy = 2.1258502513887834e-08\n","Iteration 205: Training Loss = 128.9948272705078 Training Accuracy = 6.377550931802034e-08\n","Iteration 206: Training Loss = 128.94100952148438 Training Accuracy = 2.1258502513887834e-08\n","Iteration 207: Training Loss = 128.8677520751953 Training Accuracy = 1.0629251789850969e-07\n","Iteration 208: Training Loss = 128.78175354003906 Training Accuracy = 6.377550931802034e-08\n","Iteration 209: Training Loss = 128.71084594726562 Training Accuracy = 2.1258502513887834e-08\n","Iteration 210: Training Loss = 128.65843200683594 Training Accuracy = 0.0\n","Iteration 211: Training Loss = 128.61114501953125 Training Accuracy = 2.1258502513887834e-08\n","Iteration 212: Training Loss = 128.53822326660156 Training Accuracy = 2.1258502513887834e-08\n","Iteration 213: Training Loss = 128.4564971923828 Training Accuracy = 0.0\n","Iteration 214: Training Loss = 128.3916015625 Training Accuracy = 0.0\n","Iteration 215: Training Loss = 128.3398895263672 Training Accuracy = 4.251700502777567e-08\n","Iteration 216: Training Loss = 128.28623962402344 Training Accuracy = 8.503401005555133e-08\n","Iteration 217: Training Loss = 128.2245635986328 Training Accuracy = 1.7006802011110267e-07\n","Iteration 218: Training Loss = 128.15565490722656 Training Accuracy = 4.251700502777567e-08\n","Iteration 219: Training Loss = 128.0926513671875 Training Accuracy = 0.0\n","Iteration 220: Training Loss = 128.038330078125 Training Accuracy = 4.251700502777567e-08\n","Iteration 221: Training Loss = 127.98809814453125 Training Accuracy = 2.1258502513887834e-08\n","Iteration 222: Training Loss = 127.9378662109375 Training Accuracy = 0.0\n","Iteration 223: Training Loss = 127.88629913330078 Training Accuracy = 1.0629251789850969e-07\n","Iteration 224: Training Loss = 127.83500671386719 Training Accuracy = 0.0\n","Iteration 225: Training Loss = 127.7813720703125 Training Accuracy = 2.1258502513887834e-08\n","Iteration 226: Training Loss = 127.72571563720703 Training Accuracy = 0.0\n","Iteration 227: Training Loss = 127.67106628417969 Training Accuracy = 2.1258502513887834e-08\n","Iteration 228: Training Loss = 127.62053680419922 Training Accuracy = 2.1258502513887834e-08\n","Iteration 229: Training Loss = 127.57325744628906 Training Accuracy = 2.1258502513887834e-08\n","Iteration 230: Training Loss = 127.5238265991211 Training Accuracy = 8.503401005555133e-08\n","Iteration 231: Training Loss = 127.47189331054688 Training Accuracy = 2.1258502513887834e-08\n","Iteration 232: Training Loss = 127.41828155517578 Training Accuracy = 2.1258502513887834e-08\n","Iteration 233: Training Loss = 127.36735534667969 Training Accuracy = 2.1258502513887834e-08\n","Iteration 234: Training Loss = 127.31827545166016 Training Accuracy = 0.0\n","Iteration 235: Training Loss = 127.27275085449219 Training Accuracy = 0.0\n","Iteration 236: Training Loss = 127.22782135009766 Training Accuracy = 4.251700502777567e-08\n","Iteration 237: Training Loss = 127.1790771484375 Training Accuracy = 2.1258502513887834e-08\n","Iteration 238: Training Loss = 127.12950897216797 Training Accuracy = 4.251700502777567e-08\n","Iteration 239: Training Loss = 127.07788848876953 Training Accuracy = 2.1258502513887834e-08\n","Iteration 240: Training Loss = 127.02394104003906 Training Accuracy = 2.1258502513887834e-08\n","Iteration 241: Training Loss = 126.97047424316406 Training Accuracy = 1.2755101863604068e-07\n","Iteration 242: Training Loss = 126.91535949707031 Training Accuracy = 2.1258502513887834e-08\n","Iteration 243: Training Loss = 126.85798645019531 Training Accuracy = 4.251700502777567e-08\n","Iteration 244: Training Loss = 126.79867553710938 Training Accuracy = 2.1258502513887834e-08\n","Iteration 245: Training Loss = 126.7368392944336 Training Accuracy = 4.251700502777567e-08\n","Iteration 246: Training Loss = 126.67790222167969 Training Accuracy = 4.251700502777567e-08\n","Iteration 247: Training Loss = 126.62153625488281 Training Accuracy = 0.0\n","Iteration 248: Training Loss = 126.55870819091797 Training Accuracy = 0.0\n","Iteration 249: Training Loss = 126.50069427490234 Training Accuracy = 6.377550931802034e-08\n","Iteration 250: Training Loss = 126.45198059082031 Training Accuracy = 6.377550931802034e-08\n","Iteration 251: Training Loss = 126.39868927001953 Training Accuracy = 6.377550931802034e-08\n","Iteration 252: Training Loss = 126.3404312133789 Training Accuracy = 2.1258502513887834e-08\n","Iteration 253: Training Loss = 126.28761291503906 Training Accuracy = 6.377550931802034e-08\n","Iteration 254: Training Loss = 126.23316955566406 Training Accuracy = 4.251700502777567e-08\n","Iteration 255: Training Loss = 126.172119140625 Training Accuracy = 2.1258502513887834e-08\n","Iteration 256: Training Loss = 126.11370086669922 Training Accuracy = 2.1258502513887834e-08\n","Iteration 257: Training Loss = 126.05718994140625 Training Accuracy = 4.251700502777567e-08\n","Iteration 258: Training Loss = 125.9962158203125 Training Accuracy = 1.2755101863604068e-07\n","Iteration 259: Training Loss = 125.93315887451172 Training Accuracy = 6.377550931802034e-08\n","Iteration 260: Training Loss = 125.87284088134766 Training Accuracy = 4.251700502777567e-08\n","Iteration 261: Training Loss = 125.81635284423828 Training Accuracy = 6.377550931802034e-08\n","Iteration 262: Training Loss = 125.75936126708984 Training Accuracy = 2.1258502513887834e-08\n","Iteration 263: Training Loss = 125.69922637939453 Training Accuracy = 4.251700502777567e-08\n","Iteration 264: Training Loss = 125.6374740600586 Training Accuracy = 0.0\n","Iteration 265: Training Loss = 125.58011627197266 Training Accuracy = 2.1258502513887834e-08\n","Iteration 266: Training Loss = 125.52330780029297 Training Accuracy = 6.377550931802034e-08\n","Iteration 267: Training Loss = 125.4644775390625 Training Accuracy = 6.377550931802034e-08\n","Iteration 268: Training Loss = 125.40544128417969 Training Accuracy = 2.1258502513887834e-08\n","Iteration 269: Training Loss = 125.34991455078125 Training Accuracy = 1.0629251789850969e-07\n","Iteration 270: Training Loss = 125.30226135253906 Training Accuracy = 8.503401005555133e-08\n","Iteration 271: Training Loss = 125.25161743164062 Training Accuracy = 0.0\n","Iteration 272: Training Loss = 125.22077178955078 Training Accuracy = 2.1258502513887834e-08\n","Iteration 273: Training Loss = 125.16435241699219 Training Accuracy = 6.377550931802034e-08\n","Iteration 274: Training Loss = 125.15298461914062 Training Accuracy = 2.1258502513887834e-08\n","Iteration 275: Training Loss = 125.0439682006836 Training Accuracy = 1.0629251789850969e-07\n","Iteration 276: Training Loss = 124.9364242553711 Training Accuracy = 1.2755101863604068e-07\n","Iteration 277: Training Loss = 124.8024673461914 Training Accuracy = 8.503401005555133e-08\n","Iteration 278: Training Loss = 124.71784210205078 Training Accuracy = 6.377550931802034e-08\n","Iteration 279: Training Loss = 124.6671142578125 Training Accuracy = 0.0\n","Iteration 280: Training Loss = 124.6277084350586 Training Accuracy = 2.1258502513887834e-08\n","Iteration 281: Training Loss = 124.59476470947266 Training Accuracy = 4.251700502777567e-08\n","Iteration 282: Training Loss = 124.5206527709961 Training Accuracy = 8.503401005555133e-08\n","Iteration 283: Training Loss = 124.43968200683594 Training Accuracy = 1.0629251789850969e-07\n","Iteration 284: Training Loss = 124.3293685913086 Training Accuracy = 4.251700502777567e-08\n","Iteration 285: Training Loss = 124.23666381835938 Training Accuracy = 1.2755101863604068e-07\n","Iteration 286: Training Loss = 124.16861724853516 Training Accuracy = 1.0629251789850969e-07\n","Iteration 287: Training Loss = 124.11091613769531 Training Accuracy = 4.251700502777567e-08\n","Iteration 288: Training Loss = 124.05787658691406 Training Accuracy = 2.1258502513887834e-08\n","Iteration 289: Training Loss = 123.99530029296875 Training Accuracy = 4.251700502777567e-08\n","Iteration 290: Training Loss = 123.93289947509766 Training Accuracy = 2.1258502513887834e-08\n","Iteration 291: Training Loss = 123.83552551269531 Training Accuracy = 4.251700502777567e-08\n","Iteration 292: Training Loss = 123.7403335571289 Training Accuracy = 4.251700502777567e-08\n","Iteration 293: Training Loss = 123.62852478027344 Training Accuracy = 0.0\n","Iteration 294: Training Loss = 123.52279663085938 Training Accuracy = 6.377550931802034e-08\n","Iteration 295: Training Loss = 123.42495727539062 Training Accuracy = 6.377550931802034e-08\n","Iteration 296: Training Loss = 123.32553100585938 Training Accuracy = 0.0\n","Iteration 297: Training Loss = 123.22908020019531 Training Accuracy = 2.1258502513887834e-08\n","Iteration 298: Training Loss = 123.1352767944336 Training Accuracy = 0.0\n","Iteration 299: Training Loss = 123.0374526977539 Training Accuracy = 4.251700502777567e-08\n","Iteration 300: Training Loss = 122.9452896118164 Training Accuracy = 2.1258502513887834e-08\n","Iteration 301: Training Loss = 122.89570617675781 Training Accuracy = 6.377550931802034e-08\n","Iteration 302: Training Loss = 122.88835144042969 Training Accuracy = 2.1258502513887834e-08\n","Iteration 303: Training Loss = 123.0086441040039 Training Accuracy = 4.251700502777567e-08\n","Iteration 304: Training Loss = 122.81537628173828 Training Accuracy = 8.503401005555133e-08\n","Iteration 305: Training Loss = 122.64515686035156 Training Accuracy = 6.377550931802034e-08\n","Iteration 306: Training Loss = 122.23731994628906 Training Accuracy = 8.503401005555133e-08\n","Iteration 307: Training Loss = 122.11360931396484 Training Accuracy = 8.503401005555133e-08\n","Iteration 308: Training Loss = 122.16949462890625 Training Accuracy = 6.377550931802034e-08\n","Iteration 309: Training Loss = 122.01875305175781 Training Accuracy = 2.1258502513887834e-08\n","Iteration 310: Training Loss = 121.77418518066406 Training Accuracy = 2.1258502513887834e-08\n","Iteration 311: Training Loss = 121.43871307373047 Training Accuracy = 8.503401005555133e-08\n","Iteration 312: Training Loss = 121.29067993164062 Training Accuracy = 1.0629251789850969e-07\n","Iteration 313: Training Loss = 121.25428009033203 Training Accuracy = 1.0629251789850969e-07\n","Iteration 314: Training Loss = 121.01026153564453 Training Accuracy = 2.1258502513887834e-08\n","Iteration 315: Training Loss = 120.71126556396484 Training Accuracy = 2.1258502513887834e-08\n","Iteration 316: Training Loss = 120.43049621582031 Training Accuracy = 0.0\n","Iteration 317: Training Loss = 120.24846649169922 Training Accuracy = 8.503401005555133e-08\n","Iteration 318: Training Loss = 120.08536529541016 Training Accuracy = 8.503401005555133e-08\n","Iteration 319: Training Loss = 119.80221557617188 Training Accuracy = 4.251700502777567e-08\n","Iteration 320: Training Loss = 119.47884368896484 Training Accuracy = 0.0\n","Iteration 321: Training Loss = 119.0872802734375 Training Accuracy = 8.503401005555133e-08\n","Iteration 322: Training Loss = 118.7003173828125 Training Accuracy = 2.1258502513887834e-08\n","Iteration 323: Training Loss = 118.34477233886719 Training Accuracy = 4.251700502777567e-08\n","Iteration 324: Training Loss = 117.9865493774414 Training Accuracy = 2.1258502513887834e-08\n","Iteration 325: Training Loss = 117.67906951904297 Training Accuracy = 6.377550931802034e-08\n","Iteration 326: Training Loss = 117.59481048583984 Training Accuracy = 1.0629251789850969e-07\n","Iteration 327: Training Loss = 117.9971923828125 Training Accuracy = 1.0629251789850969e-07\n","Iteration 328: Training Loss = 116.97863006591797 Training Accuracy = 0.0\n","Iteration 329: Training Loss = 116.02421569824219 Training Accuracy = 0.0\n","Iteration 330: Training Loss = 115.47308349609375 Training Accuracy = 6.377550931802034e-08\n","Iteration 331: Training Loss = 115.56980895996094 Training Accuracy = 2.1258502513887834e-08\n","Iteration 332: Training Loss = 115.82737731933594 Training Accuracy = 2.1258502513887834e-08\n","Iteration 333: Training Loss = 114.49931335449219 Training Accuracy = 2.1258502513887834e-08\n","Iteration 334: Training Loss = 113.93983459472656 Training Accuracy = 6.377550931802034e-08\n","Iteration 335: Training Loss = 114.12295532226562 Training Accuracy = 2.1258502513887834e-08\n","Iteration 336: Training Loss = 113.44599151611328 Training Accuracy = 8.503401005555133e-08\n","Iteration 337: Training Loss = 112.63520812988281 Training Accuracy = 0.0\n","Iteration 338: Training Loss = 112.44587707519531 Training Accuracy = 1.0629251789850969e-07\n","Iteration 339: Training Loss = 112.15130615234375 Training Accuracy = 1.4880951937357167e-07\n","Iteration 340: Training Loss = 111.6053466796875 Training Accuracy = 2.1258502513887834e-08\n","Iteration 341: Training Loss = 111.04434204101562 Training Accuracy = 8.503401005555133e-08\n","Iteration 342: Training Loss = 110.92691040039062 Training Accuracy = 6.377550931802034e-08\n","Iteration 343: Training Loss = 110.6526870727539 Training Accuracy = 1.0629251789850969e-07\n","Iteration 344: Training Loss = 110.04273223876953 Training Accuracy = 6.377550931802034e-08\n","Iteration 345: Training Loss = 109.71697235107422 Training Accuracy = 6.377550931802034e-08\n","Iteration 346: Training Loss = 109.60362243652344 Training Accuracy = 6.377550931802034e-08\n","Iteration 347: Training Loss = 109.03323364257812 Training Accuracy = 1.0629251789850969e-07\n","Iteration 348: Training Loss = 108.68191528320312 Training Accuracy = 8.503401005555133e-08\n","Iteration 349: Training Loss = 108.51814270019531 Training Accuracy = 8.503401005555133e-08\n","Iteration 350: Training Loss = 108.15824890136719 Training Accuracy = 1.9132653505948838e-07\n","Iteration 351: Training Loss = 107.76969909667969 Training Accuracy = 1.2755101863604068e-07\n","Iteration 352: Training Loss = 107.49764251708984 Training Accuracy = 3.4013604022220534e-07\n","Iteration 353: Training Loss = 107.32976531982422 Training Accuracy = 6.802720804444107e-07\n","Iteration 354: Training Loss = 107.05194854736328 Training Accuracy = 5.527210760192247e-07\n","Iteration 355: Training Loss = 106.71841430664062 Training Accuracy = 7.653061402379535e-07\n","Iteration 356: Training Loss = 106.43570709228516 Training Accuracy = 1.1267006811976898e-06\n","Iteration 357: Training Loss = 106.23121643066406 Training Accuracy = 1.1479592103569303e-06\n","Iteration 358: Training Loss = 105.9289321899414 Training Accuracy = 1.7431972310077981e-06\n","Iteration 359: Training Loss = 105.5541763305664 Training Accuracy = 1.7644557601670385e-06\n","Iteration 360: Training Loss = 105.2520523071289 Training Accuracy = 2.317176949873101e-06\n","Iteration 361: Training Loss = 104.98750305175781 Training Accuracy = 3.1037413918966195e-06\n","Iteration 362: Training Loss = 104.73202514648438 Training Accuracy = 3.2100340376928216e-06\n","Iteration 363: Training Loss = 104.35113525390625 Training Accuracy = 5.569727818510728e-06\n","Iteration 364: Training Loss = 103.88369750976562 Training Accuracy = 4.974489911546698e-06\n","Iteration 365: Training Loss = 103.40608215332031 Training Accuracy = 6.568877324752975e-06\n","Iteration 366: Training Loss = 102.92237854003906 Training Accuracy = 8.035714017751161e-06\n","Iteration 367: Training Loss = 102.43214416503906 Training Accuracy = 8.971088391263038e-06\n","Iteration 368: Training Loss = 101.8609390258789 Training Accuracy = 1.1798469131463207e-05\n","Iteration 369: Training Loss = 101.29178619384766 Training Accuracy = 1.1713435014826246e-05\n","Iteration 370: Training Loss = 100.73854064941406 Training Accuracy = 1.639030597289093e-05\n","Iteration 371: Training Loss = 100.20005798339844 Training Accuracy = 1.6666666851961054e-05\n","Iteration 372: Training Loss = 99.63883209228516 Training Accuracy = 1.9090135538135655e-05\n","Iteration 373: Training Loss = 99.07648468017578 Training Accuracy = 2.055697223113384e-05\n","Iteration 374: Training Loss = 98.4967269897461 Training Accuracy = 1.9494047592161223e-05\n","Iteration 375: Training Loss = 97.87566375732422 Training Accuracy = 2.334183591301553e-05\n","Iteration 376: Training Loss = 97.3892822265625 Training Accuracy = 2.104591840179637e-05\n","Iteration 377: Training Loss = 96.94959259033203 Training Accuracy = 2.842261892510578e-05\n","Iteration 378: Training Loss = 96.45487976074219 Training Accuracy = 2.62755092990119e-05\n","Iteration 379: Training Loss = 95.90156555175781 Training Accuracy = 3.62244900316e-05\n","Iteration 380: Training Loss = 95.35530090332031 Training Accuracy = 2.846513598342426e-05\n","Iteration 381: Training Loss = 94.77583312988281 Training Accuracy = 3.8477890484500676e-05\n","Iteration 382: Training Loss = 94.2573471069336 Training Accuracy = 3.224915053579025e-05\n","Iteration 383: Training Loss = 93.76195526123047 Training Accuracy = 4.4409014662960544e-05\n","Iteration 384: Training Loss = 93.26671600341797 Training Accuracy = 4.232567880535498e-05\n","Iteration 385: Training Loss = 92.74851989746094 Training Accuracy = 5.529336704057641e-05\n","Iteration 386: Training Loss = 92.30241394042969 Training Accuracy = 5.042516932007857e-05\n","Iteration 387: Training Loss = 91.88500213623047 Training Accuracy = 6.405187014024705e-05\n","Iteration 388: Training Loss = 91.45771789550781 Training Accuracy = 5.969387711957097e-05\n","Iteration 389: Training Loss = 91.08775329589844 Training Accuracy = 7.657313108211383e-05\n","Iteration 390: Training Loss = 90.7609634399414 Training Accuracy = 6.849489727756009e-05\n","Iteration 391: Training Loss = 90.46800231933594 Training Accuracy = 9.610969573259354e-05\n","Iteration 392: Training Loss = 90.25585174560547 Training Accuracy = 7.531887968070805e-05\n","Iteration 393: Training Loss = 90.03197479248047 Training Accuracy = 0.00013601190585177392\n","Iteration 394: Training Loss = 89.94053649902344 Training Accuracy = 8.968962356448174e-05\n","Iteration 395: Training Loss = 89.44298553466797 Training Accuracy = 0.00016073553706519306\n","Iteration 396: Training Loss = 88.95829010009766 Training Accuracy = 0.00012210884597152472\n","Iteration 397: Training Loss = 88.57788848876953 Training Accuracy = 0.0001590773754287511\n","Iteration 398: Training Loss = 88.42845153808594 Training Accuracy = 0.0001955994957825169\n","Iteration 399: Training Loss = 88.45092010498047 Training Accuracy = 0.00017198128625750542\n","Iteration 400: Training Loss = 88.32140350341797 Training Accuracy = 0.0002916879311669618\n","Iteration 401: Training Loss = 88.15689086914062 Training Accuracy = 0.00021196853776928037\n","Iteration 402: Training Loss = 87.723388671875 Training Accuracy = 0.0003257653152104467\n","Iteration 403: Training Loss = 87.39405822753906 Training Accuracy = 0.00028994472813792527\n","Iteration 404: Training Loss = 87.23054504394531 Training Accuracy = 0.00031056549050845206\n","Iteration 405: Training Loss = 87.19112396240234 Training Accuracy = 0.00039096514228731394\n","Iteration 406: Training Loss = 87.21028900146484 Training Accuracy = 0.00032699829898774624\n","Iteration 407: Training Loss = 87.08128356933594 Training Accuracy = 0.0005098001565784216\n","Iteration 408: Training Loss = 86.93914031982422 Training Accuracy = 0.0003820578276645392\n","Iteration 409: Training Loss = 86.60067749023438 Training Accuracy = 0.0005453869234770536\n","Iteration 410: Training Loss = 86.3321533203125 Training Accuracy = 0.00046651787124574184\n","Iteration 411: Training Loss = 86.13339233398438 Training Accuracy = 0.0005199404549784958\n","Iteration 412: Training Loss = 86.0177001953125 Training Accuracy = 0.0005822491366416216\n","Iteration 413: Training Loss = 85.97715759277344 Training Accuracy = 0.0005303358775563538\n","Iteration 414: Training Loss = 85.95142364501953 Training Accuracy = 0.0007037202594801784\n","Iteration 415: Training Loss = 86.00069427490234 Training Accuracy = 0.0005207483191043139\n","Iteration 416: Training Loss = 85.85404205322266 Training Accuracy = 0.0007693239604122937\n","Iteration 417: Training Loss = 85.73237609863281 Training Accuracy = 0.0005539115518331528\n","Iteration 418: Training Loss = 85.39088439941406 Training Accuracy = 0.0007742347079329193\n","Iteration 419: Training Loss = 85.12496185302734 Training Accuracy = 0.0006649447022937238\n","Iteration 420: Training Loss = 84.94046783447266 Training Accuracy = 0.0007406462682411075\n","Iteration 421: Training Loss = 84.85211181640625 Training Accuracy = 0.0008018494700081646\n","Iteration 422: Training Loss = 84.83634185791016 Training Accuracy = 0.00073231291025877\n","Iteration 423: Training Loss = 84.81954956054688 Training Accuracy = 0.0009649234707467258\n","Iteration 424: Training Loss = 84.85997009277344 Training Accuracy = 0.0007526148110628128\n","Iteration 425: Training Loss = 84.71483612060547 Training Accuracy = 0.0010763817699626088\n","Iteration 426: Training Loss = 84.59455871582031 Training Accuracy = 0.0008223001495935023\n","Iteration 427: Training Loss = 84.30772399902344 Training Accuracy = 0.0010959396604448557\n","Iteration 428: Training Loss = 84.08466339111328 Training Accuracy = 0.0009521471220068634\n","Iteration 429: Training Loss = 83.9137191772461 Training Accuracy = 0.0010628189193084836\n","Iteration 430: Training Loss = 83.81988525390625 Training Accuracy = 0.0011191964149475098\n","Iteration 431: Training Loss = 83.78606414794922 Training Accuracy = 0.0010743197053670883\n","Iteration 432: Training Loss = 83.77029418945312 Training Accuracy = 0.0013300807913765311\n","Iteration 433: Training Loss = 83.82101440429688 Training Accuracy = 0.0011036139912903309\n","Iteration 434: Training Loss = 83.75761413574219 Training Accuracy = 0.0014910927275195718\n","Iteration 435: Training Loss = 83.72799682617188 Training Accuracy = 0.0011552083306014538\n","Iteration 436: Training Loss = 83.49870300292969 Training Accuracy = 0.0015258077764883637\n","Iteration 437: Training Loss = 83.3041000366211 Training Accuracy = 0.001272512716241181\n","Iteration 438: Training Loss = 83.10639953613281 Training Accuracy = 0.001478932797908783\n","Iteration 439: Training Loss = 82.98625183105469 Training Accuracy = 0.001457227859646082\n","Iteration 440: Training Loss = 82.93260192871094 Training Accuracy = 0.0014409013092517853\n","Iteration 441: Training Loss = 82.91444396972656 Training Accuracy = 0.0016633290797472\n","Iteration 442: Training Loss = 82.93557739257812 Training Accuracy = 0.0014687712537124753\n","Iteration 443: Training Loss = 82.89264678955078 Training Accuracy = 0.0018509565852582455\n","Iteration 444: Training Loss = 82.86675262451172 Training Accuracy = 0.0015300170052796602\n","Iteration 445: Training Loss = 82.70955657958984 Training Accuracy = 0.0019391793757677078\n","Iteration 446: Training Loss = 82.57282257080078 Training Accuracy = 0.0016547406557947397\n","Iteration 447: Training Loss = 82.40489196777344 Training Accuracy = 0.0019244472496211529\n","Iteration 448: Training Loss = 82.27629852294922 Training Accuracy = 0.0018138604937121272\n","Iteration 449: Training Loss = 82.1883316040039 Training Accuracy = 0.0019002126064151525\n","Iteration 450: Training Loss = 82.13639831542969 Training Accuracy = 0.0020328019745647907\n","Iteration 451: Training Loss = 82.11311340332031 Training Accuracy = 0.0019420705502852798\n","Iteration 452: Training Loss = 82.09494018554688 Training Accuracy = 0.0022589073050767183\n","Iteration 453: Training Loss = 82.1037826538086 Training Accuracy = 0.001973278122022748\n","Iteration 454: Training Loss = 82.073486328125 Training Accuracy = 0.0024196216836571693\n","Iteration 455: Training Loss = 82.09226989746094 Training Accuracy = 0.0020095237996429205\n","Iteration 456: Training Loss = 81.99757385253906 Training Accuracy = 0.002545514376834035\n","Iteration 457: Training Loss = 81.9437484741211 Training Accuracy = 0.0020914115011692047\n","Iteration 458: Training Loss = 81.77401733398438 Training Accuracy = 0.002568877534940839\n","Iteration 459: Training Loss = 81.63842010498047 Training Accuracy = 0.0022156888153403997\n","Iteration 460: Training Loss = 81.49211120605469 Training Accuracy = 0.002517984714359045\n","Iteration 461: Training Loss = 81.38590240478516 Training Accuracy = 0.002416921779513359\n","Iteration 462: Training Loss = 81.31379699707031 Training Accuracy = 0.0025048470124602318\n","Iteration 463: Training Loss = 81.27020263671875 Training Accuracy = 0.002655059564858675\n","Iteration 464: Training Loss = 81.24762725830078 Training Accuracy = 0.0025355867110192776\n","Iteration 465: Training Loss = 81.23458099365234 Training Accuracy = 0.0028858205769211054\n","Iteration 466: Training Loss = 81.2575454711914 Training Accuracy = 0.0025780824944376945\n","Iteration 467: Training Loss = 81.25480651855469 Training Accuracy = 0.003115093568339944\n","Iteration 468: Training Loss = 81.31900787353516 Training Accuracy = 0.0025917303282767534\n","Iteration 469: Training Loss = 81.2432632446289 Training Accuracy = 0.003256717696785927\n","Iteration 470: Training Loss = 81.22681427001953 Training Accuracy = 0.002623448148369789\n","Iteration 471: Training Loss = 81.02214813232422 Training Accuracy = 0.003251955844461918\n","Iteration 472: Training Loss = 80.86056518554688 Training Accuracy = 0.0027784863486886024\n","Iteration 473: Training Loss = 80.68689727783203 Training Accuracy = 0.003110097721219063\n","Iteration 474: Training Loss = 80.57865905761719 Training Accuracy = 0.002993027213960886\n","Iteration 475: Training Loss = 80.53048706054688 Training Accuracy = 0.002984077436849475\n","Iteration 476: Training Loss = 80.5230941772461 Training Accuracy = 0.0032487032003700733\n","Iteration 477: Training Loss = 80.53778839111328 Training Accuracy = 0.00293524656444788\n","Iteration 478: Training Loss = 80.51508331298828 Training Accuracy = 0.003451126627624035\n","Iteration 479: Training Loss = 80.50743103027344 Training Accuracy = 0.0029775723814964294\n","Iteration 480: Training Loss = 80.4061050415039 Training Accuracy = 0.0035616070963442326\n","Iteration 481: Training Loss = 80.3243179321289 Training Accuracy = 0.003089477075263858\n","Iteration 482: Training Loss = 80.18870544433594 Training Accuracy = 0.0035464924294501543\n","Iteration 483: Training Loss = 80.07987213134766 Training Accuracy = 0.003252635942772031\n","Iteration 484: Training Loss = 79.98434448242188 Training Accuracy = 0.0035025510005652905\n","Iteration 485: Training Loss = 79.91106414794922 Training Accuracy = 0.003457249142229557\n","Iteration 486: Training Loss = 79.85547637939453 Training Accuracy = 0.003464583307504654\n","Iteration 487: Training Loss = 79.81391906738281 Training Accuracy = 0.0036538690328598022\n","Iteration 488: Training Loss = 79.7866439819336 Training Accuracy = 0.003470705822110176\n","Iteration 489: Training Loss = 79.76301574707031 Training Accuracy = 0.0038702806923538446\n","Iteration 490: Training Loss = 79.76881408691406 Training Accuracy = 0.0034649022854864597\n","Iteration 491: Training Loss = 79.7581558227539 Training Accuracy = 0.00407548900693655\n","Iteration 492: Training Loss = 79.83326721191406 Training Accuracy = 0.003401615656912327\n","Iteration 493: Training Loss = 79.8046646118164 Training Accuracy = 0.004263796843588352\n","Iteration 494: Training Loss = 79.86687469482422 Training Accuracy = 0.003368707373738289\n","Iteration 495: Training Loss = 79.67151641845703 Training Accuracy = 0.00425746152177453\n","Iteration 496: Training Loss = 79.52576446533203 Training Accuracy = 0.0034525510855019093\n","Iteration 497: Training Loss = 79.28235626220703 Training Accuracy = 0.004008482210338116\n","Iteration 498: Training Loss = 79.13240051269531 Training Accuracy = 0.0036979378201067448\n","Iteration 499: Training Loss = 79.06439971923828 Training Accuracy = 0.0037302295677363873\n","Iteration 500: Training Loss = 79.07694244384766 Training Accuracy = 0.004021152388304472\n","Iteration 501: Training Loss = 79.1404800415039 Training Accuracy = 0.003601084230467677\n","Iteration 502: Training Loss = 79.14252471923828 Training Accuracy = 0.0043403697200119495\n","Iteration 503: Training Loss = 79.15221405029297 Training Accuracy = 0.0036173255648463964\n","Iteration 504: Training Loss = 78.987060546875 Training Accuracy = 0.004385884385555983\n","Iteration 505: Training Loss = 78.84729766845703 Training Accuracy = 0.003787457477301359\n","Iteration 506: Training Loss = 78.6954345703125 Training Accuracy = 0.0042500426061451435\n","Iteration 507: Training Loss = 78.59880828857422 Training Accuracy = 0.004094111267477274\n","Iteration 508: Training Loss = 78.55400085449219 Training Accuracy = 0.004069770220667124\n","Iteration 509: Training Loss = 78.54192352294922 Training Accuracy = 0.00435389019548893\n","Iteration 510: Training Loss = 78.55219268798828 Training Accuracy = 0.003963541705161333\n","Iteration 511: Training Loss = 78.54521942138672 Training Accuracy = 0.004596471320837736\n","Iteration 512: Training Loss = 78.55824279785156 Training Accuracy = 0.003957121632993221\n","Iteration 513: Training Loss = 78.49337768554688 Training Accuracy = 0.004728040192276239\n","Iteration 514: Training Loss = 78.45406341552734 Training Accuracy = 0.004014328122138977\n","Iteration 515: Training Loss = 78.3347396850586 Training Accuracy = 0.004700510296970606\n","Iteration 516: Training Loss = 78.23760986328125 Training Accuracy = 0.004128656350076199\n","Iteration 517: Training Loss = 78.12490844726562 Training Accuracy = 0.004546853713691235\n","Iteration 518: Training Loss = 78.03541564941406 Training Accuracy = 0.004269387573003769\n","Iteration 519: Training Loss = 77.97808837890625 Training Accuracy = 0.004439689684659243\n","Iteration 520: Training Loss = 77.9228515625 Training Accuracy = 0.004502083174884319\n","Iteration 521: Training Loss = 77.88301086425781 Training Accuracy = 0.004426743369549513\n","Iteration 522: Training Loss = 77.8572998046875 Training Accuracy = 0.004731377586722374\n","Iteration 523: Training Loss = 77.84176635742188 Training Accuracy = 0.004397342912852764\n","Iteration 524: Training Loss = 77.83332824707031 Training Accuracy = 0.004955378361046314\n","Iteration 525: Training Loss = 77.87954711914062 Training Accuracy = 0.0043233842588961124\n","Iteration 526: Training Loss = 77.90436553955078 Training Accuracy = 0.005206207279115915\n","Iteration 527: Training Loss = 78.03263854980469 Training Accuracy = 0.0041897958144545555\n","Iteration 528: Training Loss = 77.99988555908203 Training Accuracy = 0.0053784651681780815\n","Iteration 529: Training Loss = 78.07469940185547 Training Accuracy = 0.004112627357244492\n","Iteration 530: Training Loss = 77.79903411865234 Training Accuracy = 0.005268877372145653\n","Iteration 531: Training Loss = 77.60157775878906 Training Accuracy = 0.00428735138848424\n","Iteration 532: Training Loss = 77.37171173095703 Training Accuracy = 0.004857759457081556\n","Iteration 533: Training Loss = 77.25938415527344 Training Accuracy = 0.004627657122910023\n","Iteration 534: Training Loss = 77.25052642822266 Training Accuracy = 0.00446662399917841\n","Iteration 535: Training Loss = 77.2963638305664 Training Accuracy = 0.004960437770932913\n","Iteration 536: Training Loss = 77.37150573730469 Training Accuracy = 0.0042626275680959225\n","Iteration 537: Training Loss = 77.3202896118164 Training Accuracy = 0.005163137800991535\n","Iteration 538: Training Loss = 77.26390838623047 Training Accuracy = 0.004357419442385435\n","Iteration 539: Training Loss = 77.10145568847656 Training Accuracy = 0.005134757608175278\n","Iteration 540: Training Loss = 76.97320556640625 Training Accuracy = 0.004641538951545954\n","Iteration 541: Training Loss = 76.8739242553711 Training Accuracy = 0.004935692995786667\n","Iteration 542: Training Loss = 76.82010650634766 Training Accuracy = 0.004945387132465839\n","Iteration 543: Training Loss = 76.80123901367188 Training Accuracy = 0.004761117976158857\n","Iteration 544: Training Loss = 76.79879760742188 Training Accuracy = 0.005204868037253618\n","Iteration 545: Training Loss = 76.81578063964844 Training Accuracy = 0.004658970981836319\n","Iteration 546: Training Loss = 76.79875183105469 Training Accuracy = 0.005413413979113102\n","Iteration 547: Training Loss = 76.80863189697266 Training Accuracy = 0.004643218591809273\n","Iteration 548: Training Loss = 76.73930358886719 Training Accuracy = 0.005536564625799656\n","Iteration 549: Training Loss = 76.70028686523438 Training Accuracy = 0.004691517911851406\n","Iteration 550: Training Loss = 76.58238220214844 Training Accuracy = 0.005500255152583122\n","Iteration 551: Training Loss = 76.49329376220703 Training Accuracy = 0.0048152850940823555\n","Iteration 552: Training Loss = 76.38229370117188 Training Accuracy = 0.005380888469517231\n","Iteration 553: Training Loss = 76.29340362548828 Training Accuracy = 0.004984630271792412\n","Iteration 554: Training Loss = 76.21306610107422 Training Accuracy = 0.0052596088498830795\n","Iteration 555: Training Loss = 76.14643859863281 Training Accuracy = 0.005196046084165573\n","Iteration 556: Training Loss = 76.09185028076172 Training Accuracy = 0.005219090264290571\n","Iteration 557: Training Loss = 76.05046844482422 Training Accuracy = 0.005434566177427769\n","Iteration 558: Training Loss = 76.02456665039062 Training Accuracy = 0.005226807203143835\n","Iteration 559: Training Loss = 76.0023193359375 Training Accuracy = 0.005704676732420921\n","Iteration 560: Training Loss = 76.0099868774414 Training Accuracy = 0.005201828200370073\n","Iteration 561: Training Loss = 76.03412628173828 Training Accuracy = 0.006000828929245472\n","Iteration 562: Training Loss = 76.1636734008789 Training Accuracy = 0.005032865796238184\n","Iteration 563: Training Loss = 76.22086334228516 Training Accuracy = 0.00629502534866333\n","Iteration 564: Training Loss = 76.46730041503906 Training Accuracy = 0.00481088412925601\n","Iteration 565: Training Loss = 76.23814392089844 Training Accuracy = 0.006363286636769772\n","Iteration 566: Training Loss = 76.099609375 Training Accuracy = 0.004912988748401403\n","Iteration 567: Training Loss = 75.71171569824219 Training Accuracy = 0.0058991494588553905\n","Iteration 568: Training Loss = 75.4935302734375 Training Accuracy = 0.005357163958251476\n","Iteration 569: Training Loss = 75.44398498535156 Training Accuracy = 0.005289179272949696\n","Iteration 570: Training Loss = 75.53244018554688 Training Accuracy = 0.005808333400636911\n","Iteration 571: Training Loss = 75.66715240478516 Training Accuracy = 0.0049902210012078285\n","Iteration 572: Training Loss = 75.59532165527344 Training Accuracy = 0.006108184345066547\n","Iteration 573: Training Loss = 75.50434112548828 Training Accuracy = 0.005137755069881678\n","Iteration 574: Training Loss = 75.28462982177734 Training Accuracy = 0.005964307114481926\n","Iteration 575: Training Loss = 75.13121795654297 Training Accuracy = 0.00554997893050313\n","Iteration 576: Training Loss = 75.07454681396484 Training Accuracy = 0.005616985727101564\n","Iteration 577: Training Loss = 75.0885009765625 Training Accuracy = 0.005976806860417128\n","Iteration 578: Training Loss = 75.13493347167969 Training Accuracy = 0.005424064584076405\n","Iteration 579: Training Loss = 75.1219253540039 Training Accuracy = 0.006280293222516775\n","Iteration 580: Training Loss = 75.11473083496094 Training Accuracy = 0.0054591200314462185\n","Iteration 581: Training Loss = 74.9975357055664 Training Accuracy = 0.006364711094647646\n","Iteration 582: Training Loss = 74.89025115966797 Training Accuracy = 0.005687372293323278\n","Iteration 583: Training Loss = 74.77391815185547 Training Accuracy = 0.006255187094211578\n","Iteration 584: Training Loss = 74.68516540527344 Training Accuracy = 0.006053294986486435\n","Iteration 585: Training Loss = 74.62890625 Training Accuracy = 0.00618163263425231\n","Iteration 586: Training Loss = 74.60054779052734 Training Accuracy = 0.006408418528735638\n","Iteration 587: Training Loss = 74.58838653564453 Training Accuracy = 0.006106100976467133\n","Iteration 588: Training Loss = 74.57514953613281 Training Accuracy = 0.006665922701358795\n","Iteration 589: Training Loss = 74.58456420898438 Training Accuracy = 0.006017984822392464\n","Iteration 590: Training Loss = 74.5697250366211 Training Accuracy = 0.006858461070805788\n","Iteration 591: Training Loss = 74.59709167480469 Training Accuracy = 0.006013732869178057\n","Iteration 592: Training Loss = 74.55320739746094 Training Accuracy = 0.007059162482619286\n","Iteration 593: Training Loss = 74.57522583007812 Training Accuracy = 0.006059502717107534\n","Iteration 594: Training Loss = 74.4766616821289 Training Accuracy = 0.0071953232400119305\n","Iteration 595: Training Loss = 74.43353271484375 Training Accuracy = 0.006216560490429401\n","Iteration 596: Training Loss = 74.2976303100586 Training Accuracy = 0.007185948081314564\n","Iteration 597: Training Loss = 74.199462890625 Training Accuracy = 0.006480527110397816\n","Iteration 598: Training Loss = 74.08550262451172 Training Accuracy = 0.0070943450555205345\n","Iteration 599: Training Loss = 74.00433349609375 Training Accuracy = 0.006757334340363741\n","Iteration 600: Training Loss = 73.94371795654297 Training Accuracy = 0.006980293430387974\n","Iteration 601: Training Loss = 73.90353393554688 Training Accuracy = 0.007052083499729633\n","Iteration 602: Training Loss = 73.87850952148438 Training Accuracy = 0.006947704125195742\n","Iteration 603: Training Loss = 73.8646011352539 Training Accuracy = 0.007394833955913782\n","Iteration 604: Training Loss = 73.87551879882812 Training Accuracy = 0.006976488046348095\n","Iteration 605: Training Loss = 73.90007019042969 Training Accuracy = 0.007768133655190468\n","Iteration 606: Training Loss = 74.01274871826172 Training Accuracy = 0.006915178615599871\n","Iteration 607: Training Loss = 74.1074447631836 Training Accuracy = 0.008211479522287846\n","Iteration 608: Training Loss = 74.42866516113281 Training Accuracy = 0.0067099700681865215\n","Iteration 609: Training Loss = 74.33483123779297 Training Accuracy = 0.00852249190211296\n","Iteration 610: Training Loss = 74.40274810791016 Training Accuracy = 0.006734183523803949\n","Iteration 611: Training Loss = 73.88358306884766 Training Accuracy = 0.008233057335019112\n","Iteration 612: Training Loss = 73.56464385986328 Training Accuracy = 0.007251828443259001\n","Iteration 613: Training Loss = 73.42703247070312 Training Accuracy = 0.0074747661128640175\n","Iteration 614: Training Loss = 73.49622344970703 Training Accuracy = 0.007791369222104549\n","Iteration 615: Training Loss = 73.68253326416016 Training Accuracy = 0.006944090127944946\n","Iteration 616: Training Loss = 73.69641876220703 Training Accuracy = 0.008086713030934334\n","Iteration 617: Training Loss = 73.676513671875 Training Accuracy = 0.006929825525730848\n","Iteration 618: Training Loss = 73.40518188476562 Training Accuracy = 0.007963563315570354\n","Iteration 619: Training Loss = 73.22010803222656 Training Accuracy = 0.007393919862806797\n","Iteration 620: Training Loss = 73.14344787597656 Training Accuracy = 0.007571705151349306\n","Iteration 621: Training Loss = 73.17095947265625 Training Accuracy = 0.007948235608637333\n","Iteration 622: Training Loss = 73.25730895996094 Training Accuracy = 0.007333206012845039\n","Iteration 623: Training Loss = 73.27581787109375 Training Accuracy = 0.00833951961249113\n","Iteration 624: Training Loss = 73.29631042480469 Training Accuracy = 0.007399107329547405\n","Iteration 625: Training Loss = 73.14862823486328 Training Accuracy = 0.008463435806334019\n","Iteration 626: Training Loss = 73.02964782714844 Training Accuracy = 0.007732142694294453\n","Iteration 627: Training Loss = 72.89264678955078 Training Accuracy = 0.00836921762675047\n","Iteration 628: Training Loss = 72.81321716308594 Training Accuracy = 0.008204102516174316\n","Iteration 629: Training Loss = 72.78132629394531 Training Accuracy = 0.008270472288131714\n","Iteration 630: Training Loss = 72.7804183959961 Training Accuracy = 0.00865980051457882\n","Iteration 631: Training Loss = 72.79981994628906 Training Accuracy = 0.008207269944250584\n","Iteration 632: Training Loss = 72.8051528930664 Training Accuracy = 0.008996237069368362\n","Iteration 633: Training Loss = 72.86929321289062 Training Accuracy = 0.008197109214961529\n","Iteration 634: Training Loss = 72.83586883544922 Training Accuracy = 0.009276552125811577\n","Iteration 635: Training Loss = 72.851318359375 Training Accuracy = 0.008273681625723839\n","Iteration 636: Training Loss = 72.77143096923828 Training Accuracy = 0.009424298070371151\n","Iteration 637: Training Loss = 72.74431610107422 Training Accuracy = 0.008487308397889137\n","Iteration 638: Training Loss = 72.6216049194336 Training Accuracy = 0.009472151286900043\n","Iteration 639: Training Loss = 72.53805541992188 Training Accuracy = 0.008714540861546993\n","Iteration 640: Training Loss = 72.43247985839844 Training Accuracy = 0.00942055694758892\n","Iteration 641: Training Loss = 72.36351013183594 Training Accuracy = 0.008984439074993134\n","Iteration 642: Training Loss = 72.3021469116211 Training Accuracy = 0.009377912618219852\n","Iteration 643: Training Loss = 72.24958801269531 Training Accuracy = 0.00928537454456091\n","Iteration 644: Training Loss = 72.20855712890625 Training Accuracy = 0.009461394511163235\n","Iteration 645: Training Loss = 72.1772232055664 Training Accuracy = 0.0096239373087883\n","Iteration 646: Training Loss = 72.14892578125 Training Accuracy = 0.009586075320839882\n","Iteration 647: Training Loss = 72.12445831298828 Training Accuracy = 0.009969855658710003\n","Iteration 648: Training Loss = 72.11309814453125 Training Accuracy = 0.009635267779231071\n","Iteration 649: Training Loss = 72.12873077392578 Training Accuracy = 0.010311990045011044\n","Iteration 650: Training Loss = 72.23786163330078 Training Accuracy = 0.009556101635098457\n","Iteration 651: Training Loss = 72.38053131103516 Training Accuracy = 0.010856186039745808\n","Iteration 652: Training Loss = 72.83902740478516 Training Accuracy = 0.009312266483902931\n","Iteration 653: Training Loss = 72.88442993164062 Training Accuracy = 0.01146857999265194\n","Iteration 654: Training Loss = 73.26580047607422 Training Accuracy = 0.009184970520436764\n","Iteration 655: Training Loss = 72.4170150756836 Training Accuracy = 0.011251721531152725\n","Iteration 656: Training Loss = 71.90887451171875 Training Accuracy = 0.009907844476401806\n","Iteration 657: Training Loss = 71.72016906738281 Training Accuracy = 0.010126679204404354\n","Iteration 658: Training Loss = 71.88383483886719 Training Accuracy = 0.010581739246845245\n","Iteration 659: Training Loss = 72.21991729736328 Training Accuracy = 0.009324532002210617\n","Iteration 660: Training Loss = 72.11994934082031 Training Accuracy = 0.010776381939649582\n","Iteration 661: Training Loss = 71.96259307861328 Training Accuracy = 0.009546917863190174\n","Iteration 662: Training Loss = 71.60033416748047 Training Accuracy = 0.01047561690211296\n","Iteration 663: Training Loss = 71.4830093383789 Training Accuracy = 0.010280250571668148\n","Iteration 664: Training Loss = 71.58818817138672 Training Accuracy = 0.009905845858156681\n","Iteration 665: Training Loss = 71.71682739257812 Training Accuracy = 0.010866709053516388\n","Iteration 666: Training Loss = 71.85162353515625 Training Accuracy = 0.00969462189823389\n","Iteration 667: Training Loss = 71.64773559570312 Training Accuracy = 0.011035863310098648\n","Iteration 668: Training Loss = 71.45252227783203 Training Accuracy = 0.010147321037948132\n","Iteration 669: Training Loss = 71.27983093261719 Training Accuracy = 0.010725297965109348\n","Iteration 670: Training Loss = 71.2394027709961 Training Accuracy = 0.010792474262416363\n","Iteration 671: Training Loss = 71.29370880126953 Training Accuracy = 0.010449128225445747\n","Iteration 672: Training Loss = 71.35926818847656 Training Accuracy = 0.011319260112941265\n","Iteration 673: Training Loss = 71.44160461425781 Training Accuracy = 0.01046751718968153\n","Iteration 674: Training Loss = 71.34848022460938 Training Accuracy = 0.011622512713074684\n","Iteration 675: Training Loss = 71.26210021972656 Training Accuracy = 0.010738478042185307\n","Iteration 676: Training Loss = 71.113037109375 Training Accuracy = 0.01161575224250555\n","Iteration 677: Training Loss = 71.00928497314453 Training Accuracy = 0.011167389340698719\n","Iteration 678: Training Loss = 70.95085906982422 Training Accuracy = 0.011413775384426117\n","Iteration 679: Training Loss = 70.93758392333984 Training Accuracy = 0.011673745699226856\n","Iteration 680: Training Loss = 70.95255279541016 Training Accuracy = 0.011350616812705994\n","Iteration 681: Training Loss = 70.97486877441406 Training Accuracy = 0.012130995281040668\n","Iteration 682: Training Loss = 71.02923583984375 Training Accuracy = 0.011436458677053452\n","Iteration 683: Training Loss = 71.03052520751953 Training Accuracy = 0.012529464438557625\n","Iteration 684: Training Loss = 71.08487701416016 Training Accuracy = 0.011582079343497753\n","Iteration 685: Training Loss = 71.02853393554688 Training Accuracy = 0.012855357490479946\n","Iteration 686: Training Loss = 71.02957153320312 Training Accuracy = 0.011775042861700058\n","Iteration 687: Training Loss = 70.91524505615234 Training Accuracy = 0.012994260527193546\n","Iteration 688: Training Loss = 70.85259246826172 Training Accuracy = 0.012052040547132492\n","Iteration 689: Training Loss = 70.73686981201172 Training Accuracy = 0.012958949431777\n","Iteration 690: Training Loss = 70.65843200683594 Training Accuracy = 0.01235882192850113\n","Iteration 691: Training Loss = 70.58326721191406 Training Accuracy = 0.012903549708425999\n","Iteration 692: Training Loss = 70.53050231933594 Training Accuracy = 0.012651445344090462\n","Iteration 693: Training Loss = 70.4906997680664 Training Accuracy = 0.012910714372992516\n","Iteration 694: Training Loss = 70.45909881591797 Training Accuracy = 0.01298971101641655\n","Iteration 695: Training Loss = 70.43387603759766 Training Accuracy = 0.013001020066440105\n","Iteration 696: Training Loss = 70.41539764404297 Training Accuracy = 0.013375340029597282\n","Iteration 697: Training Loss = 70.40887451171875 Training Accuracy = 0.013149851001799107\n","Iteration 698: Training Loss = 70.42044830322266 Training Accuracy = 0.013815710321068764\n","Iteration 699: Training Loss = 70.49163818359375 Training Accuracy = 0.013209247030317783\n","Iteration 700: Training Loss = 70.60834503173828 Training Accuracy = 0.014397491700947285\n","Iteration 701: Training Loss = 70.9761962890625 Training Accuracy = 0.013012159615755081\n","Iteration 702: Training Loss = 71.17049407958984 Training Accuracy = 0.015109779313206673\n","Iteration 703: Training Loss = 71.81233215332031 Training Accuracy = 0.012703507207334042\n","Iteration 704: Training Loss = 71.0905532836914 Training Accuracy = 0.015240008942782879\n","Iteration 705: Training Loss = 70.63140106201172 Training Accuracy = 0.013334077782928944\n","Iteration 706: Training Loss = 70.17948913574219 Training Accuracy = 0.014284035190939903\n","Iteration 707: Training Loss = 70.12846374511719 Training Accuracy = 0.0141820153221488\n","Iteration 708: Training Loss = 70.3504867553711 Training Accuracy = 0.01321815513074398\n","Iteration 709: Training Loss = 70.53099822998047 Training Accuracy = 0.014530293643474579\n","Iteration 710: Training Loss = 70.71412658691406 Training Accuracy = 0.013004804030060768\n","Iteration 711: Training Loss = 70.28246307373047 Training Accuracy = 0.014347831718623638\n","Iteration 712: Training Loss = 69.9990005493164 Training Accuracy = 0.013591623865067959\n","Iteration 713: Training Loss = 69.92989349365234 Training Accuracy = 0.013826891779899597\n","Iteration 714: Training Loss = 70.03874969482422 Training Accuracy = 0.01428025122731924\n","Iteration 715: Training Loss = 70.2446517944336 Training Accuracy = 0.013291326351463795\n","Iteration 716: Training Loss = 70.23446655273438 Training Accuracy = 0.01468762755393982\n","Iteration 717: Training Loss = 70.17899322509766 Training Accuracy = 0.013390540145337582\n","Iteration 718: Training Loss = 69.92949676513672 Training Accuracy = 0.01450754702091217\n","Iteration 719: Training Loss = 69.77994537353516 Training Accuracy = 0.014039583504199982\n","Iteration 720: Training Loss = 69.73551940917969 Training Accuracy = 0.014155186712741852\n","Iteration 721: Training Loss = 69.7774658203125 Training Accuracy = 0.01467512734234333\n","Iteration 722: Training Loss = 69.87454986572266 Training Accuracy = 0.014102742075920105\n","Iteration 723: Training Loss = 69.89778900146484 Training Accuracy = 0.01518352422863245\n","Iteration 724: Training Loss = 69.92823791503906 Training Accuracy = 0.014186968095600605\n","Iteration 725: Training Loss = 69.80286407470703 Training Accuracy = 0.015377954579889774\n","Iteration 726: Training Loss = 69.70500183105469 Training Accuracy = 0.014450998976826668\n","Iteration 727: Training Loss = 69.58622741699219 Training Accuracy = 0.015192452818155289\n","Iteration 728: Training Loss = 69.51144409179688 Training Accuracy = 0.014868601225316525\n","Iteration 729: Training Loss = 69.48085021972656 Training Accuracy = 0.014975340105593204\n","Iteration 730: Training Loss = 69.47565460205078 Training Accuracy = 0.01535799354314804\n","Iteration 731: Training Loss = 69.4896011352539 Training Accuracy = 0.014991071075201035\n","Iteration 732: Training Loss = 69.50650024414062 Training Accuracy = 0.015815751627087593\n","Iteration 733: Training Loss = 69.54817199707031 Training Accuracy = 0.015139860101044178\n","Iteration 734: Training Loss = 69.53703308105469 Training Accuracy = 0.016234587877988815\n","Iteration 735: Training Loss = 69.62186431884766 Training Accuracy = 0.015192177146673203\n","Iteration 736: Training Loss = 69.61774444580078 Training Accuracy = 0.016602465882897377\n","Iteration 737: Training Loss = 69.68640899658203 Training Accuracy = 0.015245110727846622\n","Iteration 738: Training Loss = 69.58724212646484 Training Accuracy = 0.01679510995745659\n","Iteration 739: Training Loss = 69.56780242919922 Training Accuracy = 0.015488839708268642\n","Iteration 740: Training Loss = 69.39871215820312 Training Accuracy = 0.01675795018672943\n","Iteration 741: Training Loss = 69.28807067871094 Training Accuracy = 0.015778444707393646\n","Iteration 742: Training Loss = 69.17086791992188 Training Accuracy = 0.01659766212105751\n","Iteration 743: Training Loss = 69.11245727539062 Training Accuracy = 0.01606915332376957\n","Iteration 744: Training Loss = 69.05316162109375 Training Accuracy = 0.016424745321273804\n","Iteration 745: Training Loss = 69.00225830078125 Training Accuracy = 0.01641250029206276\n","Iteration 746: Training Loss = 68.97364044189453 Training Accuracy = 0.016367560252547264\n","Iteration 747: Training Loss = 68.9632797241211 Training Accuracy = 0.016777679324150085\n","Iteration 748: Training Loss = 68.97642517089844 Training Accuracy = 0.01642731763422489\n","Iteration 749: Training Loss = 68.9977035522461 Training Accuracy = 0.017238115891814232\n","Iteration 750: Training Loss = 69.07462310791016 Training Accuracy = 0.016444940119981766\n","Iteration 751: Training Loss = 69.15721893310547 Training Accuracy = 0.017750872299075127\n","Iteration 752: Training Loss = 69.43910217285156 Training Accuracy = 0.016253231093287468\n","Iteration 753: Training Loss = 69.51153564453125 Training Accuracy = 0.018263159319758415\n","Iteration 754: Training Loss = 69.85466003417969 Training Accuracy = 0.016064094379544258\n","Iteration 755: Training Loss = 69.47117614746094 Training Accuracy = 0.018382802605628967\n","Iteration 756: Training Loss = 69.26577758789062 Training Accuracy = 0.01648116484284401\n","Iteration 757: Training Loss = 68.86268615722656 Training Accuracy = 0.017771832644939423\n","Iteration 758: Training Loss = 68.68193817138672 Training Accuracy = 0.017171258106827736\n","Iteration 759: Training Loss = 68.6706771850586 Training Accuracy = 0.01696755923330784\n","Iteration 760: Training Loss = 68.77759552001953 Training Accuracy = 0.017611436545848846\n","Iteration 761: Training Loss = 68.96587371826172 Training Accuracy = 0.016498660668730736\n","Iteration 762: Training Loss = 68.9610824584961 Training Accuracy = 0.017815709114074707\n","Iteration 763: Training Loss = 68.97505187988281 Training Accuracy = 0.016520684584975243\n","Iteration 764: Training Loss = 68.7578353881836 Training Accuracy = 0.01782657392323017\n","Iteration 765: Training Loss = 68.60214233398438 Training Accuracy = 0.0169434305280447\n","Iteration 766: Training Loss = 68.48656463623047 Training Accuracy = 0.01755671761929989\n","Iteration 767: Training Loss = 68.45347595214844 Training Accuracy = 0.01750754751265049\n","Iteration 768: Training Loss = 68.48334503173828 Training Accuracy = 0.017220046371221542\n","Iteration 769: Training Loss = 68.5344467163086 Training Accuracy = 0.017982184886932373\n","Iteration 770: Training Loss = 68.61553192138672 Training Accuracy = 0.017118176445364952\n","Iteration 771: Training Loss = 68.6329116821289 Training Accuracy = 0.018390964716672897\n","Iteration 772: Training Loss = 68.7037582397461 Training Accuracy = 0.017226148396730423\n","Iteration 773: Training Loss = 68.62027740478516 Training Accuracy = 0.01867157779633999\n","Iteration 774: Training Loss = 68.59833526611328 Training Accuracy = 0.01745593175292015\n","Iteration 775: Training Loss = 68.47978973388672 Training Accuracy = 0.018709056079387665\n","Iteration 776: Training Loss = 68.40577697753906 Training Accuracy = 0.017743324860930443\n","Iteration 777: Training Loss = 68.31454467773438 Training Accuracy = 0.01852712593972683\n","Iteration 778: Training Loss = 68.25530242919922 Training Accuracy = 0.0180172398686409\n","Iteration 779: Training Loss = 68.20653533935547 Training Accuracy = 0.01837174780666828\n","Iteration 780: Training Loss = 68.17202758789062 Training Accuracy = 0.018280314281582832\n","Iteration 781: Training Loss = 68.14933776855469 Training Accuracy = 0.01839987188577652\n","Iteration 782: Training Loss = 68.13591766357422 Training Accuracy = 0.01858418434858322\n","Iteration 783: Training Loss = 68.1285400390625 Training Accuracy = 0.018515583127737045\n","Iteration 784: Training Loss = 68.12676239013672 Training Accuracy = 0.01896641217172146\n","Iteration 785: Training Loss = 68.14639282226562 Training Accuracy = 0.018544429913163185\n","Iteration 786: Training Loss = 68.18846130371094 Training Accuracy = 0.019441476091742516\n","Iteration 787: Training Loss = 68.31857299804688 Training Accuracy = 0.018402742221951485\n","Iteration 788: Training Loss = 68.48196411132812 Training Accuracy = 0.020008843392133713\n","Iteration 789: Training Loss = 68.96288299560547 Training Accuracy = 0.018037967383861542\n","Iteration 790: Training Loss = 69.04170989990234 Training Accuracy = 0.020591964945197105\n","Iteration 791: Training Loss = 69.53494262695312 Training Accuracy = 0.017826253548264503\n","Iteration 792: Training Loss = 68.71056365966797 Training Accuracy = 0.02044428139925003\n","Iteration 793: Training Loss = 68.23229217529297 Training Accuracy = 0.018643813207745552\n","Iteration 794: Training Loss = 67.92852783203125 Training Accuracy = 0.019285820424556732\n","Iteration 795: Training Loss = 67.98595428466797 Training Accuracy = 0.019433142617344856\n","Iteration 796: Training Loss = 68.26224517822266 Training Accuracy = 0.018162371590733528\n","Iteration 797: Training Loss = 68.38819885253906 Training Accuracy = 0.019699404016137123\n","Iteration 798: Training Loss = 68.51456451416016 Training Accuracy = 0.017988668754696846\n","Iteration 799: Training Loss = 68.10546875 Training Accuracy = 0.019408375024795532\n","Iteration 800: Training Loss = 67.85432434082031 Training Accuracy = 0.018606016412377357\n","Iteration 801: Training Loss = 67.79129028320312 Training Accuracy = 0.018801232799887657\n","Iteration 802: Training Loss = 67.89592742919922 Training Accuracy = 0.01926041580736637\n","Iteration 803: Training Loss = 68.09278869628906 Training Accuracy = 0.018279103562235832\n","Iteration 804: Training Loss = 68.11365509033203 Training Accuracy = 0.01971362717449665\n","Iteration 805: Training Loss = 68.130126953125 Training Accuracy = 0.018259290605783463\n","Iteration 806: Training Loss = 67.91172790527344 Training Accuracy = 0.01971030980348587\n","Iteration 807: Training Loss = 67.75457763671875 Training Accuracy = 0.018812881782650948\n","Iteration 808: Training Loss = 67.65784454345703 Training Accuracy = 0.019362755119800568\n","Iteration 809: Training Loss = 67.64671325683594 Training Accuracy = 0.019499467685818672\n","Iteration 810: Training Loss = 67.69149780273438 Training Accuracy = 0.019120026379823685\n","Iteration 811: Training Loss = 67.75318908691406 Training Accuracy = 0.0200510211288929\n","Iteration 812: Training Loss = 67.84877014160156 Training Accuracy = 0.01903494819998741\n","Iteration 813: Training Loss = 67.82476043701172 Training Accuracy = 0.02033386565744877\n","Iteration 814: Training Loss = 67.83271026611328 Training Accuracy = 0.019086861982941628\n","Iteration 815: Training Loss = 67.73403930664062 Training Accuracy = 0.020370876416563988\n","Iteration 816: Training Loss = 67.67642974853516 Training Accuracy = 0.019331270828843117\n","Iteration 817: Training Loss = 67.58406829833984 Training Accuracy = 0.02027200162410736\n","Iteration 818: Training Loss = 67.52241516113281 Training Accuracy = 0.019707398489117622\n","Iteration 819: Training Loss = 67.47250366210938 Training Accuracy = 0.02017293870449066\n","Iteration 820: Training Loss = 67.43951416015625 Training Accuracy = 0.020075064152479172\n","Iteration 821: Training Loss = 67.41558074951172 Training Accuracy = 0.0201899241656065\n","Iteration 822: Training Loss = 67.40093994140625 Training Accuracy = 0.02037125825881958\n","Iteration 823: Training Loss = 67.39244842529297 Training Accuracy = 0.020271023735404015\n","Iteration 824: Training Loss = 67.37987518310547 Training Accuracy = 0.020705953240394592\n","Iteration 825: Training Loss = 67.4102783203125 Training Accuracy = 0.020280294120311737\n","Iteration 826: Training Loss = 67.44510650634766 Training Accuracy = 0.02123282290995121\n","Iteration 827: Training Loss = 67.56342315673828 Training Accuracy = 0.020176785066723824\n","Iteration 828: Training Loss = 67.70184326171875 Training Accuracy = 0.021851105615496635\n","Iteration 829: Training Loss = 68.09558868408203 Training Accuracy = 0.01988416165113449\n","Iteration 830: Training Loss = 68.16554260253906 Training Accuracy = 0.02235133945941925\n","Iteration 831: Training Loss = 68.60934448242188 Training Accuracy = 0.01967795565724373\n","Iteration 832: Training Loss = 67.9617691040039 Training Accuracy = 0.02228042110800743\n","Iteration 833: Training Loss = 67.59760284423828 Training Accuracy = 0.02031719870865345\n","Iteration 834: Training Loss = 67.23949432373047 Training Accuracy = 0.02135348692536354\n","Iteration 835: Training Loss = 67.17420196533203 Training Accuracy = 0.02112506330013275\n","Iteration 836: Training Loss = 67.32255554199219 Training Accuracy = 0.020310459658503532\n","Iteration 837: Training Loss = 67.49966430664062 Training Accuracy = 0.02154555730521679\n","Iteration 838: Training Loss = 67.72450256347656 Training Accuracy = 0.019888265058398247\n","Iteration 839: Training Loss = 67.50445556640625 Training Accuracy = 0.02155737578868866\n","Iteration 840: Training Loss = 67.32774353027344 Training Accuracy = 0.02027859352529049\n","Iteration 841: Training Loss = 67.11516571044922 Training Accuracy = 0.021156441420316696\n","Iteration 842: Training Loss = 67.04338836669922 Training Accuracy = 0.020904378965497017\n","Iteration 843: Training Loss = 67.08516693115234 Training Accuracy = 0.020654762163758278\n","Iteration 844: Training Loss = 67.18036651611328 Training Accuracy = 0.02144838497042656\n","Iteration 845: Training Loss = 67.32662963867188 Training Accuracy = 0.020338967442512512\n","Iteration 846: Training Loss = 67.32540130615234 Training Accuracy = 0.021860649809241295\n","Iteration 847: Training Loss = 67.35334014892578 Training Accuracy = 0.020437605679035187\n","Iteration 848: Training Loss = 67.20153045654297 Training Accuracy = 0.021984439343214035\n","Iteration 849: Training Loss = 67.10581970214844 Training Accuracy = 0.020901445299386978\n","Iteration 850: Training Loss = 66.98578643798828 Training Accuracy = 0.02185886539518833\n","Iteration 851: Training Loss = 66.91527557373047 Training Accuracy = 0.021420981734991074\n","Iteration 852: Training Loss = 66.87791442871094 Training Accuracy = 0.021644920110702515\n","Iteration 853: Training Loss = 66.87357330322266 Training Accuracy = 0.021826572716236115\n","Iteration 854: Training Loss = 66.8949966430664 Training Accuracy = 0.02147410623729229\n","Iteration 855: Training Loss = 66.92560577392578 Training Accuracy = 0.022201955318450928\n","Iteration 856: Training Loss = 66.99203491210938 Training Accuracy = 0.02139187976717949\n","Iteration 857: Training Loss = 67.04438018798828 Training Accuracy = 0.02267138659954071\n","Iteration 858: Training Loss = 67.19081115722656 Training Accuracy = 0.021363265812397003\n","Iteration 859: Training Loss = 67.22996520996094 Training Accuracy = 0.0231688991189003\n","Iteration 860: Training Loss = 67.42235565185547 Training Accuracy = 0.02136131003499031\n","Iteration 861: Training Loss = 67.3000717163086 Training Accuracy = 0.023435544222593307\n","Iteration 862: Training Loss = 67.32703399658203 Training Accuracy = 0.021507270634174347\n","Iteration 863: Training Loss = 67.05529022216797 Training Accuracy = 0.023235119879245758\n","Iteration 864: Training Loss = 66.8992691040039 Training Accuracy = 0.021856993436813354\n","Iteration 865: Training Loss = 66.73977661132812 Training Accuracy = 0.022689880803227425\n","Iteration 866: Training Loss = 66.65897369384766 Training Accuracy = 0.022263774648308754\n","Iteration 867: Training Loss = 66.65705871582031 Training Accuracy = 0.022189944982528687\n","Iteration 868: Training Loss = 66.68806457519531 Training Accuracy = 0.022727295756340027\n","Iteration 869: Training Loss = 66.75887298583984 Training Accuracy = 0.021959694102406502\n","Iteration 870: Training Loss = 66.82725524902344 Training Accuracy = 0.023159395903348923\n","Iteration 871: Training Loss = 66.96208190917969 Training Accuracy = 0.021841347217559814\n","Iteration 872: Training Loss = 66.96138000488281 Training Accuracy = 0.02345193363726139\n","Iteration 873: Training Loss = 67.09432220458984 Training Accuracy = 0.02176492288708687\n","Iteration 874: Training Loss = 66.98226165771484 Training Accuracy = 0.023634588345885277\n","Iteration 875: Training Loss = 66.9786148071289 Training Accuracy = 0.02189519628882408\n","Iteration 876: Training Loss = 66.79853820800781 Training Accuracy = 0.02356356382369995\n","Iteration 877: Training Loss = 66.70057678222656 Training Accuracy = 0.022238988429307938\n","Iteration 878: Training Loss = 66.56285858154297 Training Accuracy = 0.023233864456415176\n","Iteration 879: Training Loss = 66.48773956298828 Training Accuracy = 0.022634737193584442\n","Iteration 880: Training Loss = 66.4415512084961 Training Accuracy = 0.022976616397500038\n","Iteration 881: Training Loss = 66.419921875 Training Accuracy = 0.023029889911413193\n","Iteration 882: Training Loss = 66.42598724365234 Training Accuracy = 0.02283983863890171\n","Iteration 883: Training Loss = 66.44548034667969 Training Accuracy = 0.02340688742697239\n","Iteration 884: Training Loss = 66.48955535888672 Training Accuracy = 0.02273167483508587\n","Iteration 885: Training Loss = 66.54618835449219 Training Accuracy = 0.02380605787038803\n","Iteration 886: Training Loss = 66.70086669921875 Training Accuracy = 0.022572150453925133\n","Iteration 887: Training Loss = 66.80984497070312 Training Accuracy = 0.02433786168694496\n","Iteration 888: Training Loss = 67.1567611694336 Training Accuracy = 0.0223290603607893\n","Iteration 889: Training Loss = 67.12060546875 Training Accuracy = 0.024842454120516777\n","Iteration 890: Training Loss = 67.35398864746094 Training Accuracy = 0.02229985035955906\n","Iteration 891: Training Loss = 66.87957763671875 Training Accuracy = 0.0247482992708683\n","Iteration 892: Training Loss = 66.61871337890625 Training Accuracy = 0.02288435399532318\n","Iteration 893: Training Loss = 66.33580017089844 Training Accuracy = 0.023934906348586082\n","Iteration 894: Training Loss = 66.24276733398438 Training Accuracy = 0.023542283102869987\n","Iteration 895: Training Loss = 66.29345703125 Training Accuracy = 0.023058991879224777\n","Iteration 896: Training Loss = 66.41455841064453 Training Accuracy = 0.02399625815451145\n","Iteration 897: Training Loss = 66.6042709350586 Training Accuracy = 0.022611331194639206\n","Iteration 898: Training Loss = 66.60035705566406 Training Accuracy = 0.0242278054356575\n","Iteration 899: Training Loss = 66.65786743164062 Training Accuracy = 0.022662052884697914\n","Iteration 900: Training Loss = 66.45409393310547 Training Accuracy = 0.024196237325668335\n","Iteration 901: Training Loss = 66.32958984375 Training Accuracy = 0.023014923557639122\n","Iteration 902: Training Loss = 66.19469451904297 Training Accuracy = 0.023956824094057083\n","Iteration 903: Training Loss = 66.12445831298828 Training Accuracy = 0.02347330003976822\n","Iteration 904: Training Loss = 66.1043930053711 Training Accuracy = 0.02364126220345497\n","Iteration 905: Training Loss = 66.12081146240234 Training Accuracy = 0.023997895419597626\n","Iteration 906: Training Loss = 66.1654281616211 Training Accuracy = 0.023464031517505646\n","Iteration 907: Training Loss = 66.216796875 Training Accuracy = 0.024526147171854973\n","Iteration 908: Training Loss = 66.33982849121094 Training Accuracy = 0.023400595411658287\n","Iteration 909: Training Loss = 66.4167709350586 Training Accuracy = 0.02508571371436119\n","Iteration 910: Training Loss = 66.65365600585938 Training Accuracy = 0.023305783048272133\n","Iteration 911: Training Loss = 66.61599731445312 Training Accuracy = 0.025526806712150574\n","Iteration 912: Training Loss = 66.78685760498047 Training Accuracy = 0.02329445071518421\n","Iteration 913: Training Loss = 66.5108871459961 Training Accuracy = 0.025468112900853157\n","Iteration 914: Training Loss = 66.37205505371094 Training Accuracy = 0.02361832559108734\n","Iteration 915: Training Loss = 66.1091079711914 Training Accuracy = 0.02488439716398716\n","Iteration 916: Training Loss = 65.97728729248047 Training Accuracy = 0.024105463176965714\n","Iteration 917: Training Loss = 65.91732788085938 Training Accuracy = 0.024286458268761635\n","Iteration 918: Training Loss = 65.9344711303711 Training Accuracy = 0.024553677067160606\n","Iteration 919: Training Loss = 66.02558898925781 Training Accuracy = 0.023886246606707573\n","Iteration 920: Training Loss = 66.11901092529297 Training Accuracy = 0.02500920556485653\n","Iteration 921: Training Loss = 66.29517364501953 Training Accuracy = 0.023628294467926025\n","Iteration 922: Training Loss = 66.29607391357422 Training Accuracy = 0.025411713868379593\n","Iteration 923: Training Loss = 66.41485595703125 Training Accuracy = 0.023570343852043152\n","Iteration 924: Training Loss = 66.25163269042969 Training Accuracy = 0.025544749572873116\n","Iteration 925: Training Loss = 66.20233154296875 Training Accuracy = 0.023770876228809357\n","Iteration 926: Training Loss = 66.01091003417969 Training Accuracy = 0.025276487693190575\n","Iteration 927: Training Loss = 65.90338897705078 Training Accuracy = 0.024196108803153038\n","Iteration 928: Training Loss = 65.8018569946289 Training Accuracy = 0.02490771748125553\n","Iteration 929: Training Loss = 65.74728393554688 Training Accuracy = 0.024594303220510483\n","Iteration 930: Training Loss = 65.72000885009766 Training Accuracy = 0.024710480123758316\n","Iteration 931: Training Loss = 65.70977020263672 Training Accuracy = 0.024799808859825134\n","Iteration 932: Training Loss = 65.72756958007812 Training Accuracy = 0.024593792855739594\n","Iteration 933: Training Loss = 65.80835723876953 Training Accuracy = 0.025331567972898483\n","Iteration 934: Training Loss = 66.10050201416016 Training Accuracy = 0.02416245825588703\n","Iteration 935: Training Loss = 66.43107604980469 Training Accuracy = 0.02629980817437172\n","Iteration 936: Training Loss = 67.18205261230469 Training Accuracy = 0.023626403883099556\n","Iteration 937: Training Loss = 66.7441177368164 Training Accuracy = 0.026842134073376656\n","Iteration 938: Training Loss = 66.60540771484375 Training Accuracy = 0.024094749242067337\n","Iteration 939: Training Loss = 65.88827514648438 Training Accuracy = 0.02609538659453392\n","Iteration 940: Training Loss = 65.60020446777344 Training Accuracy = 0.025083227083086967\n","Iteration 941: Training Loss = 65.67729949951172 Training Accuracy = 0.024567028507590294\n","Iteration 942: Training Loss = 65.94345092773438 Training Accuracy = 0.02569936215877533\n","Iteration 943: Training Loss = 66.3117904663086 Training Accuracy = 0.023660991340875626\n","Iteration 944: Training Loss = 66.08908081054688 Training Accuracy = 0.025670088827610016\n","Iteration 945: Training Loss = 65.93500518798828 Training Accuracy = 0.02397151291370392\n","Iteration 946: Training Loss = 65.62494659423828 Training Accuracy = 0.025074830278754234\n","Iteration 947: Training Loss = 65.49503326416016 Training Accuracy = 0.02466307394206524\n","Iteration 948: Training Loss = 65.53665161132812 Training Accuracy = 0.024366773664951324\n","Iteration 949: Training Loss = 65.66536712646484 Training Accuracy = 0.02519330382347107\n","Iteration 950: Training Loss = 65.85220336914062 Training Accuracy = 0.023904316127300262\n","Iteration 951: Training Loss = 65.81659698486328 Training Accuracy = 0.02548818103969097\n","Iteration 952: Training Loss = 65.8327865600586 Training Accuracy = 0.023966221138834953\n","Iteration 953: Training Loss = 65.64977264404297 Training Accuracy = 0.025532441213726997\n","Iteration 954: Training Loss = 65.53823852539062 Training Accuracy = 0.024443304166197777\n","Iteration 955: Training Loss = 65.42152404785156 Training Accuracy = 0.025358589366078377\n","Iteration 956: Training Loss = 65.35887145996094 Training Accuracy = 0.025019919499754906\n","Iteration 957: Training Loss = 65.33942413330078 Training Accuracy = 0.025070982053875923\n","Iteration 958: Training Loss = 65.35681915283203 Training Accuracy = 0.025462690740823746\n","Iteration 959: Training Loss = 65.41474914550781 Training Accuracy = 0.024797683581709862\n","Iteration 960: Training Loss = 65.53856658935547 Training Accuracy = 0.02590053156018257\n","Iteration 961: Training Loss = 65.94844818115234 Training Accuracy = 0.02436024695634842\n","Iteration 962: Training Loss = 66.21815490722656 Training Accuracy = 0.026705505326390266\n","Iteration 963: Training Loss = 66.94129943847656 Training Accuracy = 0.023915646597743034\n","Iteration 964: Training Loss = 66.14274597167969 Training Accuracy = 0.027049638330936432\n","Iteration 965: Training Loss = 65.65416717529297 Training Accuracy = 0.024952594190835953\n","Iteration 966: Training Loss = 65.28585052490234 Training Accuracy = 0.02598358877003193\n","Iteration 967: Training Loss = 65.36332702636719 Training Accuracy = 0.026135863736271858\n","Iteration 968: Training Loss = 65.73110961914062 Training Accuracy = 0.024578740820288658\n","Iteration 969: Training Loss = 65.82561492919922 Training Accuracy = 0.026483183726668358\n","Iteration 970: Training Loss = 65.88483428955078 Training Accuracy = 0.02436116151511669\n","Iteration 971: Training Loss = 65.4135513305664 Training Accuracy = 0.025827594101428986\n","Iteration 972: Training Loss = 65.19503021240234 Training Accuracy = 0.025170281529426575\n","Iteration 973: Training Loss = 65.2270278930664 Training Accuracy = 0.024821385741233826\n","Iteration 974: Training Loss = 65.39534759521484 Training Accuracy = 0.025623107329010963\n","Iteration 975: Training Loss = 65.62094116210938 Training Accuracy = 0.024245642125606537\n","Iteration 976: Training Loss = 65.49884033203125 Training Accuracy = 0.025812797248363495\n","Iteration 977: Training Loss = 65.39226531982422 Training Accuracy = 0.024419089779257774\n","Iteration 978: Training Loss = 65.18394470214844 Training Accuracy = 0.025574255734682083\n","Iteration 979: Training Loss = 65.07977294921875 Training Accuracy = 0.02511228807270527\n","Iteration 980: Training Loss = 65.08765411376953 Training Accuracy = 0.02508324757218361\n","Iteration 981: Training Loss = 65.16179656982422 Training Accuracy = 0.02577742375433445\n","Iteration 982: Training Loss = 65.31548309326172 Training Accuracy = 0.024763349443674088\n","Iteration 983: Training Loss = 65.44783020019531 Training Accuracy = 0.026394622400403023\n","Iteration 984: Training Loss = 65.80763244628906 Training Accuracy = 0.024591920897364616\n","Iteration 985: Training Loss = 65.65050506591797 Training Accuracy = 0.026841241866350174\n","Iteration 986: Training Loss = 65.65237426757812 Training Accuracy = 0.024856101721525192\n","Iteration 987: Training Loss = 65.28177642822266 Training Accuracy = 0.026703422889113426\n","Iteration 988: Training Loss = 65.05330657958984 Training Accuracy = 0.025592496618628502\n","Iteration 989: Training Loss = 64.954833984375 Training Accuracy = 0.025930697098374367\n","Iteration 990: Training Loss = 65.00992584228516 Training Accuracy = 0.026236414909362793\n","Iteration 991: Training Loss = 65.158203125 Training Accuracy = 0.0252285934984684\n","Iteration 992: Training Loss = 65.26036834716797 Training Accuracy = 0.02661462500691414\n","Iteration 993: Training Loss = 65.43661499023438 Training Accuracy = 0.02501806989312172\n","Iteration 994: Training Loss = 65.30374145507812 Training Accuracy = 0.026713350787758827\n","Iteration 995: Training Loss = 65.26380157470703 Training Accuracy = 0.025189753621816635\n","Iteration 996: Training Loss = 65.06485748291016 Training Accuracy = 0.026580464094877243\n","Iteration 997: Training Loss = 64.94612121582031 Training Accuracy = 0.02563885971903801\n","Iteration 998: Training Loss = 64.85954284667969 Training Accuracy = 0.026208927854895592\n","Iteration 999: Training Loss = 64.83186340332031 Training Accuracy = 0.026148639619350433\n","Iteration 1000: Training Loss = 64.85211181640625 Training Accuracy = 0.025840263813734055\n","Iteration 1001: Training Loss = 64.90875244140625 Training Accuracy = 0.026591410860419273\n","Iteration 1002: Training Loss = 65.04682159423828 Training Accuracy = 0.02561018243432045\n","Iteration 1003: Training Loss = 65.204345703125 Training Accuracy = 0.027111884206533432\n","Iteration 1004: Training Loss = 65.68761444091797 Training Accuracy = 0.02523709647357464\n","Iteration 1005: Training Loss = 65.69622802734375 Training Accuracy = 0.027774745598435402\n","Iteration 1006: Training Loss = 66.06663513183594 Training Accuracy = 0.025074489414691925\n","Iteration 1007: Training Loss = 65.3720474243164 Training Accuracy = 0.02770673856139183\n","Iteration 1008: Training Loss = 64.94977569580078 Training Accuracy = 0.026053784415125847\n","Iteration 1009: Training Loss = 64.74246978759766 Training Accuracy = 0.026524340733885765\n","Iteration 1010: Training Loss = 64.86634826660156 Training Accuracy = 0.026892367750406265\n","Iteration 1011: Training Loss = 65.17977905273438 Training Accuracy = 0.025360416620969772\n","Iteration 1012: Training Loss = 65.25553131103516 Training Accuracy = 0.027121342718601227\n","Iteration 1013: Training Loss = 65.36026763916016 Training Accuracy = 0.025222448632121086\n","Iteration 1014: Training Loss = 64.96971893310547 Training Accuracy = 0.026834523305296898\n","Iteration 1015: Training Loss = 64.74335479736328 Training Accuracy = 0.025851594284176826\n","Iteration 1016: Training Loss = 64.65367889404297 Training Accuracy = 0.02621951512992382\n","Iteration 1017: Training Loss = 64.70989227294922 Training Accuracy = 0.026471640914678574\n","Iteration 1018: Training Loss = 64.86489868164062 Training Accuracy = 0.02561766654253006\n","Iteration 1019: Training Loss = 64.95689392089844 Training Accuracy = 0.027013754472136497\n","Iteration 1020: Training Loss = 65.11199188232422 Training Accuracy = 0.025388307869434357\n","Iteration 1021: Training Loss = 64.98941040039062 Training Accuracy = 0.027199383825063705\n","Iteration 1022: Training Loss = 64.96112823486328 Training Accuracy = 0.025723490864038467\n","Iteration 1023: Training Loss = 64.77482604980469 Training Accuracy = 0.027084099128842354\n","Iteration 1024: Training Loss = 64.67366027832031 Training Accuracy = 0.026178613305091858\n","Iteration 1025: Training Loss = 64.5838394165039 Training Accuracy = 0.02688862755894661\n","Iteration 1026: Training Loss = 64.53825378417969 Training Accuracy = 0.02653210051357746\n","Iteration 1027: Training Loss = 64.51870727539062 Training Accuracy = 0.026648299768567085\n","Iteration 1028: Training Loss = 64.51974487304688 Training Accuracy = 0.026816751807928085\n","Iteration 1029: Training Loss = 64.54319763183594 Training Accuracy = 0.02644292078912258\n","Iteration 1030: Training Loss = 64.5963134765625 Training Accuracy = 0.027232758700847626\n","Iteration 1031: Training Loss = 64.7509536743164 Training Accuracy = 0.026239965111017227\n","Iteration 1032: Training Loss = 64.97038269042969 Training Accuracy = 0.02794591896235943\n","Iteration 1033: Training Loss = 65.65865325927734 Training Accuracy = 0.025821087881922722\n","Iteration 1034: Training Loss = 65.67945861816406 Training Accuracy = 0.028797363862395287\n","Iteration 1035: Training Loss = 66.21016693115234 Training Accuracy = 0.025697512552142143\n","Iteration 1036: Training Loss = 65.08829498291016 Training Accuracy = 0.02856953628361225\n","Iteration 1037: Training Loss = 64.52869415283203 Training Accuracy = 0.027079634368419647\n","Iteration 1038: Training Loss = 64.49214935302734 Training Accuracy = 0.026945345103740692\n","Iteration 1039: Training Loss = 64.84977722167969 Training Accuracy = 0.027920812368392944\n","Iteration 1040: Training Loss = 65.35515594482422 Training Accuracy = 0.025670960545539856\n","Iteration 1041: Training Loss = 64.99021911621094 Training Accuracy = 0.027706505730748177\n","Iteration 1042: Training Loss = 64.69147491455078 Training Accuracy = 0.02610616572201252\n","Iteration 1043: Training Loss = 64.40260314941406 Training Accuracy = 0.026776976883411407\n","Iteration 1044: Training Loss = 64.4258804321289 Training Accuracy = 0.026907483115792274\n","Iteration 1045: Training Loss = 64.67005157470703 Training Accuracy = 0.025977296754717827\n","Iteration 1046: Training Loss = 64.76180267333984 Training Accuracy = 0.027371130883693695\n","Iteration 1047: Training Loss = 64.83172607421875 Training Accuracy = 0.025802763178944588\n","Iteration 1048: Training Loss = 64.5556869506836 Training Accuracy = 0.027362458407878876\n","Iteration 1049: Training Loss = 64.37202453613281 Training Accuracy = 0.026400340721011162\n","Iteration 1050: Training Loss = 64.28462219238281 Training Accuracy = 0.02679421752691269\n","Iteration 1051: Training Loss = 64.32445526123047 Training Accuracy = 0.02711060829460621\n","Iteration 1052: Training Loss = 64.45437622070312 Training Accuracy = 0.026265433058142662\n","Iteration 1053: Training Loss = 64.58585357666016 Training Accuracy = 0.027631398290395737\n","Iteration 1054: Training Loss = 64.87229919433594 Training Accuracy = 0.026047343388199806\n","Iteration 1055: Training Loss = 64.83793640136719 Training Accuracy = 0.028094684705138206\n","Iteration 1056: Training Loss = 65.01704406738281 Training Accuracy = 0.02613365277647972\n","Iteration 1057: Training Loss = 64.68653869628906 Training Accuracy = 0.028207993134856224\n","Iteration 1058: Training Loss = 64.50929260253906 Training Accuracy = 0.026606079190969467\n","Iteration 1059: Training Loss = 64.27674865722656 Training Accuracy = 0.027677465230226517\n","Iteration 1060: Training Loss = 64.17903900146484 Training Accuracy = 0.027210693806409836\n","Iteration 1061: Training Loss = 64.18743896484375 Training Accuracy = 0.026950914412736893\n","Iteration 1062: Training Loss = 64.26983642578125 Training Accuracy = 0.027584396302700043\n","Iteration 1063: Training Loss = 64.43576049804688 Training Accuracy = 0.02653975412249565\n","Iteration 1064: Training Loss = 64.52733612060547 Training Accuracy = 0.027979571372270584\n","Iteration 1065: Training Loss = 64.76626586914062 Training Accuracy = 0.026374192908406258\n","Iteration 1066: Training Loss = 64.64677429199219 Training Accuracy = 0.02839619480073452\n","Iteration 1067: Training Loss = 64.6685562133789 Training Accuracy = 0.026590730994939804\n","Iteration 1068: Training Loss = 64.39449310302734 Training Accuracy = 0.028322385624051094\n","Iteration 1069: Training Loss = 64.22972869873047 Training Accuracy = 0.027242092415690422\n","Iteration 1070: Training Loss = 64.10066986083984 Training Accuracy = 0.027790455147624016\n","Iteration 1071: Training Loss = 64.06382751464844 Training Accuracy = 0.027677806094288826\n","Iteration 1072: Training Loss = 64.10147094726562 Training Accuracy = 0.027298256754875183\n","Iteration 1073: Training Loss = 64.19060516357422 Training Accuracy = 0.028021173551678658\n","Iteration 1074: Training Loss = 64.37401580810547 Training Accuracy = 0.026954570785164833\n","Iteration 1075: Training Loss = 64.48845672607422 Training Accuracy = 0.028552040457725525\n","Iteration 1076: Training Loss = 64.85405731201172 Training Accuracy = 0.02664058282971382\n","Iteration 1077: Training Loss = 64.7654037475586 Training Accuracy = 0.029079336673021317\n","Iteration 1078: Training Loss = 64.91867065429688 Training Accuracy = 0.02681473270058632\n","Iteration 1079: Training Loss = 64.44740295410156 Training Accuracy = 0.028883948922157288\n","Iteration 1080: Training Loss = 64.18328857421875 Training Accuracy = 0.027515944093465805\n","Iteration 1081: Training Loss = 63.991432189941406 Training Accuracy = 0.028037479147315025\n","Iteration 1082: Training Loss = 63.9821891784668 Training Accuracy = 0.02795761078596115\n","Iteration 1083: Training Loss = 64.11367797851562 Training Accuracy = 0.02724049799144268\n","Iteration 1084: Training Loss = 64.2715072631836 Training Accuracy = 0.028338158503174782\n","Iteration 1085: Training Loss = 64.55937957763672 Training Accuracy = 0.026738053187727928\n","Iteration 1086: Training Loss = 64.49786376953125 Training Accuracy = 0.028756165876984596\n","Iteration 1087: Training Loss = 64.58182525634766 Training Accuracy = 0.02685627155005932\n","Iteration 1088: Training Loss = 64.27547454833984 Training Accuracy = 0.028773171827197075\n","Iteration 1089: Training Loss = 64.1008071899414 Training Accuracy = 0.027532164007425308\n","Iteration 1090: Training Loss = 63.93318176269531 Training Accuracy = 0.02826545573771\n","Iteration 1091: Training Loss = 63.8751335144043 Training Accuracy = 0.028051424771547318\n","Iteration 1092: Training Loss = 63.908267974853516 Training Accuracy = 0.027712563052773476\n","Iteration 1093: Training Loss = 63.996402740478516 Training Accuracy = 0.02837551012635231\n","Iteration 1094: Training Loss = 64.16067504882812 Training Accuracy = 0.027346597984433174\n","Iteration 1095: Training Loss = 64.25001525878906 Training Accuracy = 0.02888207882642746\n","Iteration 1096: Training Loss = 64.52884674072266 Training Accuracy = 0.027122747153043747\n","Iteration 1097: Training Loss = 64.45970153808594 Training Accuracy = 0.029380761086940765\n","Iteration 1098: Training Loss = 64.61783599853516 Training Accuracy = 0.027328358963131905\n","Iteration 1099: Training Loss = 64.27568054199219 Training Accuracy = 0.029355676844716072\n","Iteration 1100: Training Loss = 64.10020446777344 Training Accuracy = 0.027865560725331306\n","Iteration 1101: Training Loss = 63.871864318847656 Training Accuracy = 0.028762223199009895\n","Iteration 1102: Training Loss = 63.778282165527344 Training Accuracy = 0.028241198509931564\n","Iteration 1103: Training Loss = 63.7913703918457 Training Accuracy = 0.028053507208824158\n","Iteration 1104: Training Loss = 63.875938415527344 Training Accuracy = 0.028592325747013092\n","Iteration 1105: Training Loss = 64.0439453125 Training Accuracy = 0.02756800502538681\n","Iteration 1106: Training Loss = 64.1604232788086 Training Accuracy = 0.0291312076151371\n","Iteration 1107: Training Loss = 64.45913696289062 Training Accuracy = 0.027381546795368195\n","Iteration 1108: Training Loss = 64.36494445800781 Training Accuracy = 0.029614711180329323\n","Iteration 1109: Training Loss = 64.48542022705078 Training Accuracy = 0.02762714773416519\n","Iteration 1110: Training Loss = 64.13013458251953 Training Accuracy = 0.029566582292318344\n","Iteration 1111: Training Loss = 63.9309196472168 Training Accuracy = 0.028185375034809113\n","Iteration 1112: Training Loss = 63.74030685424805 Training Accuracy = 0.02892952784895897\n","Iteration 1113: Training Loss = 63.68124771118164 Training Accuracy = 0.028583886101841927\n","Iteration 1114: Training Loss = 63.72228240966797 Training Accuracy = 0.02822702005505562\n","Iteration 1115: Training Loss = 63.8245849609375 Training Accuracy = 0.02895892783999443\n","Iteration 1116: Training Loss = 64.02558135986328 Training Accuracy = 0.027789073064923286\n","Iteration 1117: Training Loss = 64.13511657714844 Training Accuracy = 0.029502317309379578\n","Iteration 1118: Training Loss = 64.46733093261719 Training Accuracy = 0.027618303894996643\n","Iteration 1119: Training Loss = 64.30928039550781 Training Accuracy = 0.029948193579912186\n","Iteration 1120: Training Loss = 64.38285064697266 Training Accuracy = 0.027891304343938828\n","Iteration 1121: Training Loss = 63.996826171875 Training Accuracy = 0.029758013784885406\n","Iteration 1122: Training Loss = 63.77717971801758 Training Accuracy = 0.028464455157518387\n","Iteration 1123: Training Loss = 63.61272430419922 Training Accuracy = 0.028985118493437767\n","Iteration 1124: Training Loss = 63.578041076660156 Training Accuracy = 0.028802571818232536\n","Iteration 1125: Training Loss = 63.64487075805664 Training Accuracy = 0.028245408087968826\n","Iteration 1126: Training Loss = 63.78885269165039 Training Accuracy = 0.02918282337486744\n","Iteration 1127: Training Loss = 64.10670471191406 Training Accuracy = 0.02776322327554226\n","Iteration 1128: Training Loss = 64.19801330566406 Training Accuracy = 0.029797831550240517\n","Iteration 1129: Training Loss = 64.52629852294922 Training Accuracy = 0.027676232159137726\n","Iteration 1130: Training Loss = 64.11045837402344 Training Accuracy = 0.030118579044938087\n","Iteration 1131: Training Loss = 63.91190719604492 Training Accuracy = 0.028383823111653328\n","Iteration 1132: Training Loss = 63.601417541503906 Training Accuracy = 0.029560523107647896\n","Iteration 1133: Training Loss = 63.476959228515625 Training Accuracy = 0.029029378667473793\n","Iteration 1134: Training Loss = 63.502349853515625 Training Accuracy = 0.028662031516432762\n","Iteration 1135: Training Loss = 63.62518310546875 Training Accuracy = 0.02933528833091259\n","Iteration 1136: Training Loss = 63.8607177734375 Training Accuracy = 0.028043558821082115\n","Iteration 1137: Training Loss = 63.923980712890625 Training Accuracy = 0.029726509004831314\n","Iteration 1138: Training Loss = 64.1415786743164 Training Accuracy = 0.027934927493333817\n","Iteration 1139: Training Loss = 63.9229621887207 Training Accuracy = 0.03007989004254341\n","Iteration 1140: Training Loss = 63.85585403442383 Training Accuracy = 0.028426317498087883\n","Iteration 1141: Training Loss = 63.59974670410156 Training Accuracy = 0.02989857643842697\n","Iteration 1142: Training Loss = 63.46208190917969 Training Accuracy = 0.029028529301285744\n","Iteration 1143: Training Loss = 63.37370300292969 Training Accuracy = 0.02937425673007965\n","Iteration 1144: Training Loss = 63.35881805419922 Training Accuracy = 0.029283927753567696\n","Iteration 1145: Training Loss = 63.40251541137695 Training Accuracy = 0.028875701129436493\n","Iteration 1146: Training Loss = 63.47773742675781 Training Accuracy = 0.02958284504711628\n","Iteration 1147: Training Loss = 63.63945770263672 Training Accuracy = 0.02849164605140686\n","Iteration 1148: Training Loss = 63.75778579711914 Training Accuracy = 0.03015880100429058\n","Iteration 1149: Training Loss = 64.12268829345703 Training Accuracy = 0.02833205834031105\n","Iteration 1150: Training Loss = 64.07381439208984 Training Accuracy = 0.030774511396884918\n","Iteration 1151: Training Loss = 64.34378051757812 Training Accuracy = 0.028559992089867592\n","Iteration 1152: Training Loss = 63.83965301513672 Training Accuracy = 0.030830632895231247\n","Iteration 1153: Training Loss = 63.577667236328125 Training Accuracy = 0.02917591482400894\n","Iteration 1154: Training Loss = 63.32512664794922 Training Accuracy = 0.030002955347299576\n","Iteration 1155: Training Loss = 63.2653694152832 Training Accuracy = 0.029620599001646042\n","Iteration 1156: Training Loss = 63.34983825683594 Training Accuracy = 0.029020769521594048\n","Iteration 1157: Training Loss = 63.50559997558594 Training Accuracy = 0.029981952160596848\n","Iteration 1158: Training Loss = 63.77947998046875 Training Accuracy = 0.02850399725139141\n","Iteration 1159: Training Loss = 63.778297424316406 Training Accuracy = 0.03043186664581299\n","Iteration 1160: Training Loss = 63.934539794921875 Training Accuracy = 0.028603125363588333\n","Iteration 1161: Training Loss = 63.648197174072266 Training Accuracy = 0.0306173674762249\n","Iteration 1162: Training Loss = 63.50563049316406 Training Accuracy = 0.029175998643040657\n","Iteration 1163: Training Loss = 63.29801559448242 Training Accuracy = 0.03024272993206978\n","Iteration 1164: Training Loss = 63.19891357421875 Training Accuracy = 0.029667772352695465\n","Iteration 1165: Training Loss = 63.179229736328125 Training Accuracy = 0.029602592810988426\n","Iteration 1166: Training Loss = 63.22169494628906 Training Accuracy = 0.029940199106931686\n","Iteration 1167: Training Loss = 63.3277587890625 Training Accuracy = 0.029190178960561752\n","Iteration 1168: Training Loss = 63.44664001464844 Training Accuracy = 0.030396172776818275\n","Iteration 1169: Training Loss = 63.748775482177734 Training Accuracy = 0.028872746974229813\n","Iteration 1170: Training Loss = 63.82959747314453 Training Accuracy = 0.03105822764337063\n","Iteration 1171: Training Loss = 64.26426696777344 Training Accuracy = 0.028771407902240753\n","Iteration 1172: Training Loss = 63.89196014404297 Training Accuracy = 0.03136596456170082\n","Iteration 1173: Training Loss = 63.794952392578125 Training Accuracy = 0.02929704450070858\n","Iteration 1174: Training Loss = 63.349937438964844 Training Accuracy = 0.030816752463579178\n","Iteration 1175: Training Loss = 63.13935470581055 Training Accuracy = 0.029844770208001137\n","Iteration 1176: Training Loss = 63.0999755859375 Training Accuracy = 0.029800808057188988\n","Iteration 1177: Training Loss = 63.199405670166016 Training Accuracy = 0.030176658183336258\n","Iteration 1178: Training Loss = 63.41753005981445 Training Accuracy = 0.02901575341820717\n","Iteration 1179: Training Loss = 63.53640365600586 Training Accuracy = 0.030610671266913414\n","Iteration 1180: Training Loss = 63.80614471435547 Training Accuracy = 0.028791602700948715\n","Iteration 1181: Training Loss = 63.586299896240234 Training Accuracy = 0.03096747398376465\n","Iteration 1182: Training Loss = 63.517818450927734 Training Accuracy = 0.02928730845451355\n","Iteration 1183: Training Loss = 63.239227294921875 Training Accuracy = 0.030769705772399902\n","Iteration 1184: Training Loss = 63.08594512939453 Training Accuracy = 0.029955420643091202\n","Iteration 1185: Training Loss = 63.0107307434082 Training Accuracy = 0.03021564707159996\n","Iteration 1186: Training Loss = 63.0252571105957 Training Accuracy = 0.03030187077820301\n","Iteration 1187: Training Loss = 63.10776138305664 Training Accuracy = 0.029702380299568176\n","Iteration 1188: Training Loss = 63.21004104614258 Training Accuracy = 0.03069474920630455\n","Iteration 1189: Training Loss = 63.433349609375 Training Accuracy = 0.029351189732551575\n","Iteration 1190: Training Loss = 63.52273178100586 Training Accuracy = 0.031282227486371994\n","Iteration 1191: Training Loss = 63.88468933105469 Training Accuracy = 0.029265837743878365\n","Iteration 1192: Training Loss = 63.6779670715332 Training Accuracy = 0.031710099428892136\n","Iteration 1193: Training Loss = 63.7479248046875 Training Accuracy = 0.029609736055135727\n","Iteration 1194: Training Loss = 63.32911682128906 Training Accuracy = 0.03147559612989426\n","Iteration 1195: Training Loss = 63.109275817871094 Training Accuracy = 0.03012125939130783\n","Iteration 1196: Training Loss = 62.95085906982422 Training Accuracy = 0.030678847804665565\n","Iteration 1197: Training Loss = 62.922630310058594 Training Accuracy = 0.030431080609560013\n","Iteration 1198: Training Loss = 62.98930358886719 Training Accuracy = 0.029933609068393707\n","Iteration 1199: Training Loss = 63.11006546020508 Training Accuracy = 0.030841348692774773\n","Iteration 1200: Training Loss = 63.350894927978516 Training Accuracy = 0.029517048969864845\n","Iteration 1201: Training Loss = 63.42837142944336 Training Accuracy = 0.03141677379608154\n","Iteration 1202: Training Loss = 63.722198486328125 Training Accuracy = 0.029498809948563576\n","Iteration 1203: Training Loss = 63.47781753540039 Training Accuracy = 0.0317663699388504\n","Iteration 1204: Training Loss = 63.422977447509766 Training Accuracy = 0.029951445758342743\n","Iteration 1205: Training Loss = 63.10169982910156 Training Accuracy = 0.031485822051763535\n","Iteration 1206: Training Loss = 62.93115997314453 Training Accuracy = 0.030515922233462334\n","Iteration 1207: Training Loss = 62.847328186035156 Training Accuracy = 0.030804166570305824\n","Iteration 1208: Training Loss = 62.85658264160156 Training Accuracy = 0.030828125774860382\n","Iteration 1209: Training Loss = 62.938255310058594 Training Accuracy = 0.03022918850183487\n","Iteration 1210: Training Loss = 63.055091857910156 Training Accuracy = 0.03124772571027279\n","Iteration 1211: Training Loss = 63.307010650634766 Training Accuracy = 0.029867645353078842\n","Iteration 1212: Training Loss = 63.3936882019043 Training Accuracy = 0.03184936195611954\n","Iteration 1213: Training Loss = 63.75543212890625 Training Accuracy = 0.029773149639368057\n","Iteration 1214: Training Loss = 63.502742767333984 Training Accuracy = 0.032202381640672684\n","Iteration 1215: Training Loss = 63.500938415527344 Training Accuracy = 0.030157269909977913\n","Iteration 1216: Training Loss = 63.09698486328125 Training Accuracy = 0.031861480325460434\n","Iteration 1217: Training Loss = 62.888710021972656 Training Accuracy = 0.0306796133518219\n","Iteration 1218: Training Loss = 62.7738037109375 Training Accuracy = 0.03104693815112114\n","Iteration 1219: Training Loss = 62.77598190307617 Training Accuracy = 0.03098973259329796\n","Iteration 1220: Training Loss = 62.86919403076172 Training Accuracy = 0.030355866998434067\n","Iteration 1221: Training Loss = 63.00081253051758 Training Accuracy = 0.03141254186630249\n","Iteration 1222: Training Loss = 63.26323699951172 Training Accuracy = 0.02996860072016716\n","Iteration 1223: Training Loss = 63.296409606933594 Training Accuracy = 0.03195803612470627\n","Iteration 1224: Training Loss = 63.54232406616211 Training Accuracy = 0.030005570501089096\n","Iteration 1225: Training Loss = 63.27471923828125 Training Accuracy = 0.03220763057470322\n","Iteration 1226: Training Loss = 63.19082260131836 Training Accuracy = 0.030499426648020744\n","Iteration 1227: Training Loss = 62.90402603149414 Training Accuracy = 0.031884800642728806\n","Iteration 1228: Training Loss = 62.75735855102539 Training Accuracy = 0.030966773629188538\n","Iteration 1229: Training Loss = 62.687294006347656 Training Accuracy = 0.03124251775443554\n","Iteration 1230: Training Loss = 62.692787170410156 Training Accuracy = 0.0312710665166378\n","Iteration 1231: Training Loss = 62.762290954589844 Training Accuracy = 0.030722832307219505\n","Iteration 1232: Training Loss = 62.86930465698242 Training Accuracy = 0.031700510531663895\n","Iteration 1233: Training Loss = 63.09915542602539 Training Accuracy = 0.030391879379749298\n","Iteration 1234: Training Loss = 63.20104217529297 Training Accuracy = 0.03229908645153046\n","Iteration 1235: Training Loss = 63.581974029541016 Training Accuracy = 0.030271831899881363\n","Iteration 1236: Training Loss = 63.38175964355469 Training Accuracy = 0.03271166980266571\n","Iteration 1237: Training Loss = 63.45337677001953 Training Accuracy = 0.03056628443300724\n","Iteration 1238: Training Loss = 63.002845764160156 Training Accuracy = 0.03245903551578522\n","Iteration 1239: Training Loss = 62.77653503417969 Training Accuracy = 0.03106747381389141\n","Iteration 1240: Training Loss = 62.623321533203125 Training Accuracy = 0.03164440765976906\n","Iteration 1241: Training Loss = 62.59542465209961 Training Accuracy = 0.03143371641635895\n","Iteration 1242: Training Loss = 62.66154098510742 Training Accuracy = 0.030935565009713173\n","Iteration 1243: Training Loss = 62.78349304199219 Training Accuracy = 0.03186624124646187\n","Iteration 1244: Training Loss = 63.02149200439453 Training Accuracy = 0.030535204336047173\n","Iteration 1245: Training Loss = 63.079742431640625 Training Accuracy = 0.03239600360393524\n","Iteration 1246: Training Loss = 63.334068298339844 Training Accuracy = 0.030487116426229477\n","Iteration 1247: Training Loss = 63.11073303222656 Training Accuracy = 0.032654911279678345\n","Iteration 1248: Training Loss = 63.06727600097656 Training Accuracy = 0.030890008434653282\n","Iteration 1249: Training Loss = 62.774620056152344 Training Accuracy = 0.032385822385549545\n","Iteration 1250: Training Loss = 62.62482452392578 Training Accuracy = 0.03134749084711075\n","Iteration 1251: Training Loss = 62.52945327758789 Training Accuracy = 0.03182004764676094\n","Iteration 1252: Training Loss = 62.507102966308594 Training Accuracy = 0.031674277037382126\n","Iteration 1253: Training Loss = 62.54218292236328 Training Accuracy = 0.03133118525147438\n","Iteration 1254: Training Loss = 62.62336349487305 Training Accuracy = 0.032083675265312195\n","Iteration 1255: Training Loss = 62.79501724243164 Training Accuracy = 0.031017836183309555\n","Iteration 1256: Training Loss = 62.91913604736328 Training Accuracy = 0.03267287462949753\n","Iteration 1257: Training Loss = 63.2824821472168 Training Accuracy = 0.030841348692774773\n","Iteration 1258: Training Loss = 63.22609329223633 Training Accuracy = 0.033195748925209045\n","Iteration 1259: Training Loss = 63.48025131225586 Training Accuracy = 0.030957376584410667\n","Iteration 1260: Training Loss = 63.01796340942383 Training Accuracy = 0.03318779915571213\n","Iteration 1261: Training Loss = 62.8066291809082 Training Accuracy = 0.03147251158952713\n","Iteration 1262: Training Loss = 62.543426513671875 Training Accuracy = 0.03250197693705559\n","Iteration 1263: Training Loss = 62.43986511230469 Training Accuracy = 0.03191770985722542\n","Iteration 1264: Training Loss = 62.44913864135742 Training Accuracy = 0.03167533874511719\n","Iteration 1265: Training Loss = 62.546199798583984 Training Accuracy = 0.032248787581920624\n","Iteration 1266: Training Loss = 62.74338912963867 Training Accuracy = 0.031134353950619698\n","Iteration 1267: Training Loss = 62.84821319580078 Training Accuracy = 0.0327240452170372\n","Iteration 1268: Training Loss = 63.11313247680664 Training Accuracy = 0.030976062640547752\n","Iteration 1269: Training Loss = 62.96989822387695 Training Accuracy = 0.03312176838517189\n","Iteration 1270: Training Loss = 63.00203323364258 Training Accuracy = 0.03133069723844528\n","Iteration 1271: Training Loss = 62.70818328857422 Training Accuracy = 0.03303605318069458\n","Iteration 1272: Training Loss = 62.555625915527344 Training Accuracy = 0.03183325007557869\n","Iteration 1273: Training Loss = 62.41844177246094 Training Accuracy = 0.03253603354096413\n","Iteration 1274: Training Loss = 62.36394119262695 Training Accuracy = 0.03213122859597206\n","Iteration 1275: Training Loss = 62.3653450012207 Training Accuracy = 0.032013244926929474\n","Iteration 1276: Training Loss = 62.4139518737793 Training Accuracy = 0.03245121240615845\n","Iteration 1277: Training Loss = 62.52530288696289 Training Accuracy = 0.031647831201553345\n","Iteration 1278: Training Loss = 62.638423919677734 Training Accuracy = 0.03297092020511627\n","Iteration 1279: Training Loss = 62.9173583984375 Training Accuracy = 0.03145204111933708\n","Iteration 1280: Training Loss = 62.98424530029297 Training Accuracy = 0.033555589616298676\n","Iteration 1281: Training Loss = 63.36927032470703 Training Accuracy = 0.03142215311527252\n","Iteration 1282: Training Loss = 63.035091400146484 Training Accuracy = 0.03382128104567528\n","Iteration 1283: Training Loss = 62.97587203979492 Training Accuracy = 0.031755231320858\n","Iteration 1284: Training Loss = 62.58837890625 Training Accuracy = 0.03337400034070015\n","Iteration 1285: Training Loss = 62.39794158935547 Training Accuracy = 0.03224564343690872\n","Iteration 1286: Training Loss = 62.2960090637207 Training Accuracy = 0.03256554156541824\n","Iteration 1287: Training Loss = 62.29752731323242 Training Accuracy = 0.03253399208188057\n","Iteration 1288: Training Loss = 62.3836669921875 Training Accuracy = 0.03192102536559105\n","Iteration 1289: Training Loss = 62.50239181518555 Training Accuracy = 0.03289961814880371\n","Iteration 1290: Training Loss = 62.729801177978516 Training Accuracy = 0.031579189002513885\n","Iteration 1291: Training Loss = 62.7786865234375 Training Accuracy = 0.03343258798122406\n","Iteration 1292: Training Loss = 63.014408111572266 Training Accuracy = 0.0315980426967144\n","Iteration 1293: Training Loss = 62.79635238647461 Training Accuracy = 0.03370659053325653\n","Iteration 1294: Training Loss = 62.75223922729492 Training Accuracy = 0.032002657651901245\n","Iteration 1295: Training Loss = 62.480281829833984 Training Accuracy = 0.033448003232479095\n","Iteration 1296: Training Loss = 62.340789794921875 Training Accuracy = 0.0324099063873291\n","Iteration 1297: Training Loss = 62.2436637878418 Training Accuracy = 0.03291262686252594\n","Iteration 1298: Training Loss = 62.21324157714844 Training Accuracy = 0.03266016021370888\n","Iteration 1299: Training Loss = 62.234657287597656 Training Accuracy = 0.03240378573536873\n","Iteration 1300: Training Loss = 62.29616928100586 Training Accuracy = 0.0330083966255188\n","Iteration 1301: Training Loss = 62.42156982421875 Training Accuracy = 0.032105930149555206\n","Iteration 1302: Training Loss = 62.53788757324219 Training Accuracy = 0.03353171795606613\n","Iteration 1303: Training Loss = 62.85036087036133 Training Accuracy = 0.03194897994399071\n","Iteration 1304: Training Loss = 62.88612365722656 Training Accuracy = 0.03410537913441658\n","Iteration 1305: Training Loss = 63.235923767089844 Training Accuracy = 0.03190023452043533\n","Iteration 1306: Training Loss = 62.8486213684082 Training Accuracy = 0.03426996245980263\n","Iteration 1307: Training Loss = 62.729278564453125 Training Accuracy = 0.032292984426021576\n","Iteration 1308: Training Loss = 62.379032135009766 Training Accuracy = 0.03370761126279831\n","Iteration 1309: Training Loss = 62.20386505126953 Training Accuracy = 0.03271891921758652\n","Iteration 1310: Training Loss = 62.126766204833984 Training Accuracy = 0.032876573503017426\n","Iteration 1311: Training Loss = 62.153709411621094 Training Accuracy = 0.03294483572244644\n","Iteration 1312: Training Loss = 62.256324768066406 Training Accuracy = 0.03227633982896805\n","Iteration 1313: Training Loss = 62.37228012084961 Training Accuracy = 0.033354293555021286\n","Iteration 1314: Training Loss = 62.600303649902344 Training Accuracy = 0.03199804574251175\n","Iteration 1315: Training Loss = 62.62797546386719 Training Accuracy = 0.033906590193510056\n","Iteration 1316: Training Loss = 62.84226608276367 Training Accuracy = 0.032059118151664734\n","Iteration 1317: Training Loss = 62.60927200317383 Training Accuracy = 0.03409651294350624\n","Iteration 1318: Training Loss = 62.5450439453125 Training Accuracy = 0.032431356608867645\n","Iteration 1319: Training Loss = 62.29001235961914 Training Accuracy = 0.03375926986336708\n","Iteration 1320: Training Loss = 62.15976333618164 Training Accuracy = 0.03276698663830757\n","Iteration 1321: Training Loss = 62.07544708251953 Training Accuracy = 0.033226191997528076\n","Iteration 1322: Training Loss = 62.05010986328125 Training Accuracy = 0.03300438076257706\n","Iteration 1323: Training Loss = 62.069034576416016 Training Accuracy = 0.032766178250312805\n","Iteration 1324: Training Loss = 62.12247085571289 Training Accuracy = 0.03336643427610397\n","Iteration 1325: Training Loss = 62.232765197753906 Training Accuracy = 0.032546088099479675\n","Iteration 1326: Training Loss = 62.343074798583984 Training Accuracy = 0.03388609737157822\n","Iteration 1327: Training Loss = 62.634891510009766 Training Accuracy = 0.0323789119720459\n","Iteration 1328: Training Loss = 62.695655822753906 Training Accuracy = 0.03445014730095863\n","Iteration 1329: Training Loss = 63.072017669677734 Training Accuracy = 0.03226490318775177\n","Iteration 1330: Training Loss = 62.725826263427734 Training Accuracy = 0.03466881439089775\n","Iteration 1331: Training Loss = 62.658390045166016 Training Accuracy = 0.03264020010828972\n","Iteration 1332: Training Loss = 62.27439880371094 Training Accuracy = 0.034213755279779434\n","Iteration 1333: Training Loss = 62.08757019042969 Training Accuracy = 0.033120810985565186\n","Iteration 1334: Training Loss = 61.98715591430664 Training Accuracy = 0.03344111517071724\n","Iteration 1335: Training Loss = 61.985782623291016 Training Accuracy = 0.033360693603754044\n","Iteration 1336: Training Loss = 62.06121826171875 Training Accuracy = 0.03281928226351738\n","Iteration 1337: Training Loss = 62.169490814208984 Training Accuracy = 0.0337069071829319\n","Iteration 1338: Training Loss = 62.372737884521484 Training Accuracy = 0.03246592357754707\n","Iteration 1339: Training Loss = 62.43313980102539 Training Accuracy = 0.03422999754548073\n","Iteration 1340: Training Loss = 62.66058349609375 Training Accuracy = 0.03249632194638252\n","Iteration 1341: Training Loss = 62.479862213134766 Training Accuracy = 0.034544918686151505\n","Iteration 1342: Training Loss = 62.46990966796875 Training Accuracy = 0.03285703808069229\n","Iteration 1343: Training Loss = 62.208133697509766 Training Accuracy = 0.034375447779893875\n","Iteration 1344: Training Loss = 62.07524108886719 Training Accuracy = 0.03323465213179588\n","Iteration 1345: Training Loss = 61.96162796020508 Training Accuracy = 0.033884014934301376\n","Iteration 1346: Training Loss = 61.91178512573242 Training Accuracy = 0.0334576740860939\n","Iteration 1347: Training Loss = 61.9051513671875 Training Accuracy = 0.03339989483356476\n","Iteration 1348: Training Loss = 61.93244934082031 Training Accuracy = 0.03371613472700119\n","Iteration 1349: Training Loss = 61.99744415283203 Training Accuracy = 0.033142220228910446\n","Iteration 1350: Training Loss = 62.080692291259766 Training Accuracy = 0.03416728228330612\n","Iteration 1351: Training Loss = 62.279170989990234 Training Accuracy = 0.03300146758556366\n","Iteration 1352: Training Loss = 62.40350341796875 Training Accuracy = 0.0347815677523613\n","Iteration 1353: Training Loss = 62.81511688232422 Training Accuracy = 0.03283475711941719\n","Iteration 1354: Training Loss = 62.673667907714844 Training Accuracy = 0.035237498581409454\n","Iteration 1355: Training Loss = 62.85082244873047 Training Accuracy = 0.032956864684820175\n","Iteration 1356: Training Loss = 62.36799240112305 Training Accuracy = 0.03505125269293785\n","Iteration 1357: Training Loss = 62.14765930175781 Training Accuracy = 0.03344821557402611\n","Iteration 1358: Training Loss = 61.92255401611328 Training Accuracy = 0.03428333252668381\n","Iteration 1359: Training Loss = 61.836814880371094 Training Accuracy = 0.033735886216163635\n","Iteration 1360: Training Loss = 61.844444274902344 Training Accuracy = 0.0335301011800766\n","Iteration 1361: Training Loss = 61.92136764526367 Training Accuracy = 0.034014008939266205\n","Iteration 1362: Training Loss = 62.073665618896484 Training Accuracy = 0.033079080283641815\n","Iteration 1363: Training Loss = 62.18119430541992 Training Accuracy = 0.03452315181493759\n","Iteration 1364: Training Loss = 62.425506591796875 Training Accuracy = 0.0329749770462513\n","Iteration 1365: Training Loss = 62.34809112548828 Training Accuracy = 0.03500911965966225\n","Iteration 1366: Training Loss = 62.436920166015625 Training Accuracy = 0.03324713185429573\n","Iteration 1367: Training Loss = 62.169883728027344 Training Accuracy = 0.03502079099416733\n","Iteration 1368: Training Loss = 62.04914093017578 Training Accuracy = 0.033664263784885406\n","Iteration 1369: Training Loss = 61.8810920715332 Training Accuracy = 0.034614838659763336\n","Iteration 1370: Training Loss = 61.79814147949219 Training Accuracy = 0.03389536589384079\n","Iteration 1371: Training Loss = 61.75751495361328 Training Accuracy = 0.03409464284777641\n","Iteration 1372: Training Loss = 61.75528335571289 Training Accuracy = 0.03409498184919357\n","Iteration 1373: Training Loss = 61.78460693359375 Training Accuracy = 0.033760182559490204\n","Iteration 1374: Training Loss = 61.837669372558594 Training Accuracy = 0.034456633031368256\n","Iteration 1375: Training Loss = 61.95366668701172 Training Accuracy = 0.03362969681620598\n","Iteration 1376: Training Loss = 62.06829071044922 Training Accuracy = 0.03501490131020546\n","Iteration 1377: Training Loss = 62.38302993774414 Training Accuracy = 0.033467281609773636\n","Iteration 1378: Training Loss = 62.44481658935547 Training Accuracy = 0.03560947999358177\n","Iteration 1379: Training Loss = 62.853240966796875 Training Accuracy = 0.03335924819111824\n","Iteration 1380: Training Loss = 62.463706970214844 Training Accuracy = 0.03579457849264145\n","Iteration 1381: Training Loss = 62.38298797607422 Training Accuracy = 0.033722471445798874\n","Iteration 1382: Training Loss = 61.983917236328125 Training Accuracy = 0.035263966768980026\n","Iteration 1383: Training Loss = 61.7938346862793 Training Accuracy = 0.0341358408331871\n","Iteration 1384: Training Loss = 61.69392776489258 Training Accuracy = 0.03443896770477295\n","Iteration 1385: Training Loss = 61.69355010986328 Training Accuracy = 0.03433329239487648\n","Iteration 1386: Training Loss = 61.76940155029297 Training Accuracy = 0.033783480525016785\n","Iteration 1387: Training Loss = 61.87889099121094 Training Accuracy = 0.03468373790383339\n","Iteration 1388: Training Loss = 62.08051300048828 Training Accuracy = 0.03345155343413353\n","Iteration 1389: Training Loss = 62.127017974853516 Training Accuracy = 0.03518649935722351\n","Iteration 1390: Training Loss = 62.33076477050781 Training Accuracy = 0.03350356966257095\n","Iteration 1391: Training Loss = 62.148353576660156 Training Accuracy = 0.03548962622880936\n","Iteration 1392: Training Loss = 62.114315032958984 Training Accuracy = 0.03391101211309433\n","Iteration 1393: Training Loss = 61.878173828125 Training Accuracy = 0.03533635288476944\n","Iteration 1394: Training Loss = 61.7592887878418 Training Accuracy = 0.034251149743795395\n","Iteration 1395: Training Loss = 61.65969467163086 Training Accuracy = 0.03483046218752861\n","Iteration 1396: Training Loss = 61.61420822143555 Training Accuracy = 0.034379273653030396\n","Iteration 1397: Training Loss = 61.60257339477539 Training Accuracy = 0.034353017807006836\n","Iteration 1398: Training Loss = 61.61956787109375 Training Accuracy = 0.034600041806697845\n","Iteration 1399: Training Loss = 61.6682014465332 Training Accuracy = 0.03414479270577431\n","Iteration 1400: Training Loss = 61.73767852783203 Training Accuracy = 0.035076871514320374\n","Iteration 1401: Training Loss = 61.89619827270508 Training Accuracy = 0.03409266471862793\n","Iteration 1402: Training Loss = 62.02531814575195 Training Accuracy = 0.03572191670536995\n","Iteration 1403: Training Loss = 62.413631439208984 Training Accuracy = 0.033931419253349304\n","Iteration 1404: Training Loss = 62.38164138793945 Training Accuracy = 0.03621307387948036\n","Iteration 1405: Training Loss = 62.70208740234375 Training Accuracy = 0.03387714549899101\n","Iteration 1406: Training Loss = 62.21342468261719 Training Accuracy = 0.03617270290851593\n","Iteration 1407: Training Loss = 62.02614974975586 Training Accuracy = 0.034313224256038666\n","Iteration 1408: Training Loss = 61.71735763549805 Training Accuracy = 0.03549625724554062\n","Iteration 1409: Training Loss = 61.579654693603516 Training Accuracy = 0.03466670960187912\n","Iteration 1410: Training Loss = 61.536842346191406 Training Accuracy = 0.034717898815870285\n","Iteration 1411: Training Loss = 61.574134826660156 Training Accuracy = 0.03487153351306915\n","Iteration 1412: Training Loss = 61.67955017089844 Training Accuracy = 0.03417901694774628\n","Iteration 1413: Training Loss = 61.792083740234375 Training Accuracy = 0.035286203026771545\n","Iteration 1414: Training Loss = 62.01072311401367 Training Accuracy = 0.033942196518182755\n","Iteration 1415: Training Loss = 62.01860046386719 Training Accuracy = 0.0357728935778141\n","Iteration 1416: Training Loss = 62.1785888671875 Training Accuracy = 0.03408171609044075\n","Iteration 1417: Training Loss = 61.960289001464844 Training Accuracy = 0.03595912083983421\n","Iteration 1418: Training Loss = 61.886817932128906 Training Accuracy = 0.03448040038347244\n","Iteration 1419: Training Loss = 61.68252182006836 Training Accuracy = 0.035689305514097214\n","Iteration 1420: Training Loss = 61.57747268676758 Training Accuracy = 0.034754678606987\n","Iteration 1421: Training Loss = 61.503623962402344 Training Accuracy = 0.03520943969488144\n","Iteration 1422: Training Loss = 61.474510192871094 Training Accuracy = 0.0349176861345768\n","Iteration 1423: Training Loss = 61.47563171386719 Training Accuracy = 0.034817006438970566\n","Iteration 1424: Training Loss = 61.50162124633789 Training Accuracy = 0.03518373891711235\n","Iteration 1425: Training Loss = 61.56024932861328 Training Accuracy = 0.03465036302804947\n","Iteration 1426: Training Loss = 61.63734817504883 Training Accuracy = 0.03566522151231766\n","Iteration 1427: Training Loss = 61.82033920288086 Training Accuracy = 0.03457750752568245\n","Iteration 1428: Training Loss = 61.94562530517578 Training Accuracy = 0.036264605820178986\n","Iteration 1429: Training Loss = 62.34121322631836 Training Accuracy = 0.0343974269926548\n","Iteration 1430: Training Loss = 62.247352600097656 Training Accuracy = 0.03670971468091011\n","Iteration 1431: Training Loss = 62.485225677490234 Training Accuracy = 0.03443703055381775\n","Iteration 1432: Training Loss = 62.011932373046875 Training Accuracy = 0.03658926486968994\n","Iteration 1433: Training Loss = 61.82041931152344 Training Accuracy = 0.03486471250653267\n","Iteration 1434: Training Loss = 61.558048248291016 Training Accuracy = 0.03588709607720375\n","Iteration 1435: Training Loss = 61.44192123413086 Training Accuracy = 0.03512553125619888\n","Iteration 1436: Training Loss = 61.40879440307617 Training Accuracy = 0.035155847668647766\n","Iteration 1437: Training Loss = 61.44202423095703 Training Accuracy = 0.03531169146299362\n","Iteration 1438: Training Loss = 61.53184127807617 Training Accuracy = 0.03467955067753792\n","Iteration 1439: Training Loss = 61.631961822509766 Training Accuracy = 0.0357360765337944\n","Iteration 1440: Training Loss = 61.82695770263672 Training Accuracy = 0.0345059297978878\n","Iteration 1441: Training Loss = 61.85740280151367 Training Accuracy = 0.036241836845874786\n","Iteration 1442: Training Loss = 62.039730072021484 Training Accuracy = 0.03459755703806877\n","Iteration 1443: Training Loss = 61.862491607666016 Training Accuracy = 0.03647210821509361\n","Iteration 1444: Training Loss = 61.838890075683594 Training Accuracy = 0.03489113599061966\n","Iteration 1445: Training Loss = 61.622535705566406 Training Accuracy = 0.03625410422682762\n","Iteration 1446: Training Loss = 61.51861572265625 Training Accuracy = 0.03512703999876976\n","Iteration 1447: Training Loss = 61.4180908203125 Training Accuracy = 0.035804592072963715\n","Iteration 1448: Training Loss = 61.36793899536133 Training Accuracy = 0.03528226912021637\n","Iteration 1449: Training Loss = 61.344425201416016 Training Accuracy = 0.03544814884662628\n","Iteration 1450: Training Loss = 61.341453552246094 Training Accuracy = 0.035497043281793594\n","Iteration 1451: Training Loss = 61.35415267944336 Training Accuracy = 0.03533133491873741\n","Iteration 1452: Training Loss = 61.381134033203125 Training Accuracy = 0.0358787402510643\n","Iteration 1453: Training Loss = 61.441776275634766 Training Accuracy = 0.035316623747348785\n","Iteration 1454: Training Loss = 61.521759033203125 Training Accuracy = 0.0363648384809494\n","Iteration 1455: Training Loss = 61.72726821899414 Training Accuracy = 0.03518718108534813\n","Iteration 1456: Training Loss = 61.87433624267578 Training Accuracy = 0.03692967817187309\n","Iteration 1457: Training Loss = 62.3558464050293 Training Accuracy = 0.034914348274469376\n","Iteration 1458: Training Loss = 62.21346664428711 Training Accuracy = 0.037369899451732635\n","Iteration 1459: Training Loss = 62.46869659423828 Training Accuracy = 0.0349593311548233\n","Iteration 1460: Training Loss = 61.90219497680664 Training Accuracy = 0.03719453513622284\n","Iteration 1461: Training Loss = 61.662139892578125 Training Accuracy = 0.03547436371445656\n","Iteration 1462: Training Loss = 61.39869689941406 Training Accuracy = 0.03637625277042389\n","Iteration 1463: Training Loss = 61.294822692871094 Training Accuracy = 0.035714879631996155\n","Iteration 1464: Training Loss = 61.28955841064453 Training Accuracy = 0.0355568453669548\n","Iteration 1465: Training Loss = 61.3576774597168 Training Accuracy = 0.03588711842894554\n","Iteration 1466: Training Loss = 61.49705123901367 Training Accuracy = 0.03504215553402901\n","Iteration 1467: Training Loss = 61.603538513183594 Training Accuracy = 0.03633278235793114\n","Iteration 1468: Training Loss = 61.82154846191406 Training Accuracy = 0.034891899675130844\n","Iteration 1469: Training Loss = 61.75688171386719 Training Accuracy = 0.03676311671733856\n","Iteration 1470: Training Loss = 61.830535888671875 Training Accuracy = 0.035150978714227676\n","Iteration 1471: Training Loss = 61.60523223876953 Training Accuracy = 0.03677595779299736\n","Iteration 1472: Training Loss = 61.50496292114258 Training Accuracy = 0.035500913858413696\n","Iteration 1473: Training Loss = 61.354286193847656 Training Accuracy = 0.036410268396139145\n","Iteration 1474: Training Loss = 61.27768325805664 Training Accuracy = 0.035679761320352554\n","Iteration 1475: Training Loss = 61.23456573486328 Training Accuracy = 0.03598346188664436\n","Iteration 1476: Training Loss = 61.221195220947266 Training Accuracy = 0.03586730360984802\n","Iteration 1477: Training Loss = 61.229042053222656 Training Accuracy = 0.03573694825172424\n","Iteration 1478: Training Loss = 61.25554656982422 Training Accuracy = 0.03621445596218109\n","Iteration 1479: Training Loss = 61.31272888183594 Training Accuracy = 0.035689305514097214\n","Iteration 1480: Training Loss = 61.38474655151367 Training Accuracy = 0.03668307885527611\n","Iteration 1481: Training Loss = 61.56084442138672 Training Accuracy = 0.03559136763215065\n","Iteration 1482: Training Loss = 61.68510055541992 Training Accuracy = 0.037216730415821075\n","Iteration 1483: Training Loss = 62.085243225097656 Training Accuracy = 0.03535197675228119\n","Iteration 1484: Training Loss = 62.013038635253906 Training Accuracy = 0.03762183338403702\n","Iteration 1485: Training Loss = 62.295555114746094 Training Accuracy = 0.0353396050632\n","Iteration 1486: Training Loss = 61.80965805053711 Training Accuracy = 0.03755272179841995\n","Iteration 1487: Training Loss = 61.627967834472656 Training Accuracy = 0.035771384835243225\n","Iteration 1488: Training Loss = 61.339271545410156 Training Accuracy = 0.03689868375658989\n","Iteration 1489: Training Loss = 61.209285736083984 Training Accuracy = 0.036057014018297195\n","Iteration 1490: Training Loss = 61.16131591796875 Training Accuracy = 0.03616753965616226\n","Iteration 1491: Training Loss = 61.182098388671875 Training Accuracy = 0.03622189536690712\n","Iteration 1492: Training Loss = 61.25880813598633 Training Accuracy = 0.035650063306093216\n","Iteration 1493: Training Loss = 61.35374069213867 Training Accuracy = 0.0365896038711071\n","Iteration 1494: Training Loss = 61.532711029052734 Training Accuracy = 0.03543877601623535\n","Iteration 1495: Training Loss = 61.575313568115234 Training Accuracy = 0.03707421198487282\n","Iteration 1496: Training Loss = 61.75993728637695 Training Accuracy = 0.03551050275564194\n","Iteration 1497: Training Loss = 61.616661071777344 Training Accuracy = 0.037345387041568756\n","Iteration 1498: Training Loss = 61.6252326965332 Training Accuracy = 0.035767581313848495\n","Iteration 1499: Training Loss = 61.413970947265625 Training Accuracy = 0.03718513995409012\n","Iteration 1500: Training Loss = 61.31937789916992 Training Accuracy = 0.03597995266318321\n","Iteration 1501: Training Loss = 61.20450973510742 Training Accuracy = 0.03676532581448555\n","Iteration 1502: Training Loss = 61.14520263671875 Training Accuracy = 0.03610144555568695\n","Iteration 1503: Training Loss = 61.10906982421875 Training Accuracy = 0.03639853373169899\n","Iteration 1504: Training Loss = 61.09477233886719 Training Accuracy = 0.03629105165600777\n","Iteration 1505: Training Loss = 61.09488296508789 Training Accuracy = 0.03626738861203194\n","Iteration 1506: Training Loss = 61.10597610473633 Training Accuracy = 0.036594707518815994\n","Iteration 1507: Training Loss = 61.13359069824219 Training Accuracy = 0.03625907748937607\n","Iteration 1508: Training Loss = 61.17837142944336 Training Accuracy = 0.03700074553489685\n","Iteration 1509: Training Loss = 61.28762435913086 Training Accuracy = 0.03617652878165245\n","Iteration 1510: Training Loss = 61.409889221191406 Training Accuracy = 0.03747623413801193\n","Iteration 1511: Training Loss = 61.75987243652344 Training Accuracy = 0.035910267382860184\n","Iteration 1512: Training Loss = 61.87576675415039 Training Accuracy = 0.03803297132253647\n","Iteration 1513: Training Loss = 62.43192672729492 Training Accuracy = 0.0356588214635849\n","Iteration 1514: Training Loss = 61.94093704223633 Training Accuracy = 0.03825138136744499\n","Iteration 1515: Training Loss = 61.85049057006836 Training Accuracy = 0.036058034747838974\n","Iteration 1516: Training Loss = 61.377227783203125 Training Accuracy = 0.03767145052552223\n","Iteration 1517: Training Loss = 61.16634750366211 Training Accuracy = 0.036487799137830734\n","Iteration 1518: Training Loss = 61.053279876708984 Training Accuracy = 0.03675905615091324\n","Iteration 1519: Training Loss = 61.04704666137695 Training Accuracy = 0.03656528517603874\n","Iteration 1520: Training Loss = 61.118141174316406 Training Accuracy = 0.03607376664876938\n","Iteration 1521: Training Loss = 61.22569274902344 Training Accuracy = 0.03686683624982834\n","Iteration 1522: Training Loss = 61.422035217285156 Training Accuracy = 0.03571740910410881\n","Iteration 1523: Training Loss = 61.468353271484375 Training Accuracy = 0.037375934422016144\n","Iteration 1524: Training Loss = 61.64143371582031 Training Accuracy = 0.03578607738018036\n","Iteration 1525: Training Loss = 61.46428298950195 Training Accuracy = 0.03759842738509178\n","Iteration 1526: Training Loss = 61.42903518676758 Training Accuracy = 0.03614268824458122\n","Iteration 1527: Training Loss = 61.226890563964844 Training Accuracy = 0.03737308830022812\n","Iteration 1528: Training Loss = 61.12391662597656 Training Accuracy = 0.036365240812301636\n","Iteration 1529: Training Loss = 61.036468505859375 Training Accuracy = 0.03694228455424309\n","Iteration 1530: Training Loss = 60.99585723876953 Training Accuracy = 0.036490730941295624\n","Iteration 1531: Training Loss = 60.98106384277344 Training Accuracy = 0.036578573286533356\n","Iteration 1532: Training Loss = 60.98542022705078 Training Accuracy = 0.03674623742699623\n","Iteration 1533: Training Loss = 61.00934600830078 Training Accuracy = 0.03645914047956467\n","Iteration 1534: Training Loss = 61.05125045776367 Training Accuracy = 0.037129975855350494\n","Iteration 1535: Training Loss = 61.140281677246094 Training Accuracy = 0.036425042897462845\n","Iteration 1536: Training Loss = 61.228240966796875 Training Accuracy = 0.03761250153183937\n","Iteration 1537: Training Loss = 61.47227478027344 Training Accuracy = 0.03624759614467621\n","Iteration 1538: Training Loss = 61.57416534423828 Training Accuracy = 0.038099512457847595\n","Iteration 1539: Training Loss = 61.99468994140625 Training Accuracy = 0.03605357185006142\n","Iteration 1540: Training Loss = 61.74705505371094 Training Accuracy = 0.03834521770477295\n","Iteration 1541: Training Loss = 61.82730484008789 Training Accuracy = 0.03623012453317642\n","Iteration 1542: Training Loss = 61.38694763183594 Training Accuracy = 0.03808475658297539\n","Iteration 1543: Training Loss = 61.198116302490234 Training Accuracy = 0.03662450984120369\n","Iteration 1544: Training Loss = 61.010623931884766 Training Accuracy = 0.03738756477832794\n","Iteration 1545: Training Loss = 60.93269348144531 Training Accuracy = 0.03679787367582321\n","Iteration 1546: Training Loss = 60.92143249511719 Training Accuracy = 0.03672163933515549\n","Iteration 1547: Training Loss = 60.96171188354492 Training Accuracy = 0.03699266538023949\n","Iteration 1548: Training Loss = 61.051029205322266 Training Accuracy = 0.03635072335600853\n","Iteration 1549: Training Loss = 61.14118194580078 Training Accuracy = 0.037409354001283646\n","Iteration 1550: Training Loss = 61.326541900634766 Training Accuracy = 0.036229103803634644\n","Iteration 1551: Training Loss = 61.35688400268555 Training Accuracy = 0.03790333867073059\n","Iteration 1552: Training Loss = 61.544132232666016 Training Accuracy = 0.0362955778837204\n","Iteration 1553: Training Loss = 61.384334564208984 Training Accuracy = 0.0380941741168499\n","Iteration 1554: Training Loss = 61.38801193237305 Training Accuracy = 0.03650080785155296\n","Iteration 1555: Training Loss = 61.171199798583984 Training Accuracy = 0.03787064179778099\n","Iteration 1556: Training Loss = 61.07881546020508 Training Accuracy = 0.036654356867074966\n","Iteration 1557: Training Loss = 60.96569061279297 Training Accuracy = 0.03743242099881172\n","Iteration 1558: Training Loss = 60.908329010009766 Training Accuracy = 0.03676987811923027\n","Iteration 1559: Training Loss = 60.87207794189453 Training Accuracy = 0.037094198167324066\n","Iteration 1560: Training Loss = 60.855712890625 Training Accuracy = 0.03693998605012894\n","Iteration 1561: Training Loss = 60.8509635925293 Training Accuracy = 0.03699934110045433\n","Iteration 1562: Training Loss = 60.853851318359375 Training Accuracy = 0.03722863644361496\n","Iteration 1563: Training Loss = 60.86616897583008 Training Accuracy = 0.03701809048652649\n","Iteration 1564: Training Loss = 60.889862060546875 Training Accuracy = 0.037566687911748886\n","Iteration 1565: Training Loss = 60.94732666015625 Training Accuracy = 0.03699043393135071\n","Iteration 1566: Training Loss = 61.02566146850586 Training Accuracy = 0.037968240678310394\n","Iteration 1567: Training Loss = 61.23628234863281 Training Accuracy = 0.03681369125843048\n","Iteration 1568: Training Loss = 61.402645111083984 Training Accuracy = 0.0385221503674984\n","Iteration 1569: Training Loss = 61.9452018737793 Training Accuracy = 0.03648722544312477\n","Iteration 1570: Training Loss = 61.79764175415039 Training Accuracy = 0.038975149393081665\n","Iteration 1571: Training Loss = 62.097198486328125 Training Accuracy = 0.03652544692158699\n","Iteration 1572: Training Loss = 61.455081939697266 Training Accuracy = 0.038787372410297394\n","Iteration 1573: Training Loss = 61.18859100341797 Training Accuracy = 0.03705654665827751\n","Iteration 1574: Training Loss = 60.91063690185547 Training Accuracy = 0.03790297731757164\n","Iteration 1575: Training Loss = 60.80693435668945 Training Accuracy = 0.03725246712565422\n","Iteration 1576: Training Loss = 60.80772018432617 Training Accuracy = 0.03705820441246033\n","Iteration 1577: Training Loss = 60.88251495361328 Training Accuracy = 0.03737897425889969\n","Iteration 1578: Training Loss = 61.035057067871094 Training Accuracy = 0.036519814282655716\n","Iteration 1579: Training Loss = 61.14365768432617 Training Accuracy = 0.03782837837934494\n","Iteration 1580: Training Loss = 61.3733024597168 Training Accuracy = 0.03636758029460907\n","Iteration 1581: Training Loss = 61.276023864746094 Training Accuracy = 0.038220472633838654\n","Iteration 1582: Training Loss = 61.31919479370117 Training Accuracy = 0.03664383664727211\n","Iteration 1583: Training Loss = 61.087467193603516 Training Accuracy = 0.0381350964307785\n","Iteration 1584: Training Loss = 60.97975158691406 Training Accuracy = 0.03696250170469284\n","Iteration 1585: Training Loss = 60.844444274902344 Training Accuracy = 0.03771689906716347\n","Iteration 1586: Training Loss = 60.777774810791016 Training Accuracy = 0.03709232434630394\n","Iteration 1587: Training Loss = 60.746700286865234 Training Accuracy = 0.03731103241443634\n","Iteration 1588: Training Loss = 60.741600036621094 Training Accuracy = 0.037271108478307724\n","Iteration 1589: Training Loss = 60.75553894042969 Training Accuracy = 0.03709525987505913\n","Iteration 1590: Training Loss = 60.787803649902344 Training Accuracy = 0.03763962537050247\n","Iteration 1591: Training Loss = 60.85721206665039 Training Accuracy = 0.03704691678285599\n","Iteration 1592: Training Loss = 60.9320182800293 Training Accuracy = 0.03809681162238121\n","Iteration 1593: Training Loss = 61.117000579833984 Training Accuracy = 0.0369613952934742\n","Iteration 1594: Training Loss = 61.20530700683594 Training Accuracy = 0.038565561175346375\n","Iteration 1595: Training Loss = 61.54315185546875 Training Accuracy = 0.03675752505660057\n","Iteration 1596: Training Loss = 61.43981170654297 Training Accuracy = 0.038845133036375046\n","Iteration 1597: Training Loss = 61.63400650024414 Training Accuracy = 0.03679615259170532\n","Iteration 1598: Training Loss = 61.253055572509766 Training Accuracy = 0.03874530270695686\n","Iteration 1599: Training Loss = 61.1233024597168 Training Accuracy = 0.03711991757154465\n","Iteration 1600: Training Loss = 60.87834548950195 Training Accuracy = 0.038246046751737595\n","Iteration 1601: Training Loss = 60.76494598388672 Training Accuracy = 0.0373145192861557\n","Iteration 1602: Training Loss = 60.69591522216797 Training Accuracy = 0.03765729069709778\n","Iteration 1603: Training Loss = 60.67538070678711 Training Accuracy = 0.03744396194815636\n","Iteration 1604: Training Loss = 60.688758850097656 Training Accuracy = 0.03726528584957123\n","Iteration 1605: Training Loss = 60.72697830200195 Training Accuracy = 0.037732675671577454\n","Iteration 1606: Training Loss = 60.800899505615234 Training Accuracy = 0.03713679686188698\n","Iteration 1607: Training Loss = 60.872013092041016 Training Accuracy = 0.03819411247968674\n","Iteration 1608: Training Loss = 61.0392951965332 Training Accuracy = 0.03709349408745766\n","Iteration 1609: Training Loss = 61.09783172607422 Training Accuracy = 0.03865763172507286\n","Iteration 1610: Training Loss = 61.34262466430664 Training Accuracy = 0.0370323546230793\n","Iteration 1611: Training Loss = 61.23276138305664 Training Accuracy = 0.038892198354005814\n","Iteration 1612: Training Loss = 61.33951187133789 Training Accuracy = 0.03706960007548332\n","Iteration 1613: Training Loss = 61.07602310180664 Training Accuracy = 0.03873886168003082\n","Iteration 1614: Training Loss = 60.996131896972656 Training Accuracy = 0.037239305675029755\n","Iteration 1615: Training Loss = 60.81581497192383 Training Accuracy = 0.03830746188759804\n","Iteration 1616: Training Loss = 60.73244094848633 Training Accuracy = 0.03737140819430351\n","Iteration 1617: Training Loss = 60.66346740722656 Training Accuracy = 0.03792211040854454\n","Iteration 1618: Training Loss = 60.629295349121094 Training Accuracy = 0.03751881420612335\n","Iteration 1619: Training Loss = 60.61074447631836 Training Accuracy = 0.037748873233795166\n","Iteration 1620: Training Loss = 60.603126525878906 Training Accuracy = 0.037760715931653976\n","Iteration 1621: Training Loss = 60.60292053222656 Training Accuracy = 0.03775167837738991\n","Iteration 1622: Training Loss = 60.609676361083984 Training Accuracy = 0.038065966218709946\n","Iteration 1623: Training Loss = 60.628623962402344 Training Accuracy = 0.037795260548591614\n","Iteration 1624: Training Loss = 60.66072082519531 Training Accuracy = 0.03842051327228546\n","Iteration 1625: Training Loss = 60.73978805541992 Training Accuracy = 0.03774270787835121\n","Iteration 1626: Training Loss = 60.841148376464844 Training Accuracy = 0.03884570673108101\n","Iteration 1627: Training Loss = 61.12498474121094 Training Accuracy = 0.03749825805425644\n","Iteration 1628: Training Loss = 61.289512634277344 Training Accuracy = 0.03938354551792145\n","Iteration 1629: Training Loss = 61.88777542114258 Training Accuracy = 0.037150297313928604\n","Iteration 1630: Training Loss = 61.546932220458984 Training Accuracy = 0.03969411179423332\n","Iteration 1631: Training Loss = 61.64651107788086 Training Accuracy = 0.03736194595694542\n","Iteration 1632: Training Loss = 61.06312561035156 Training Accuracy = 0.03928809612989426\n","Iteration 1633: Training Loss = 60.811676025390625 Training Accuracy = 0.03781362622976303\n","Iteration 1634: Training Loss = 60.61527633666992 Training Accuracy = 0.0384056530892849\n","Iteration 1635: Training Loss = 60.55485916137695 Training Accuracy = 0.03791220113635063\n","Iteration 1636: Training Loss = 60.57599639892578 Training Accuracy = 0.03767954930663109\n","Iteration 1637: Training Loss = 60.65319061279297 Training Accuracy = 0.03813639283180237\n","Iteration 1638: Training Loss = 60.806884765625 Training Accuracy = 0.03726517781615257\n","Iteration 1639: Training Loss = 60.90485382080078 Training Accuracy = 0.038646258413791656\n","Iteration 1640: Training Loss = 61.12324142456055 Training Accuracy = 0.03720410168170929\n","Iteration 1641: Training Loss = 61.01973342895508 Training Accuracy = 0.039003849029541016\n","Iteration 1642: Training Loss = 61.06406021118164 Training Accuracy = 0.0374549962580204\n","Iteration 1643: Training Loss = 60.847862243652344 Training Accuracy = 0.03888903185725212\n","Iteration 1644: Training Loss = 60.751625061035156 Training Accuracy = 0.0377008281648159\n","Iteration 1645: Training Loss = 60.61808776855469 Training Accuracy = 0.03849942609667778\n","Iteration 1646: Training Loss = 60.55267333984375 Training Accuracy = 0.03779153898358345\n","Iteration 1647: Training Loss = 60.514034271240234 Training Accuracy = 0.0381353534758091\n","Iteration 1648: Training Loss = 60.49723434448242 Training Accuracy = 0.03798091039061546\n","Iteration 1649: Training Loss = 60.49330520629883 Training Accuracy = 0.037993792444467545\n","Iteration 1650: Training Loss = 60.50293731689453 Training Accuracy = 0.03830225393176079\n","Iteration 1651: Training Loss = 60.53175354003906 Training Accuracy = 0.038003869354724884\n","Iteration 1652: Training Loss = 60.5732421875 Training Accuracy = 0.03871738910675049\n","Iteration 1653: Training Loss = 60.66841506958008 Training Accuracy = 0.03795299679040909\n","Iteration 1654: Training Loss = 60.76680374145508 Training Accuracy = 0.03916575387120247\n","Iteration 1655: Training Loss = 61.044742584228516 Training Accuracy = 0.037749554961919785\n","Iteration 1656: Training Loss = 61.14612579345703 Training Accuracy = 0.03959530219435692\n","Iteration 1657: Training Loss = 61.59593200683594 Training Accuracy = 0.037545811384916306\n","Iteration 1658: Training Loss = 61.281558990478516 Training Accuracy = 0.03985559195280075\n","Iteration 1659: Training Loss = 61.315147399902344 Training Accuracy = 0.03776704892516136\n","Iteration 1660: Training Loss = 60.87094497680664 Training Accuracy = 0.0395091213285923\n","Iteration 1661: Training Loss = 60.68135070800781 Training Accuracy = 0.03812668099999428\n","Iteration 1662: Training Loss = 60.51313400268555 Training Accuracy = 0.03875622898340225\n","Iteration 1663: Training Loss = 60.44578170776367 Training Accuracy = 0.03820263594388962\n","Iteration 1664: Training Loss = 60.437835693359375 Training Accuracy = 0.0381452813744545\n","Iteration 1665: Training Loss = 60.47443389892578 Training Accuracy = 0.0384078249335289\n","Iteration 1666: Training Loss = 60.55608367919922 Training Accuracy = 0.0378531888127327\n","Iteration 1667: Training Loss = 60.64209747314453 Training Accuracy = 0.038907695561647415\n","Iteration 1668: Training Loss = 60.819007873535156 Training Accuracy = 0.03780323266983032\n","Iteration 1669: Training Loss = 60.85860824584961 Training Accuracy = 0.03939234837889671\n","Iteration 1670: Training Loss = 61.052757263183594 Training Accuracy = 0.037851955741643906\n","Iteration 1671: Training Loss = 60.909915924072266 Training Accuracy = 0.039557185024023056\n","Iteration 1672: Training Loss = 60.934810638427734 Training Accuracy = 0.037960756570100784\n","Iteration 1673: Training Loss = 60.72138977050781 Training Accuracy = 0.03934800252318382\n","Iteration 1674: Training Loss = 60.63689422607422 Training Accuracy = 0.038090817630290985\n","Iteration 1675: Training Loss = 60.51449966430664 Training Accuracy = 0.03894691914319992\n","Iteration 1676: Training Loss = 60.45574951171875 Training Accuracy = 0.03822563961148262\n","Iteration 1677: Training Loss = 60.40994644165039 Training Accuracy = 0.03868358954787254\n","Iteration 1678: Training Loss = 60.3863410949707 Training Accuracy = 0.038397450000047684\n","Iteration 1679: Training Loss = 60.373634338378906 Training Accuracy = 0.03861866518855095\n","Iteration 1680: Training Loss = 60.36750030517578 Training Accuracy = 0.038641709834337234\n","Iteration 1681: Training Loss = 60.36538314819336 Training Accuracy = 0.03864060342311859\n","Iteration 1682: Training Loss = 60.367027282714844 Training Accuracy = 0.038912203162908554\n","Iteration 1683: Training Loss = 60.37586212158203 Training Accuracy = 0.03871868550777435\n","Iteration 1684: Training Loss = 60.394962310791016 Training Accuracy = 0.03919751197099686\n","Iteration 1685: Training Loss = 60.444393157958984 Training Accuracy = 0.038706060498952866\n","Iteration 1686: Training Loss = 60.52208709716797 Training Accuracy = 0.039605166763067245\n","Iteration 1687: Training Loss = 60.73420333862305 Training Accuracy = 0.038513731211423874\n","Iteration 1688: Training Loss = 60.936279296875 Training Accuracy = 0.04017504304647446\n","Iteration 1689: Training Loss = 61.56477355957031 Training Accuracy = 0.03813139721751213\n","Iteration 1690: Training Loss = 61.42094039916992 Training Accuracy = 0.040684863924980164\n","Iteration 1691: Training Loss = 61.79557418823242 Training Accuracy = 0.03814462199807167\n","Iteration 1692: Training Loss = 61.02618408203125 Training Accuracy = 0.04047776386141777\n","Iteration 1693: Training Loss = 60.70417022705078 Training Accuracy = 0.03875691071152687\n","Iteration 1694: Training Loss = 60.41434860229492 Training Accuracy = 0.03949619457125664\n","Iteration 1695: Training Loss = 60.32404327392578 Training Accuracy = 0.03890280798077583\n","Iteration 1696: Training Loss = 60.34934997558594 Training Accuracy = 0.03859657794237137\n","Iteration 1697: Training Loss = 60.45085525512695 Training Accuracy = 0.039021555334329605\n","Iteration 1698: Training Loss = 60.64854431152344 Training Accuracy = 0.038062501698732376\n","Iteration 1699: Training Loss = 60.75104904174805 Training Accuracy = 0.03951607272028923\n","Iteration 1700: Training Loss = 60.9896125793457 Training Accuracy = 0.03795542195439339\n","Iteration 1701: Training Loss = 60.79743194580078 Training Accuracy = 0.03983188793063164\n","Iteration 1702: Training Loss = 60.74590301513672 Training Accuracy = 0.038335032761096954\n","Iteration 1703: Training Loss = 60.51606369018555 Training Accuracy = 0.03954944759607315\n","Iteration 1704: Training Loss = 60.395816802978516 Training Accuracy = 0.03864610940217972\n","Iteration 1705: Training Loss = 60.307823181152344 Training Accuracy = 0.039025019854307175\n","Iteration 1706: Training Loss = 60.27488327026367 Training Accuracy = 0.03872355446219444\n","Iteration 1707: Training Loss = 60.282188415527344 Training Accuracy = 0.038670558482408524\n","Iteration 1708: Training Loss = 60.31679153442383 Training Accuracy = 0.03905380517244339\n","Iteration 1709: Training Loss = 60.38252258300781 Training Accuracy = 0.0385482981801033\n","Iteration 1710: Training Loss = 60.4526481628418 Training Accuracy = 0.039546556770801544\n","Iteration 1711: Training Loss = 60.61045455932617 Training Accuracy = 0.03851468861103058\n","Iteration 1712: Training Loss = 60.66630935668945 Training Accuracy = 0.03997412696480751\n","Iteration 1713: Training Loss = 60.88742446899414 Training Accuracy = 0.038428232073783875\n","Iteration 1714: Training Loss = 60.79004669189453 Training Accuracy = 0.040205761790275574\n","Iteration 1715: Training Loss = 60.898109436035156 Training Accuracy = 0.038469940423965454\n","Iteration 1716: Training Loss = 60.67756652832031 Training Accuracy = 0.040087245404720306\n","Iteration 1717: Training Loss = 60.62632369995117 Training Accuracy = 0.03868597000837326\n","Iteration 1718: Training Loss = 60.451812744140625 Training Accuracy = 0.039809755980968475\n","Iteration 1719: Training Loss = 60.38015365600586 Training Accuracy = 0.038825444877147675\n","Iteration 1720: Training Loss = 60.303733825683594 Training Accuracy = 0.039566200226545334\n","Iteration 1721: Training Loss = 60.26557540893555 Training Accuracy = 0.038959331810474396\n","Iteration 1722: Training Loss = 60.233543395996094 Training Accuracy = 0.03939032554626465\n","Iteration 1723: Training Loss = 60.2159538269043 Training Accuracy = 0.03913622349500656\n","Iteration 1724: Training Loss = 60.2057991027832 Training Accuracy = 0.03934912756085396\n","Iteration 1725: Training Loss = 60.19978332519531 Training Accuracy = 0.03934068977832794\n","Iteration 1726: Training Loss = 60.19364929199219 Training Accuracy = 0.03947761654853821\n","Iteration 1727: Training Loss = 60.18748092651367 Training Accuracy = 0.03957376629114151\n","Iteration 1728: Training Loss = 60.182861328125 Training Accuracy = 0.039628464728593826\n","Iteration 1729: Training Loss = 60.18069076538086 Training Accuracy = 0.039791006594896317\n","Iteration 1730: Training Loss = 60.18040084838867 Training Accuracy = 0.03973456472158432\n","Iteration 1731: Training Loss = 60.182884216308594 Training Accuracy = 0.03999476879835129\n","Iteration 1732: Training Loss = 60.194740295410156 Training Accuracy = 0.039772532880306244\n","Iteration 1733: Training Loss = 60.225460052490234 Training Accuracy = 0.04029317572712898\n","Iteration 1734: Training Loss = 60.31775665283203 Training Accuracy = 0.039642028510570526\n","Iteration 1735: Training Loss = 60.478580474853516 Training Accuracy = 0.0408141165971756\n","Iteration 1736: Training Loss = 60.96065139770508 Training Accuracy = 0.039244215935468674\n","Iteration 1737: Training Loss = 61.27360916137695 Training Accuracy = 0.04156143590807915\n","Iteration 1738: Training Loss = 62.388553619384766 Training Accuracy = 0.0387495756149292\n","Iteration 1739: Training Loss = 61.3080940246582 Training Accuracy = 0.04188607633113861\n","Iteration 1740: Training Loss = 60.934085845947266 Training Accuracy = 0.039588093757629395\n","Iteration 1741: Training Loss = 60.372920989990234 Training Accuracy = 0.040856294333934784\n","Iteration 1742: Training Loss = 60.18339157104492 Training Accuracy = 0.03999496251344681\n","Iteration 1743: Training Loss = 60.17184829711914 Training Accuracy = 0.039611224085092545\n","Iteration 1744: Training Loss = 60.28273391723633 Training Accuracy = 0.039771512150764465\n","Iteration 1745: Training Loss = 60.52775955200195 Training Accuracy = 0.03880542144179344\n","Iteration 1746: Training Loss = 60.65769958496094 Training Accuracy = 0.04019980877637863\n","Iteration 1747: Training Loss = 60.972225189208984 Training Accuracy = 0.03846060857176781\n","Iteration 1748: Training Loss = 60.6938362121582 Training Accuracy = 0.04055272042751312\n","Iteration 1749: Training Loss = 60.56474685668945 Training Accuracy = 0.0389961302280426\n","Iteration 1750: Training Loss = 60.2955322265625 Training Accuracy = 0.04014123976230621\n","Iteration 1751: Training Loss = 60.170440673828125 Training Accuracy = 0.03949878737330437\n","Iteration 1752: Training Loss = 60.11185836791992 Training Accuracy = 0.03946704789996147\n","Iteration 1753: Training Loss = 60.13610076904297 Training Accuracy = 0.03951018303632736\n","Iteration 1754: Training Loss = 60.223148345947266 Training Accuracy = 0.03902038559317589\n","Iteration 1755: Training Loss = 60.316131591796875 Training Accuracy = 0.0398532934486866\n","Iteration 1756: Training Loss = 60.487579345703125 Training Accuracy = 0.03877967596054077\n","Iteration 1757: Training Loss = 60.50429153442383 Training Accuracy = 0.04038422554731369\n","Iteration 1758: Training Loss = 60.63456726074219 Training Accuracy = 0.0389600545167923\n","Iteration 1759: Training Loss = 60.48085403442383 Training Accuracy = 0.04051908850669861\n","Iteration 1760: Training Loss = 60.45417785644531 Training Accuracy = 0.03923584148287773\n","Iteration 1761: Training Loss = 60.28416061401367 Training Accuracy = 0.04037627577781677\n","Iteration 1762: Training Loss = 60.20575714111328 Training Accuracy = 0.039315879344940186\n","Iteration 1763: Training Loss = 60.127376556396484 Training Accuracy = 0.039978720247745514\n","Iteration 1764: Training Loss = 60.088356018066406 Training Accuracy = 0.03950909897685051\n","Iteration 1765: Training Loss = 60.06252670288086 Training Accuracy = 0.03975002095103264\n","Iteration 1766: Training Loss = 60.048095703125 Training Accuracy = 0.039715200662612915\n","Iteration 1767: Training Loss = 60.04225158691406 Training Accuracy = 0.0398942194879055\n","Iteration 1768: Training Loss = 60.0411262512207 Training Accuracy = 0.04000546410679817\n","Iteration 1769: Training Loss = 60.042964935302734 Training Accuracy = 0.03996577486395836\n","Iteration 1770: Training Loss = 60.051307678222656 Training Accuracy = 0.04033986106514931\n","Iteration 1771: Training Loss = 60.07378387451172 Training Accuracy = 0.03997621312737465\n","Iteration 1772: Training Loss = 60.113258361816406 Training Accuracy = 0.04063805192708969\n","Iteration 1773: Training Loss = 60.219154357910156 Training Accuracy = 0.03988822177052498\n","Iteration 1774: Training Loss = 60.350406646728516 Training Accuracy = 0.04109978675842285\n","Iteration 1775: Training Loss = 60.730682373046875 Training Accuracy = 0.03963448107242584\n","Iteration 1776: Training Loss = 60.88018798828125 Training Accuracy = 0.04173458740115166\n","Iteration 1777: Training Loss = 61.52479553222656 Training Accuracy = 0.03936687856912613\n","Iteration 1778: Training Loss = 60.95062255859375 Training Accuracy = 0.04196243733167648\n","Iteration 1779: Training Loss = 60.83032989501953 Training Accuracy = 0.03983696922659874\n","Iteration 1780: Training Loss = 60.32857131958008 Training Accuracy = 0.04128212109208107\n","Iteration 1781: Training Loss = 60.11658477783203 Training Accuracy = 0.04009689763188362\n","Iteration 1782: Training Loss = 60.01081085205078 Training Accuracy = 0.04033501446247101\n","Iteration 1783: Training Loss = 60.004756927490234 Training Accuracy = 0.040050022304058075\n","Iteration 1784: Training Loss = 60.06551742553711 Training Accuracy = 0.03966237232089043\n","Iteration 1785: Training Loss = 60.16770935058594 Training Accuracy = 0.0404115654528141\n","Iteration 1786: Training Loss = 60.368228912353516 Training Accuracy = 0.03935329616069794\n","Iteration 1787: Training Loss = 60.432857513427734 Training Accuracy = 0.04095620661973953\n","Iteration 1788: Training Loss = 60.63253402709961 Training Accuracy = 0.039418600499629974\n","Iteration 1789: Training Loss = 60.4401741027832 Training Accuracy = 0.04115801304578781\n","Iteration 1790: Training Loss = 60.40164566040039 Training Accuracy = 0.03971371054649353\n","Iteration 1791: Training Loss = 60.19380187988281 Training Accuracy = 0.0408453643321991\n","Iteration 1792: Training Loss = 60.08690643310547 Training Accuracy = 0.039874956011772156\n","Iteration 1793: Training Loss = 59.99675750732422 Training Accuracy = 0.04037202522158623\n","Iteration 1794: Training Loss = 59.95899200439453 Training Accuracy = 0.03997189551591873\n","Iteration 1795: Training Loss = 59.947959899902344 Training Accuracy = 0.04010021314024925\n","Iteration 1796: Training Loss = 59.95261001586914 Training Accuracy = 0.04024236649274826\n","Iteration 1797: Training Loss = 59.97190856933594 Training Accuracy = 0.040048256516456604\n","Iteration 1798: Training Loss = 60.00830078125 Training Accuracy = 0.040653910487890244\n","Iteration 1799: Training Loss = 60.08884048461914 Training Accuracy = 0.0400090366601944\n","Iteration 1800: Training Loss = 60.16419982910156 Training Accuracy = 0.041098449379205704\n","Iteration 1801: Training Loss = 60.35572052001953 Training Accuracy = 0.03987623378634453\n","Iteration 1802: Training Loss = 60.42041015625 Training Accuracy = 0.04146815463900566\n","Iteration 1803: Training Loss = 60.72343063354492 Training Accuracy = 0.03974570706486702\n","Iteration 1804: Training Loss = 60.58567428588867 Training Accuracy = 0.04166921600699425\n","Iteration 1805: Training Loss = 60.71580123901367 Training Accuracy = 0.039824724197387695\n","Iteration 1806: Training Loss = 60.3917350769043 Training Accuracy = 0.04156811162829399\n","Iteration 1807: Training Loss = 60.289974212646484 Training Accuracy = 0.040054697543382645\n","Iteration 1808: Training Loss = 60.08803939819336 Training Accuracy = 0.04111815616488457\n","Iteration 1809: Training Loss = 59.99603271484375 Training Accuracy = 0.040178678929805756\n","Iteration 1810: Training Loss = 59.924068450927734 Training Accuracy = 0.040652401745319366\n","Iteration 1811: Training Loss = 59.890926361083984 Training Accuracy = 0.040278803557157516\n","Iteration 1812: Training Loss = 59.880523681640625 Training Accuracy = 0.04037121683359146\n","Iteration 1813: Training Loss = 59.885223388671875 Training Accuracy = 0.04054802283644676\n","Iteration 1814: Training Loss = 59.901702880859375 Training Accuracy = 0.040348298847675323\n","Iteration 1815: Training Loss = 59.92845153808594 Training Accuracy = 0.04094249755144119\n","Iteration 1816: Training Loss = 59.98627853393555 Training Accuracy = 0.04038722440600395\n","Iteration 1817: Training Loss = 60.05342483520508 Training Accuracy = 0.04134275019168854\n","Iteration 1818: Training Loss = 60.22014236450195 Training Accuracy = 0.04028947651386261\n","Iteration 1819: Training Loss = 60.315284729003906 Training Accuracy = 0.04173450171947479\n","Iteration 1820: Training Loss = 60.637264251708984 Training Accuracy = 0.04005848243832588\n","Iteration 1821: Training Loss = 60.55717468261719 Training Accuracy = 0.04202604293823242\n","Iteration 1822: Training Loss = 60.77360916137695 Training Accuracy = 0.04002228006720543\n","Iteration 1823: Training Loss = 60.41265106201172 Training Accuracy = 0.041938796639442444\n","Iteration 1824: Training Loss = 60.31338882446289 Training Accuracy = 0.04030240327119827\n","Iteration 1825: Training Loss = 60.065574645996094 Training Accuracy = 0.04146050289273262\n","Iteration 1826: Training Loss = 59.953800201416016 Training Accuracy = 0.04046424478292465\n","Iteration 1827: Training Loss = 59.869834899902344 Training Accuracy = 0.040953487157821655\n","Iteration 1828: Training Loss = 59.83300018310547 Training Accuracy = 0.04053369536995888\n","Iteration 1829: Training Loss = 59.820220947265625 Training Accuracy = 0.04063905030488968\n","Iteration 1830: Training Loss = 59.8238525390625 Training Accuracy = 0.040761798620224\n","Iteration 1831: Training Loss = 59.84275817871094 Training Accuracy = 0.0405673049390316\n","Iteration 1832: Training Loss = 59.875526428222656 Training Accuracy = 0.041159652173519135\n","Iteration 1833: Training Loss = 59.94402313232422 Training Accuracy = 0.04055184870958328\n","Iteration 1834: Training Loss = 60.01392364501953 Training Accuracy = 0.041570790112018585\n","Iteration 1835: Training Loss = 60.187442779541016 Training Accuracy = 0.04046698659658432\n","Iteration 1836: Training Loss = 60.261474609375 Training Accuracy = 0.04196556285023689\n","Iteration 1837: Training Loss = 60.55369186401367 Training Accuracy = 0.04029124230146408\n","Iteration 1838: Training Loss = 60.45233917236328 Training Accuracy = 0.04220059514045715\n","Iteration 1839: Training Loss = 60.61747360229492 Training Accuracy = 0.040307845920324326\n","Iteration 1840: Training Loss = 60.30479049682617 Training Accuracy = 0.04208263009786606\n","Iteration 1841: Training Loss = 60.220455169677734 Training Accuracy = 0.04051481559872627\n","Iteration 1842: Training Loss = 60.000362396240234 Training Accuracy = 0.04166058823466301\n","Iteration 1843: Training Loss = 59.90286636352539 Training Accuracy = 0.04064207151532173\n","Iteration 1844: Training Loss = 59.82011795043945 Training Accuracy = 0.041202615946531296\n","Iteration 1845: Training Loss = 59.780025482177734 Training Accuracy = 0.04073037952184677\n","Iteration 1846: Training Loss = 59.760284423828125 Training Accuracy = 0.0409238301217556\n","Iteration 1847: Training Loss = 59.755767822265625 Training Accuracy = 0.04095595329999924\n","Iteration 1848: Training Loss = 59.76246643066406 Training Accuracy = 0.04085582494735718\n","Iteration 1849: Training Loss = 59.7788200378418 Training Accuracy = 0.04127863422036171\n","Iteration 1850: Training Loss = 59.8145637512207 Training Accuracy = 0.040879614651203156\n","Iteration 1851: Training Loss = 59.862979888916016 Training Accuracy = 0.041646238416433334\n","Iteration 1852: Training Loss = 59.97796630859375 Training Accuracy = 0.040800001472234726\n","Iteration 1853: Training Loss = 60.078636169433594 Training Accuracy = 0.04203450307250023\n","Iteration 1854: Training Loss = 60.36370849609375 Training Accuracy = 0.040580250322818756\n","Iteration 1855: Training Loss = 60.401458740234375 Training Accuracy = 0.04242023825645447\n","Iteration 1856: Training Loss = 60.75153350830078 Training Accuracy = 0.04042595624923706\n","Iteration 1857: Training Loss = 60.43077087402344 Training Accuracy = 0.042529910802841187\n","Iteration 1858: Training Loss = 60.41493225097656 Training Accuracy = 0.04063509777188301\n","Iteration 1859: Training Loss = 60.07244110107422 Training Accuracy = 0.04211602732539177\n","Iteration 1860: Training Loss = 59.92734146118164 Training Accuracy = 0.04085760936141014\n","Iteration 1861: Training Loss = 59.79025650024414 Training Accuracy = 0.04150448739528656\n","Iteration 1862: Training Loss = 59.728214263916016 Training Accuracy = 0.040898893028497696\n","Iteration 1863: Training Loss = 59.70140838623047 Training Accuracy = 0.04107338562607765\n","Iteration 1864: Training Loss = 59.70002365112305 Training Accuracy = 0.041077740490436554\n","Iteration 1865: Training Loss = 59.71867752075195 Training Accuracy = 0.04089685529470444\n","Iteration 1866: Training Loss = 59.75431823730469 Training Accuracy = 0.04144170880317688\n","Iteration 1867: Training Loss = 59.82590103149414 Training Accuracy = 0.04083641618490219\n","Iteration 1868: Training Loss = 59.895626068115234 Training Accuracy = 0.04186668619513512\n","Iteration 1869: Training Loss = 60.06121826171875 Training Accuracy = 0.040764711797237396\n","Iteration 1870: Training Loss = 60.11247253417969 Training Accuracy = 0.04223588481545448\n","Iteration 1871: Training Loss = 60.36123275756836 Training Accuracy = 0.04061906784772873\n","Iteration 1872: Training Loss = 60.258853912353516 Training Accuracy = 0.042443305253982544\n","Iteration 1873: Training Loss = 60.376319885253906 Training Accuracy = 0.040663011372089386\n","Iteration 1874: Training Loss = 60.1120491027832 Training Accuracy = 0.04231560230255127\n","Iteration 1875: Training Loss = 60.0386962890625 Training Accuracy = 0.040863923728466034\n","Iteration 1876: Training Loss = 59.85395431518555 Training Accuracy = 0.04193686321377754\n","Iteration 1877: Training Loss = 59.77371597290039 Training Accuracy = 0.04092848673462868\n","Iteration 1878: Training Loss = 59.69710922241211 Training Accuracy = 0.04152759537100792\n","Iteration 1879: Training Loss = 59.657066345214844 Training Accuracy = 0.04099821299314499\n","Iteration 1880: Training Loss = 59.63294982910156 Training Accuracy = 0.04129151627421379\n","Iteration 1881: Training Loss = 59.622230529785156 Training Accuracy = 0.04121777042746544\n","Iteration 1882: Training Loss = 59.61724090576172 Training Accuracy = 0.0413111187517643\n","Iteration 1883: Training Loss = 59.61552429199219 Training Accuracy = 0.041500043123960495\n","Iteration 1884: Training Loss = 59.618675231933594 Training Accuracy = 0.04141201078891754\n","Iteration 1885: Training Loss = 59.628746032714844 Training Accuracy = 0.04178055003285408\n","Iteration 1886: Training Loss = 59.65452575683594 Training Accuracy = 0.041411202400922775\n","Iteration 1887: Training Loss = 59.695350646972656 Training Accuracy = 0.042078934609889984\n","Iteration 1888: Training Loss = 59.79979705810547 Training Accuracy = 0.041300445795059204\n","Iteration 1889: Training Loss = 59.92413330078125 Training Accuracy = 0.04248941317200661\n","Iteration 1890: Training Loss = 60.276206970214844 Training Accuracy = 0.04103737324476242\n","Iteration 1891: Training Loss = 60.397037506103516 Training Accuracy = 0.04304628074169159\n","Iteration 1892: Training Loss = 60.959800720214844 Training Accuracy = 0.04080301895737648\n","Iteration 1893: Training Loss = 60.46207809448242 Training Accuracy = 0.04325085133314133\n","Iteration 1894: Training Loss = 60.381473541259766 Training Accuracy = 0.04115012660622597\n","Iteration 1895: Training Loss = 59.93343734741211 Training Accuracy = 0.042632460594177246\n","Iteration 1896: Training Loss = 59.74664306640625 Training Accuracy = 0.04141636937856674\n","Iteration 1897: Training Loss = 59.613948822021484 Training Accuracy = 0.041791368275880814\n","Iteration 1898: Training Loss = 59.5678825378418 Training Accuracy = 0.04130244627594948\n","Iteration 1899: Training Loss = 59.568687438964844 Training Accuracy = 0.0413082055747509\n","Iteration 1900: Training Loss = 59.598915100097656 Training Accuracy = 0.041521236300468445\n","Iteration 1901: Training Loss = 59.6678581237793 Training Accuracy = 0.041029930114746094\n","Iteration 1902: Training Loss = 59.74948501586914 Training Accuracy = 0.042048364877700806\n","Iteration 1903: Training Loss = 59.915409088134766 Training Accuracy = 0.040965668857097626\n","Iteration 1904: Training Loss = 59.95924758911133 Training Accuracy = 0.04246741160750389\n","Iteration 1905: Training Loss = 60.15066909790039 Training Accuracy = 0.04096124693751335\n","Iteration 1906: Training Loss = 60.020484924316406 Training Accuracy = 0.04258318617939949\n","Iteration 1907: Training Loss = 60.06147003173828 Training Accuracy = 0.04101932421326637\n","Iteration 1908: Training Loss = 59.86465835571289 Training Accuracy = 0.04239808768033981\n","Iteration 1909: Training Loss = 59.79254150390625 Training Accuracy = 0.041140858083963394\n","Iteration 1910: Training Loss = 59.666656494140625 Training Accuracy = 0.04202612489461899\n","Iteration 1911: Training Loss = 59.612342834472656 Training Accuracy = 0.04124912992119789\n","Iteration 1912: Training Loss = 59.561500549316406 Training Accuracy = 0.041835542768239975\n","Iteration 1913: Training Loss = 59.536712646484375 Training Accuracy = 0.04134400561451912\n","Iteration 1914: Training Loss = 59.51488494873047 Training Accuracy = 0.041804954409599304\n","Iteration 1915: Training Loss = 59.500274658203125 Training Accuracy = 0.041559118777513504\n","Iteration 1916: Training Loss = 59.491573333740234 Training Accuracy = 0.04179742932319641\n","Iteration 1917: Training Loss = 59.486995697021484 Training Accuracy = 0.04174194484949112\n","Iteration 1918: Training Loss = 59.483341217041016 Training Accuracy = 0.041916921734809875\n","Iteration 1919: Training Loss = 59.479671478271484 Training Accuracy = 0.041837096214294434\n","Iteration 1920: Training Loss = 59.47633743286133 Training Accuracy = 0.04210055246949196\n","Iteration 1921: Training Loss = 59.47739028930664 Training Accuracy = 0.041951850056648254\n","Iteration 1922: Training Loss = 59.486244201660156 Training Accuracy = 0.04233647882938385\n","Iteration 1923: Training Loss = 59.51509475708008 Training Accuracy = 0.04197191819548607\n","Iteration 1924: Training Loss = 59.5687141418457 Training Accuracy = 0.04270263761281967\n","Iteration 1925: Training Loss = 59.719749450683594 Training Accuracy = 0.04179953411221504\n","Iteration 1926: Training Loss = 59.9190788269043 Training Accuracy = 0.04321959987282753\n","Iteration 1927: Training Loss = 60.519012451171875 Training Accuracy = 0.04139081761240959\n","Iteration 1928: Training Loss = 60.60938262939453 Training Accuracy = 0.043840985745191574\n","Iteration 1929: Training Loss = 61.33512496948242 Training Accuracy = 0.04116877168416977\n","Iteration 1930: Training Loss = 60.32704544067383 Training Accuracy = 0.04384776949882507\n","Iteration 1931: Training Loss = 59.92377853393555 Training Accuracy = 0.04188813641667366\n","Iteration 1932: Training Loss = 59.5543098449707 Training Accuracy = 0.042729273438453674\n","Iteration 1933: Training Loss = 59.44591522216797 Training Accuracy = 0.04205973818898201\n","Iteration 1934: Training Loss = 59.469608306884766 Training Accuracy = 0.041637010872364044\n","Iteration 1935: Training Loss = 59.585723876953125 Training Accuracy = 0.041985075920820236\n","Iteration 1936: Training Loss = 59.8278694152832 Training Accuracy = 0.04101230949163437\n","Iteration 1937: Training Loss = 59.94485092163086 Training Accuracy = 0.04247812554240227\n","Iteration 1938: Training Loss = 60.24172592163086 Training Accuracy = 0.0407891571521759\n","Iteration 1939: Training Loss = 59.9713134765625 Training Accuracy = 0.04285712167620659\n","Iteration 1940: Training Loss = 59.8614501953125 Training Accuracy = 0.04129119962453842\n","Iteration 1941: Training Loss = 59.59714126586914 Training Accuracy = 0.04240739718079567\n","Iteration 1942: Training Loss = 59.471153259277344 Training Accuracy = 0.04166996106505394\n","Iteration 1943: Training Loss = 59.402767181396484 Training Accuracy = 0.04171320050954819\n","Iteration 1944: Training Loss = 59.40501403808594 Training Accuracy = 0.04166175425052643\n","Iteration 1945: Training Loss = 59.459999084472656 Training Accuracy = 0.04135186970233917\n","Iteration 1946: Training Loss = 59.535884857177734 Training Accuracy = 0.04201853647828102\n","Iteration 1947: Training Loss = 59.67814254760742 Training Accuracy = 0.041161712259054184\n","Iteration 1948: Training Loss = 59.738075256347656 Training Accuracy = 0.0425943024456501\n","Iteration 1949: Training Loss = 59.90078353881836 Training Accuracy = 0.04124927893280983\n","Iteration 1950: Training Loss = 59.80841827392578 Training Accuracy = 0.04282240569591522\n","Iteration 1951: Training Loss = 59.84709167480469 Training Accuracy = 0.04143580049276352\n","Iteration 1952: Training Loss = 59.6640739440918 Training Accuracy = 0.04274589568376541\n","Iteration 1953: Training Loss = 59.59547805786133 Training Accuracy = 0.041494473814964294\n","Iteration 1954: Training Loss = 59.48503494262695 Training Accuracy = 0.042381737381219864\n","Iteration 1955: Training Loss = 59.43347930908203 Training Accuracy = 0.0416400283575058\n","Iteration 1956: Training Loss = 59.388816833496094 Training Accuracy = 0.04210454970598221\n","Iteration 1957: Training Loss = 59.3642578125 Training Accuracy = 0.041826847940683365\n","Iteration 1958: Training Loss = 59.34736251831055 Training Accuracy = 0.042195387184619904\n","Iteration 1959: Training Loss = 59.338016510009766 Training Accuracy = 0.0420350544154644\n","Iteration 1960: Training Loss = 59.330440521240234 Training Accuracy = 0.0423109270632267\n","Iteration 1961: Training Loss = 59.324554443359375 Training Accuracy = 0.042292751371860504\n","Iteration 1962: Training Loss = 59.31978225708008 Training Accuracy = 0.0423998087644577\n","Iteration 1963: Training Loss = 59.314674377441406 Training Accuracy = 0.042439986020326614\n","Iteration 1964: Training Loss = 59.31026077270508 Training Accuracy = 0.04254366457462311\n","Iteration 1965: Training Loss = 59.3065185546875 Training Accuracy = 0.04257521405816078\n","Iteration 1966: Training Loss = 59.30215835571289 Training Accuracy = 0.0427299328148365\n","Iteration 1967: Training Loss = 59.29808044433594 Training Accuracy = 0.04277455434203148\n","Iteration 1968: Training Loss = 59.29483413696289 Training Accuracy = 0.042908161878585815\n","Iteration 1969: Training Loss = 59.292442321777344 Training Accuracy = 0.04292638227343559\n","Iteration 1970: Training Loss = 59.29290008544922 Training Accuracy = 0.043140560388565063\n","Iteration 1971: Training Loss = 59.302040100097656 Training Accuracy = 0.04293452203273773\n","Iteration 1972: Training Loss = 59.328338623046875 Training Accuracy = 0.04344232752919197\n","Iteration 1973: Training Loss = 59.411033630371094 Training Accuracy = 0.042840052396059036\n","Iteration 1974: Training Loss = 59.58063888549805 Training Accuracy = 0.04390682280063629\n","Iteration 1975: Training Loss = 60.106502532958984 Training Accuracy = 0.042398616671562195\n","Iteration 1976: Training Loss = 60.50767517089844 Training Accuracy = 0.04475044459104538\n","Iteration 1977: Training Loss = 61.849403381347656 Training Accuracy = 0.04175049066543579\n","Iteration 1978: Training Loss = 60.461631774902344 Training Accuracy = 0.045034077018499374\n","Iteration 1979: Training Loss = 59.9423713684082 Training Accuracy = 0.042752720415592194\n","Iteration 1980: Training Loss = 59.42973327636719 Training Accuracy = 0.04367147013545036\n","Iteration 1981: Training Loss = 59.28470993041992 Training Accuracy = 0.043014880269765854\n","Iteration 1982: Training Loss = 59.31119918823242 Training Accuracy = 0.04231371358036995\n","Iteration 1983: Training Loss = 59.4772834777832 Training Accuracy = 0.04263461008667946\n","Iteration 1984: Training Loss = 59.826934814453125 Training Accuracy = 0.04159281402826309\n","Iteration 1985: Training Loss = 59.92683029174805 Training Accuracy = 0.043145257979631424\n","Iteration 1986: Training Loss = 60.27362060546875 Training Accuracy = 0.04120524972677231\n","Iteration 1987: Training Loss = 59.85316848754883 Training Accuracy = 0.04350423067808151\n","Iteration 1988: Training Loss = 59.63539123535156 Training Accuracy = 0.041898999363183975\n","Iteration 1989: Training Loss = 59.360008239746094 Training Accuracy = 0.042784906923770905\n","Iteration 1990: Training Loss = 59.25074768066406 Training Accuracy = 0.04228883981704712\n","Iteration 1991: Training Loss = 59.239234924316406 Training Accuracy = 0.041937604546546936\n","Iteration 1992: Training Loss = 59.324684143066406 Training Accuracy = 0.042222894728183746\n","Iteration 1993: Training Loss = 59.482826232910156 Training Accuracy = 0.04147850722074509\n","Iteration 1994: Training Loss = 59.562095642089844 Training Accuracy = 0.04265984147787094\n","Iteration 1995: Training Loss = 59.743499755859375 Training Accuracy = 0.04136534780263901\n","Iteration 1996: Training Loss = 59.64214324951172 Training Accuracy = 0.04314323887228966\n","Iteration 1997: Training Loss = 59.637123107910156 Training Accuracy = 0.041781291365623474\n","Iteration 1998: Training Loss = 59.4476318359375 Training Accuracy = 0.04293445870280266\n","Iteration 1999: Training Loss = 59.352149963378906 Training Accuracy = 0.04208231344819069\n","Iteration 2000: Training Loss = 59.252525329589844 Training Accuracy = 0.04268331080675125\n","Iteration 2001: Training Loss = 59.207279205322266 Training Accuracy = 0.04203850030899048\n","Iteration 2002: Training Loss = 59.1806755065918 Training Accuracy = 0.042344048619270325\n","Iteration 2003: Training Loss = 59.176429748535156 Training Accuracy = 0.04237697646021843\n","Iteration 2004: Training Loss = 59.18759536743164 Training Accuracy = 0.042171407490968704\n","Iteration 2005: Training Loss = 59.21075439453125 Training Accuracy = 0.04277889057993889\n","Iteration 2006: Training Loss = 59.26318359375 Training Accuracy = 0.04235444217920303\n","Iteration 2007: Training Loss = 59.3195915222168 Training Accuracy = 0.04318063333630562\n","Iteration 2008: Training Loss = 59.44963455200195 Training Accuracy = 0.04225941747426987\n","Iteration 2009: Training Loss = 59.52804946899414 Training Accuracy = 0.04364712908864021\n","Iteration 2010: Training Loss = 59.781898498535156 Training Accuracy = 0.04211530461907387\n","Iteration 2011: Training Loss = 59.756465911865234 Training Accuracy = 0.043891943991184235\n","Iteration 2012: Training Loss = 59.980491638183594 Training Accuracy = 0.04208195209503174\n","Iteration 2013: Training Loss = 59.704933166503906 Training Accuracy = 0.04387582838535309\n","Iteration 2014: Training Loss = 59.667457580566406 Training Accuracy = 0.042281631380319595\n","Iteration 2015: Training Loss = 59.41926193237305 Training Accuracy = 0.043606653809547424\n","Iteration 2016: Training Loss = 59.316959381103516 Training Accuracy = 0.042395323514938354\n","Iteration 2017: Training Loss = 59.21139144897461 Training Accuracy = 0.04314013570547104\n","Iteration 2018: Training Loss = 59.16106414794922 Training Accuracy = 0.04253173992037773\n","Iteration 2019: Training Loss = 59.12627029418945 Training Accuracy = 0.042834099382162094\n","Iteration 2020: Training Loss = 59.1082763671875 Training Accuracy = 0.0426337793469429\n","Iteration 2021: Training Loss = 59.099666595458984 Training Accuracy = 0.04280916228890419\n","Iteration 2022: Training Loss = 59.09573745727539 Training Accuracy = 0.042885225266218185\n","Iteration 2023: Training Loss = 59.09651565551758 Training Accuracy = 0.042852699756622314\n","Iteration 2024: Training Loss = 59.101844787597656 Training Accuracy = 0.04323284327983856\n","Iteration 2025: Training Loss = 59.116493225097656 Training Accuracy = 0.04295125603675842\n","Iteration 2026: Training Loss = 59.142852783203125 Training Accuracy = 0.04348854348063469\n","Iteration 2027: Training Loss = 59.20815658569336 Training Accuracy = 0.04292268306016922\n","Iteration 2028: Training Loss = 59.29348373413086 Training Accuracy = 0.043846748769283295\n","Iteration 2029: Training Loss = 59.52149963378906 Training Accuracy = 0.042685460299253464\n","Iteration 2030: Training Loss = 59.674720764160156 Training Accuracy = 0.044348765164613724\n","Iteration 2031: Training Loss = 60.184024810791016 Training Accuracy = 0.04239783063530922\n","Iteration 2032: Training Loss = 59.9648551940918 Training Accuracy = 0.044726934283971786\n","Iteration 2033: Training Loss = 60.13923645019531 Training Accuracy = 0.04254487529397011\n","Iteration 2034: Training Loss = 59.60271072387695 Training Accuracy = 0.044443219900131226\n","Iteration 2035: Training Loss = 59.39003372192383 Training Accuracy = 0.0428992360830307\n","Iteration 2036: Training Loss = 59.16661071777344 Training Accuracy = 0.043646663427352905\n","Iteration 2037: Training Loss = 59.074058532714844 Training Accuracy = 0.04287691414356232\n","Iteration 2038: Training Loss = 59.04182815551758 Training Accuracy = 0.042974382638931274\n","Iteration 2039: Training Loss = 59.050941467285156 Training Accuracy = 0.04297501966357231\n","Iteration 2040: Training Loss = 59.09178161621094 Training Accuracy = 0.04269948974251747\n","Iteration 2041: Training Loss = 59.1551399230957 Training Accuracy = 0.04338939115405083\n","Iteration 2042: Training Loss = 59.285301208496094 Training Accuracy = 0.04259258136153221\n","Iteration 2043: Training Loss = 59.37080383300781 Training Accuracy = 0.04395440220832825\n","Iteration 2044: Training Loss = 59.59002685546875 Training Accuracy = 0.04254613071680069\n","Iteration 2045: Training Loss = 59.53008270263672 Training Accuracy = 0.04424494132399559\n","Iteration 2046: Training Loss = 59.64798355102539 Training Accuracy = 0.04257991164922714\n","Iteration 2047: Training Loss = 59.43385696411133 Training Accuracy = 0.04408866912126541\n","Iteration 2048: Training Loss = 59.37980270385742 Training Accuracy = 0.042694877833127975\n","Iteration 2049: Training Loss = 59.21266555786133 Training Accuracy = 0.043742794543504715\n","Iteration 2050: Training Loss = 59.13985061645508 Training Accuracy = 0.04281747341156006\n","Iteration 2051: Training Loss = 59.068233489990234 Training Accuracy = 0.04348397254943848\n","Iteration 2052: Training Loss = 59.03694152832031 Training Accuracy = 0.04294411092996597\n","Iteration 2053: Training Loss = 59.01184844970703 Training Accuracy = 0.04343545809388161\n","Iteration 2054: Training Loss = 58.99626541137695 Training Accuracy = 0.04310578107833862\n","Iteration 2055: Training Loss = 58.98294448852539 Training Accuracy = 0.043445706367492676\n","Iteration 2056: Training Loss = 58.9747428894043 Training Accuracy = 0.043296873569488525\n","Iteration 2057: Training Loss = 58.9700927734375 Training Accuracy = 0.04352487251162529\n","Iteration 2058: Training Loss = 58.967926025390625 Training Accuracy = 0.043459203094244\n","Iteration 2059: Training Loss = 58.966148376464844 Training Accuracy = 0.04369519650936127\n","Iteration 2060: Training Loss = 58.966461181640625 Training Accuracy = 0.0435747466981411\n","Iteration 2061: Training Loss = 58.971038818359375 Training Accuracy = 0.04393375664949417\n","Iteration 2062: Training Loss = 58.99112319946289 Training Accuracy = 0.04361113905906677\n","Iteration 2063: Training Loss = 59.03117752075195 Training Accuracy = 0.04425367712974548\n","Iteration 2064: Training Loss = 59.141807556152344 Training Accuracy = 0.04349676892161369\n","Iteration 2065: Training Loss = 59.29189682006836 Training Accuracy = 0.04473809525370598\n","Iteration 2066: Training Loss = 59.72770690917969 Training Accuracy = 0.04315510019659996\n","Iteration 2067: Training Loss = 59.89079666137695 Training Accuracy = 0.04532733932137489\n","Iteration 2068: Training Loss = 60.61565399169922 Training Accuracy = 0.04286415874958038\n","Iteration 2069: Training Loss = 59.89512634277344 Training Accuracy = 0.04548152536153793\n","Iteration 2070: Training Loss = 59.68770980834961 Training Accuracy = 0.0433652438223362\n","Iteration 2071: Training Loss = 59.20503234863281 Training Accuracy = 0.04464634507894516\n","Iteration 2072: Training Loss = 59.01290512084961 Training Accuracy = 0.043573491275310516\n","Iteration 2073: Training Loss = 58.930877685546875 Training Accuracy = 0.043675489723682404\n","Iteration 2074: Training Loss = 58.933475494384766 Training Accuracy = 0.04341183975338936\n","Iteration 2075: Training Loss = 58.99139404296875 Training Accuracy = 0.043125469237565994\n","Iteration 2076: Training Loss = 59.074363708496094 Training Accuracy = 0.04371926188468933\n","Iteration 2077: Training Loss = 59.253360748291016 Training Accuracy = 0.04278135672211647\n","Iteration 2078: Training Loss = 59.348628997802734 Training Accuracy = 0.044356461614370346\n","Iteration 2079: Training Loss = 59.59618377685547 Training Accuracy = 0.042752593755722046\n","Iteration 2080: Training Loss = 59.41311264038086 Training Accuracy = 0.044621068984270096\n","Iteration 2081: Training Loss = 59.40149688720703 Training Accuracy = 0.043062031269073486\n","Iteration 2082: Training Loss = 59.164974212646484 Training Accuracy = 0.04424619302153587\n","Iteration 2083: Training Loss = 59.058990478515625 Training Accuracy = 0.043209437280893326\n","Iteration 2084: Training Loss = 58.947975158691406 Training Accuracy = 0.04375767335295677\n","Iteration 2085: Training Loss = 58.89070510864258 Training Accuracy = 0.04325384646654129\n","Iteration 2086: Training Loss = 58.86618423461914 Training Accuracy = 0.04350070282816887\n","Iteration 2087: Training Loss = 58.86652374267578 Training Accuracy = 0.04356107488274574\n","Iteration 2088: Training Loss = 58.8795166015625 Training Accuracy = 0.04348805174231529\n","Iteration 2089: Training Loss = 58.90040969848633 Training Accuracy = 0.04389511048793793\n","Iteration 2090: Training Loss = 58.94324493408203 Training Accuracy = 0.04352030158042908\n","Iteration 2091: Training Loss = 58.998695373535156 Training Accuracy = 0.04430660977959633\n","Iteration 2092: Training Loss = 59.134307861328125 Training Accuracy = 0.043340180069208145\n","Iteration 2093: Training Loss = 59.22216796875 Training Accuracy = 0.04473885893821716\n","Iteration 2094: Training Loss = 59.4838981628418 Training Accuracy = 0.04316098988056183\n","Iteration 2095: Training Loss = 59.437042236328125 Training Accuracy = 0.044959332793951035\n","Iteration 2096: Training Loss = 59.64612579345703 Training Accuracy = 0.04316547513008118\n","Iteration 2097: Training Loss = 59.39310073852539 Training Accuracy = 0.04494810849428177\n","Iteration 2098: Training Loss = 59.364871978759766 Training Accuracy = 0.04336175695061684\n","Iteration 2099: Training Loss = 59.12270736694336 Training Accuracy = 0.04464075341820717\n","Iteration 2100: Training Loss = 59.03105545043945 Training Accuracy = 0.04347893223166466\n","Iteration 2101: Training Loss = 58.927459716796875 Training Accuracy = 0.044259268790483475\n","Iteration 2102: Training Loss = 58.88420486450195 Training Accuracy = 0.04348928481340408\n","Iteration 2103: Training Loss = 58.843238830566406 Training Accuracy = 0.044001318514347076\n","Iteration 2104: Training Loss = 58.817771911621094 Training Accuracy = 0.043613605201244354\n","Iteration 2105: Training Loss = 58.800296783447266 Training Accuracy = 0.043937649577856064\n","Iteration 2106: Training Loss = 58.7927360534668 Training Accuracy = 0.043871067464351654\n","Iteration 2107: Training Loss = 58.789772033691406 Training Accuracy = 0.04400988668203354\n","Iteration 2108: Training Loss = 58.786380767822266 Training Accuracy = 0.04411785677075386\n","Iteration 2109: Training Loss = 58.780208587646484 Training Accuracy = 0.04416962340474129\n","Iteration 2110: Training Loss = 58.772884368896484 Training Accuracy = 0.04424547031521797\n","Iteration 2111: Training Loss = 58.76736068725586 Training Accuracy = 0.044264454394578934\n","Iteration 2112: Training Loss = 58.76485061645508 Training Accuracy = 0.04439685493707657\n","Iteration 2113: Training Loss = 58.764644622802734 Training Accuracy = 0.0443701334297657\n","Iteration 2114: Training Loss = 58.766048431396484 Training Accuracy = 0.044589392840862274\n","Iteration 2115: Training Loss = 58.77259063720703 Training Accuracy = 0.044486477971076965\n","Iteration 2116: Training Loss = 58.790950775146484 Training Accuracy = 0.044914647936820984\n","Iteration 2117: Training Loss = 58.84880828857422 Training Accuracy = 0.04445091262459755\n","Iteration 2118: Training Loss = 58.963829040527344 Training Accuracy = 0.045404572039842606\n","Iteration 2119: Training Loss = 59.29582214355469 Training Accuracy = 0.04412480816245079\n","Iteration 2120: Training Loss = 59.603981018066406 Training Accuracy = 0.04606841132044792\n","Iteration 2121: Training Loss = 60.58357620239258 Training Accuracy = 0.04356794059276581\n","Iteration 2122: Training Loss = 59.965965270996094 Training Accuracy = 0.046470917761325836\n","Iteration 2123: Training Loss = 59.98980712890625 Training Accuracy = 0.04395752400159836\n","Iteration 2124: Training Loss = 59.23582458496094 Training Accuracy = 0.04576149955391884\n","Iteration 2125: Training Loss = 58.9300537109375 Training Accuracy = 0.044399701058864594\n","Iteration 2126: Training Loss = 58.76775360107422 Training Accuracy = 0.044570598751306534\n","Iteration 2127: Training Loss = 58.74978256225586 Training Accuracy = 0.04411126673221588\n","Iteration 2128: Training Loss = 58.808067321777344 Training Accuracy = 0.043887052685022354\n","Iteration 2129: Training Loss = 58.89161682128906 Training Accuracy = 0.04425784572958946\n","Iteration 2130: Training Loss = 59.07291793823242 Training Accuracy = 0.04335508123040199\n","Iteration 2131: Training Loss = 59.190330505371094 Training Accuracy = 0.04492882639169693\n","Iteration 2132: Training Loss = 59.482421875 Training Accuracy = 0.043215326964855194\n","Iteration 2133: Training Loss = 59.24966812133789 Training Accuracy = 0.0452246367931366\n","Iteration 2134: Training Loss = 59.19278335571289 Training Accuracy = 0.04368533194065094\n","Iteration 2135: Training Loss = 58.9404411315918 Training Accuracy = 0.04476173594594002\n","Iteration 2136: Training Loss = 58.823829650878906 Training Accuracy = 0.04383080452680588\n","Iteration 2137: Training Loss = 58.725677490234375 Training Accuracy = 0.044163286685943604\n","Iteration 2138: Training Loss = 58.67871856689453 Training Accuracy = 0.04382229968905449\n","Iteration 2139: Training Loss = 58.67851638793945 Training Accuracy = 0.04383037984371185\n","Iteration 2140: Training Loss = 58.711669921875 Training Accuracy = 0.044212501496076584\n","Iteration 2141: Training Loss = 58.77262878417969 Training Accuracy = 0.04377731680870056\n","Iteration 2142: Training Loss = 58.839447021484375 Training Accuracy = 0.04465427249670029\n","Iteration 2143: Training Loss = 58.98281478881836 Training Accuracy = 0.04380091279745102\n","Iteration 2144: Training Loss = 59.047672271728516 Training Accuracy = 0.04513854160904884\n","Iteration 2145: Training Loss = 59.27196502685547 Training Accuracy = 0.04360223188996315\n","Iteration 2146: Training Loss = 59.17113494873047 Training Accuracy = 0.04540408030152321\n","Iteration 2147: Training Loss = 59.25503921508789 Training Accuracy = 0.0436684750020504\n","Iteration 2148: Training Loss = 59.034725189208984 Training Accuracy = 0.0451226606965065\n","Iteration 2149: Training Loss = 58.98063278198242 Training Accuracy = 0.04383382201194763\n","Iteration 2150: Training Loss = 58.83156967163086 Training Accuracy = 0.04478994384407997\n","Iteration 2151: Training Loss = 58.764793395996094 Training Accuracy = 0.0439143069088459\n","Iteration 2152: Training Loss = 58.69748306274414 Training Accuracy = 0.04462529718875885\n","Iteration 2153: Training Loss = 58.67116928100586 Training Accuracy = 0.04403126984834671\n","Iteration 2154: Training Loss = 58.65096664428711 Training Accuracy = 0.04458467289805412\n","Iteration 2155: Training Loss = 58.63955307006836 Training Accuracy = 0.044211648404598236\n","Iteration 2156: Training Loss = 58.624080657958984 Training Accuracy = 0.044623639434576035\n","Iteration 2157: Training Loss = 58.61753463745117 Training Accuracy = 0.04432627558708191\n","Iteration 2158: Training Loss = 58.619075775146484 Training Accuracy = 0.04479615390300751\n","Iteration 2159: Training Loss = 58.633575439453125 Training Accuracy = 0.044461991637945175\n","Iteration 2160: Training Loss = 58.653141021728516 Training Accuracy = 0.044993918389081955\n","Iteration 2161: Training Loss = 58.70336151123047 Training Accuracy = 0.0445077158510685\n","Iteration 2162: Training Loss = 58.7672004699707 Training Accuracy = 0.04537489265203476\n","Iteration 2163: Training Loss = 58.95391082763672 Training Accuracy = 0.044306546449661255\n","Iteration 2164: Training Loss = 59.0953254699707 Training Accuracy = 0.045901551842689514\n","Iteration 2165: Training Loss = 59.545162200927734 Training Accuracy = 0.04404517263174057\n","Iteration 2166: Training Loss = 59.41778564453125 Training Accuracy = 0.04621913284063339\n","Iteration 2167: Training Loss = 59.688819885253906 Training Accuracy = 0.04407886788249016\n","Iteration 2168: Training Loss = 59.20050048828125 Training Accuracy = 0.04604610800743103\n","Iteration 2169: Training Loss = 59.03621292114258 Training Accuracy = 0.04436685889959335\n","Iteration 2170: Training Loss = 58.755271911621094 Training Accuracy = 0.04536256194114685\n","Iteration 2171: Training Loss = 58.639198303222656 Training Accuracy = 0.04441600665450096\n","Iteration 2172: Training Loss = 58.57406997680664 Training Accuracy = 0.04478469491004944\n","Iteration 2173: Training Loss = 58.55154037475586 Training Accuracy = 0.044400572776794434\n","Iteration 2174: Training Loss = 58.54448699951172 Training Accuracy = 0.044470302760601044\n","Iteration 2175: Training Loss = 58.55268859863281 Training Accuracy = 0.04466873034834862\n","Iteration 2176: Training Loss = 58.59056091308594 Training Accuracy = 0.04433354735374451\n","Iteration 2177: Training Loss = 58.651466369628906 Training Accuracy = 0.04516694322228432\n","Iteration 2178: Training Loss = 58.77385330200195 Training Accuracy = 0.04428894445300102\n","Iteration 2179: Training Loss = 58.844051361083984 Training Accuracy = 0.04561413824558258\n","Iteration 2180: Training Loss = 59.051292419433594 Training Accuracy = 0.04423758387565613\n","Iteration 2181: Training Loss = 59.031517028808594 Training Accuracy = 0.045834142714738846\n","Iteration 2182: Training Loss = 59.22904586791992 Training Accuracy = 0.044192686676979065\n","Iteration 2183: Training Loss = 59.03966522216797 Training Accuracy = 0.045869726687669754\n","Iteration 2184: Training Loss = 59.047569274902344 Training Accuracy = 0.04427429661154747\n","Iteration 2185: Training Loss = 58.82601547241211 Training Accuracy = 0.045632652938365936\n","Iteration 2186: Training Loss = 58.759525299072266 Training Accuracy = 0.0444253608584404\n","Iteration 2187: Training Loss = 58.640655517578125 Training Accuracy = 0.045353442430496216\n","Iteration 2188: Training Loss = 58.58815383911133 Training Accuracy = 0.04446430876851082\n","Iteration 2189: Training Loss = 58.5311164855957 Training Accuracy = 0.04509636387228966\n","Iteration 2190: Training Loss = 58.502376556396484 Training Accuracy = 0.04458076134324074\n","Iteration 2191: Training Loss = 58.484676361083984 Training Accuracy = 0.04495752602815628\n","Iteration 2192: Training Loss = 58.477256774902344 Training Accuracy = 0.04475954547524452\n","Iteration 2193: Training Loss = 58.469932556152344 Training Accuracy = 0.04503420367836952\n","Iteration 2194: Training Loss = 58.46336364746094 Training Accuracy = 0.04492669925093651\n","Iteration 2195: Training Loss = 58.45801544189453 Training Accuracy = 0.04523420333862305\n","Iteration 2196: Training Loss = 58.45906066894531 Training Accuracy = 0.04503018781542778\n","Iteration 2197: Training Loss = 58.46714401245117 Training Accuracy = 0.04547455534338951\n","Iteration 2198: Training Loss = 58.490535736083984 Training Accuracy = 0.04506910964846611\n","Iteration 2199: Training Loss = 58.52629089355469 Training Accuracy = 0.04575765132904053\n","Iteration 2200: Training Loss = 58.62545394897461 Training Accuracy = 0.045007143169641495\n","Iteration 2201: Training Loss = 58.750640869140625 Training Accuracy = 0.046215541660785675\n","Iteration 2202: Training Loss = 59.119537353515625 Training Accuracy = 0.04477604106068611\n","Iteration 2203: Training Loss = 59.27652359008789 Training Accuracy = 0.04680125415325165\n","Iteration 2204: Training Loss = 59.92361068725586 Training Accuracy = 0.04456092789769173\n","Iteration 2205: Training Loss = 59.35380935668945 Training Accuracy = 0.047048747539520264\n","Iteration 2206: Training Loss = 59.237037658691406 Training Accuracy = 0.044887881726026535\n","Iteration 2207: Training Loss = 58.74826431274414 Training Accuracy = 0.04633142054080963\n","Iteration 2208: Training Loss = 58.54949951171875 Training Accuracy = 0.04510601609945297\n","Iteration 2209: Training Loss = 58.431732177734375 Training Accuracy = 0.04537159949541092\n","Iteration 2210: Training Loss = 58.39979934692383 Training Accuracy = 0.04495348781347275\n","Iteration 2211: Training Loss = 58.41912078857422 Training Accuracy = 0.0448099710047245\n","Iteration 2212: Training Loss = 58.475399017333984 Training Accuracy = 0.04521651938557625\n","Iteration 2213: Training Loss = 58.59565734863281 Training Accuracy = 0.044554825872182846\n","Iteration 2214: Training Loss = 58.704612731933594 Training Accuracy = 0.04584783315658569\n","Iteration 2215: Training Loss = 58.94313049316406 Training Accuracy = 0.04448235407471657\n","Iteration 2216: Training Loss = 58.90363693237305 Training Accuracy = 0.04627009108662605\n","Iteration 2217: Training Loss = 59.03522491455078 Training Accuracy = 0.044631801545619965\n","Iteration 2218: Training Loss = 58.79855728149414 Training Accuracy = 0.04612693563103676\n","Iteration 2219: Training Loss = 58.72837448120117 Training Accuracy = 0.04475967213511467\n","Iteration 2220: Training Loss = 58.55065155029297 Training Accuracy = 0.04570746049284935\n","Iteration 2221: Training Loss = 58.46515655517578 Training Accuracy = 0.04477597773075104\n","Iteration 2222: Training Loss = 58.396732330322266 Training Accuracy = 0.04535599425435066\n","Iteration 2223: Training Loss = 58.36618423461914 Training Accuracy = 0.04496945068240166\n","Iteration 2224: Training Loss = 58.3456916809082 Training Accuracy = 0.045250892639160156\n","Iteration 2225: Training Loss = 58.33266830444336 Training Accuracy = 0.04514317587018013\n","Iteration 2226: Training Loss = 58.32543182373047 Training Accuracy = 0.04531390219926834\n","Iteration 2227: Training Loss = 58.322025299072266 Training Accuracy = 0.04546930268406868\n","Iteration 2228: Training Loss = 58.327205657958984 Training Accuracy = 0.045332398265600204\n","Iteration 2229: Training Loss = 58.34199142456055 Training Accuracy = 0.04578639566898346\n","Iteration 2230: Training Loss = 58.37393569946289 Training Accuracy = 0.04530087113380432\n","Iteration 2231: Training Loss = 58.41731643676758 Training Accuracy = 0.04604778811335564\n","Iteration 2232: Training Loss = 58.53654098510742 Training Accuracy = 0.04520956799387932\n","Iteration 2233: Training Loss = 58.669586181640625 Training Accuracy = 0.04647812619805336\n","Iteration 2234: Training Loss = 59.059444427490234 Training Accuracy = 0.04503382369875908\n","Iteration 2235: Training Loss = 59.13982391357422 Training Accuracy = 0.047115668654441833\n","Iteration 2236: Training Loss = 59.654293060302734 Training Accuracy = 0.0449051670730114\n","Iteration 2237: Training Loss = 59.10527420043945 Training Accuracy = 0.04725729301571846\n","Iteration 2238: Training Loss = 58.96061325073242 Training Accuracy = 0.04528994485735893\n","Iteration 2239: Training Loss = 58.55435562133789 Training Accuracy = 0.04652604088187218\n","Iteration 2240: Training Loss = 58.38439178466797 Training Accuracy = 0.04542185366153717\n","Iteration 2241: Training Loss = 58.28586196899414 Training Accuracy = 0.045715007930994034\n","Iteration 2242: Training Loss = 58.25938034057617 Training Accuracy = 0.045340050011873245\n","Iteration 2243: Training Loss = 58.27680969238281 Training Accuracy = 0.0452115423977375\n","Iteration 2244: Training Loss = 58.3265380859375 Training Accuracy = 0.04565408080816269\n","Iteration 2245: Training Loss = 58.43203353881836 Training Accuracy = 0.045020364224910736\n","Iteration 2246: Training Loss = 58.53011703491211 Training Accuracy = 0.04624170809984207\n","Iteration 2247: Training Loss = 58.74733352661133 Training Accuracy = 0.04495731368660927\n","Iteration 2248: Training Loss = 58.73573303222656 Training Accuracy = 0.046632569283246994\n","Iteration 2249: Training Loss = 58.897891998291016 Training Accuracy = 0.04503071680665016\n","Iteration 2250: Training Loss = 58.688194274902344 Training Accuracy = 0.046578168869018555\n","Iteration 2251: Training Loss = 58.653411865234375 Training Accuracy = 0.04512521252036095\n","Iteration 2252: Training Loss = 58.463584899902344 Training Accuracy = 0.046220749616622925\n","Iteration 2253: Training Loss = 58.37932586669922 Training Accuracy = 0.04521475359797478\n","Iteration 2254: Training Loss = 58.291446685791016 Training Accuracy = 0.045901551842689514\n","Iteration 2255: Training Loss = 58.25276184082031 Training Accuracy = 0.045348066836595535\n","Iteration 2256: Training Loss = 58.223697662353516 Training Accuracy = 0.04580814018845558\n","Iteration 2257: Training Loss = 58.20660400390625 Training Accuracy = 0.04550899192690849\n","Iteration 2258: Training Loss = 58.19107437133789 Training Accuracy = 0.04583437368273735\n","Iteration 2259: Training Loss = 58.18035888671875 Training Accuracy = 0.045742496848106384\n","Iteration 2260: Training Loss = 58.174293518066406 Training Accuracy = 0.04587629809975624\n","Iteration 2261: Training Loss = 58.17105484008789 Training Accuracy = 0.04596169292926788\n","Iteration 2262: Training Loss = 58.16789245605469 Training Accuracy = 0.04595467820763588\n","Iteration 2263: Training Loss = 58.163448333740234 Training Accuracy = 0.04610799252986908\n","Iteration 2264: Training Loss = 58.1585807800293 Training Accuracy = 0.046090368181467056\n","Iteration 2265: Training Loss = 58.15589141845703 Training Accuracy = 0.04631764441728592\n","Iteration 2266: Training Loss = 58.159202575683594 Training Accuracy = 0.04620981961488724\n","Iteration 2267: Training Loss = 58.17184066772461 Training Accuracy = 0.04660814255475998\n","Iteration 2268: Training Loss = 58.20949935913086 Training Accuracy = 0.04626435041427612\n","Iteration 2269: Training Loss = 58.28306579589844 Training Accuracy = 0.047025468200445175\n","Iteration 2270: Training Loss = 58.49783706665039 Training Accuracy = 0.04607735946774483\n","Iteration 2271: Training Loss = 58.77318572998047 Training Accuracy = 0.04764829948544502\n","Iteration 2272: Training Loss = 59.610107421875 Training Accuracy = 0.0455668568611145\n","Iteration 2273: Training Loss = 59.40681076049805 Training Accuracy = 0.04832032322883606\n","Iteration 2274: Training Loss = 59.84117126464844 Training Accuracy = 0.04559466242790222\n","Iteration 2275: Training Loss = 58.835201263427734 Training Accuracy = 0.047942645847797394\n","Iteration 2276: Training Loss = 58.42425537109375 Training Accuracy = 0.04627999663352966\n","Iteration 2277: Training Loss = 58.16618728637695 Training Accuracy = 0.0465613529086113\n","Iteration 2278: Training Loss = 58.12268829345703 Training Accuracy = 0.046201977878808975\n","Iteration 2279: Training Loss = 58.21763610839844 Training Accuracy = 0.04558490589261055\n","Iteration 2280: Training Loss = 58.39590835571289 Training Accuracy = 0.04623165354132652\n","Iteration 2281: Training Loss = 58.7359504699707 Training Accuracy = 0.04509073123335838\n","Iteration 2282: Training Loss = 58.75911331176758 Training Accuracy = 0.04685238003730774\n","Iteration 2283: Training Loss = 58.98051834106445 Training Accuracy = 0.04500259459018707\n","Iteration 2284: Training Loss = 58.58418655395508 Training Accuracy = 0.047041602432727814\n","Iteration 2285: Training Loss = 58.38592529296875 Training Accuracy = 0.04562263935804367\n","Iteration 2286: Training Loss = 58.17547607421875 Training Accuracy = 0.04628954082727432\n","Iteration 2287: Training Loss = 58.09135055541992 Training Accuracy = 0.045832399278879166\n","Iteration 2288: Training Loss = 58.0871696472168 Training Accuracy = 0.04554056003689766\n","Iteration 2289: Training Loss = 58.14990234375 Training Accuracy = 0.04585571959614754\n","Iteration 2290: Training Loss = 58.274959564208984 Training Accuracy = 0.04525811970233917\n","Iteration 2291: Training Loss = 58.36520767211914 Training Accuracy = 0.046344559639692307\n","Iteration 2292: Training Loss = 58.55510711669922 Training Accuracy = 0.04516594484448433\n","Iteration 2293: Training Loss = 58.513336181640625 Training Accuracy = 0.04690246656537056\n","Iteration 2294: Training Loss = 58.588802337646484 Training Accuracy = 0.04545261338353157\n","Iteration 2295: Training Loss = 58.395816802978516 Training Accuracy = 0.046855103224515915\n","Iteration 2296: Training Loss = 58.321311950683594 Training Accuracy = 0.0457460880279541\n","Iteration 2297: Training Loss = 58.179786682128906 Training Accuracy = 0.0465216189622879\n","Iteration 2298: Training Loss = 58.11076736450195 Training Accuracy = 0.04571130871772766\n","Iteration 2299: Training Loss = 58.060569763183594 Training Accuracy = 0.04617474600672722\n","Iteration 2300: Training Loss = 58.035362243652344 Training Accuracy = 0.0458407960832119\n","Iteration 2301: Training Loss = 58.02363204956055 Training Accuracy = 0.046013712882995605\n","Iteration 2302: Training Loss = 58.023006439208984 Training Accuracy = 0.04621428623795509\n","Iteration 2303: Training Loss = 58.03105163574219 Training Accuracy = 0.046075914055109024\n","Iteration 2304: Training Loss = 58.049407958984375 Training Accuracy = 0.04656628519296646\n","Iteration 2305: Training Loss = 58.088226318359375 Training Accuracy = 0.04618696868419647\n","Iteration 2306: Training Loss = 58.13597869873047 Training Accuracy = 0.046930186450481415\n","Iteration 2307: Training Loss = 58.25474548339844 Training Accuracy = 0.04605261608958244\n","Iteration 2308: Training Loss = 58.355159759521484 Training Accuracy = 0.04732701927423477\n","Iteration 2309: Training Loss = 58.654991149902344 Training Accuracy = 0.045825254172086716\n","Iteration 2310: Training Loss = 58.68550109863281 Training Accuracy = 0.04768078401684761\n","Iteration 2311: Training Loss = 59.03843688964844 Training Accuracy = 0.0457841195166111\n","Iteration 2312: Training Loss = 58.68461990356445 Training Accuracy = 0.047821640968322754\n","Iteration 2313: Training Loss = 58.63557815551758 Training Accuracy = 0.04600439965724945\n","Iteration 2314: Training Loss = 58.30224609375 Training Accuracy = 0.04741917550563812\n","Iteration 2315: Training Loss = 58.16350173950195 Training Accuracy = 0.046204932034015656\n","Iteration 2316: Training Loss = 58.04438400268555 Training Accuracy = 0.04675552621483803\n","Iteration 2317: Training Loss = 57.99277877807617 Training Accuracy = 0.04619211331009865\n","Iteration 2318: Training Loss = 57.96806716918945 Training Accuracy = 0.046369537711143494\n","Iteration 2319: Training Loss = 57.96328353881836 Training Accuracy = 0.046326763927936554\n","Iteration 2320: Training Loss = 57.9742431640625 Training Accuracy = 0.0462447926402092\n","Iteration 2321: Training Loss = 58.00236892700195 Training Accuracy = 0.046715326607227325\n","Iteration 2322: Training Loss = 58.06309127807617 Training Accuracy = 0.04619719460606575\n","Iteration 2323: Training Loss = 58.13011169433594 Training Accuracy = 0.04719562083482742\n","Iteration 2324: Training Loss = 58.28624725341797 Training Accuracy = 0.046114031225442886\n","Iteration 2325: Training Loss = 58.359832763671875 Training Accuracy = 0.04753095284104347\n","Iteration 2326: Training Loss = 58.634490966796875 Training Accuracy = 0.04599951207637787\n","Iteration 2327: Training Loss = 58.559329986572266 Training Accuracy = 0.04776604846119881\n","Iteration 2328: Training Loss = 58.741519927978516 Training Accuracy = 0.0459732785820961\n","Iteration 2329: Training Loss = 58.4571418762207 Training Accuracy = 0.047729890793561935\n","Iteration 2330: Training Loss = 58.39352798461914 Training Accuracy = 0.0462055467069149\n","Iteration 2331: Training Loss = 58.169700622558594 Training Accuracy = 0.04735355079174042\n","Iteration 2332: Training Loss = 58.079959869384766 Training Accuracy = 0.04633046314120293\n","Iteration 2333: Training Loss = 57.989524841308594 Training Accuracy = 0.04696911200881004\n","Iteration 2334: Training Loss = 57.94682312011719 Training Accuracy = 0.04633596912026405\n","Iteration 2335: Training Loss = 57.91591262817383 Training Accuracy = 0.046717625111341476\n","Iteration 2336: Training Loss = 57.8979606628418 Training Accuracy = 0.046485524624586105\n","Iteration 2337: Training Loss = 57.88963317871094 Training Accuracy = 0.04657772183418274\n","Iteration 2338: Training Loss = 57.890201568603516 Training Accuracy = 0.04677499830722809\n","Iteration 2339: Training Loss = 57.898433685302734 Training Accuracy = 0.046618156135082245\n","Iteration 2340: Training Loss = 57.91288375854492 Training Accuracy = 0.0470796562731266\n","Iteration 2341: Training Loss = 57.943790435791016 Training Accuracy = 0.04668169468641281\n","Iteration 2342: Training Loss = 57.98624038696289 Training Accuracy = 0.04739447310566902\n","Iteration 2343: Training Loss = 58.10004425048828 Training Accuracy = 0.04658297076821327\n","Iteration 2344: Training Loss = 58.22145080566406 Training Accuracy = 0.04782276600599289\n","Iteration 2345: Training Loss = 58.57989501953125 Training Accuracy = 0.04636184126138687\n","Iteration 2346: Training Loss = 58.66400909423828 Training Accuracy = 0.04833956062793732\n","Iteration 2347: Training Loss = 59.151695251464844 Training Accuracy = 0.04624870419502258\n","Iteration 2348: Training Loss = 58.65457534790039 Training Accuracy = 0.048504505306482315\n","Iteration 2349: Training Loss = 58.533897399902344 Training Accuracy = 0.046581462025642395\n","Iteration 2350: Training Loss = 58.137908935546875 Training Accuracy = 0.04788065329194069\n","Iteration 2351: Training Loss = 57.96814727783203 Training Accuracy = 0.04674583300948143\n","Iteration 2352: Training Loss = 57.867523193359375 Training Accuracy = 0.04700888693332672\n","Iteration 2353: Training Loss = 57.84022903442383 Training Accuracy = 0.046667177230119705\n","Iteration 2354: Training Loss = 57.85767364501953 Training Accuracy = 0.04650609940290451\n","Iteration 2355: Training Loss = 57.90622329711914 Training Accuracy = 0.046850044280290604\n","Iteration 2356: Training Loss = 58.006011962890625 Training Accuracy = 0.046274296939373016\n","Iteration 2357: Training Loss = 58.100120544433594 Training Accuracy = 0.047400277107954025\n","Iteration 2358: Training Loss = 58.314788818359375 Training Accuracy = 0.04614472761750221\n","Iteration 2359: Training Loss = 58.31716537475586 Training Accuracy = 0.047853633761405945\n","Iteration 2360: Training Loss = 58.485206604003906 Training Accuracy = 0.04622302204370499\n","Iteration 2361: Training Loss = 58.27325439453125 Training Accuracy = 0.047816261649131775\n","Iteration 2362: Training Loss = 58.238563537597656 Training Accuracy = 0.046369343996047974\n","Iteration 2363: Training Loss = 58.04978561401367 Training Accuracy = 0.047448236495256424\n","Iteration 2364: Training Loss = 57.968353271484375 Training Accuracy = 0.04645184800028801\n","Iteration 2365: Training Loss = 57.87752151489258 Training Accuracy = 0.04712127894163132\n","Iteration 2366: Training Loss = 57.83392333984375 Training Accuracy = 0.04659723490476608\n","Iteration 2367: Training Loss = 57.80563735961914 Training Accuracy = 0.047011371701955795\n","Iteration 2368: Training Loss = 57.79219055175781 Training Accuracy = 0.046772152185440063\n","Iteration 2369: Training Loss = 57.78203201293945 Training Accuracy = 0.047062989324331284\n","Iteration 2370: Training Loss = 57.77304458618164 Training Accuracy = 0.04694139212369919\n","Iteration 2371: Training Loss = 57.764739990234375 Training Accuracy = 0.047091495245695114\n","Iteration 2372: Training Loss = 57.7589225769043 Training Accuracy = 0.047139327973127365\n","Iteration 2373: Training Loss = 57.75634765625 Training Accuracy = 0.04711628332734108\n","Iteration 2374: Training Loss = 57.75674057006836 Training Accuracy = 0.04733356833457947\n","Iteration 2375: Training Loss = 57.76055908203125 Training Accuracy = 0.04717954993247986\n","Iteration 2376: Training Loss = 57.76877212524414 Training Accuracy = 0.04758618026971817\n","Iteration 2377: Training Loss = 57.794071197509766 Training Accuracy = 0.04726162925362587\n","Iteration 2378: Training Loss = 57.84440231323242 Training Accuracy = 0.04797661677002907\n","Iteration 2379: Training Loss = 57.99436950683594 Training Accuracy = 0.047184184193611145\n","Iteration 2380: Training Loss = 58.2094612121582 Training Accuracy = 0.04857642576098442\n","Iteration 2381: Training Loss = 58.85177230834961 Training Accuracy = 0.046819452196359634\n","Iteration 2382: Training Loss = 58.92096710205078 Training Accuracy = 0.04925911873579025\n","Iteration 2383: Training Loss = 59.62338638305664 Training Accuracy = 0.046654846519231796\n","Iteration 2384: Training Loss = 58.55704879760742 Training Accuracy = 0.049240052700042725\n","Iteration 2385: Training Loss = 58.11299133300781 Training Accuracy = 0.04735816270112991\n","Iteration 2386: Training Loss = 57.79130554199219 Training Accuracy = 0.04790357127785683\n","Iteration 2387: Training Loss = 57.725067138671875 Training Accuracy = 0.04746012017130852\n","Iteration 2388: Training Loss = 57.81293869018555 Training Accuracy = 0.04671900346875191\n","Iteration 2389: Training Loss = 57.996524810791016 Training Accuracy = 0.047397173941135406\n","Iteration 2390: Training Loss = 58.34855651855469 Training Accuracy = 0.04622015357017517\n","Iteration 2391: Training Loss = 58.36193084716797 Training Accuracy = 0.04785023257136345\n","Iteration 2392: Training Loss = 58.5533332824707 Training Accuracy = 0.046131737530231476\n","Iteration 2393: Training Loss = 58.16392517089844 Training Accuracy = 0.048093345016241074\n","Iteration 2394: Training Loss = 57.96030807495117 Training Accuracy = 0.04664985090494156\n","Iteration 2395: Training Loss = 57.76253890991211 Training Accuracy = 0.047414518892765045\n","Iteration 2396: Training Loss = 57.69390869140625 Training Accuracy = 0.04699757695198059\n","Iteration 2397: Training Loss = 57.7028923034668 Training Accuracy = 0.046598851680755615\n","Iteration 2398: Training Loss = 57.77186584472656 Training Accuracy = 0.04709523916244507\n","Iteration 2399: Training Loss = 57.912410736083984 Training Accuracy = 0.04637646675109863\n","Iteration 2400: Training Loss = 57.99562072753906 Training Accuracy = 0.04745044559240341\n","Iteration 2401: Training Loss = 58.177310943603516 Training Accuracy = 0.04636551812291145\n","Iteration 2402: Training Loss = 58.104000091552734 Training Accuracy = 0.04803992435336113\n","Iteration 2403: Training Loss = 58.14472961425781 Training Accuracy = 0.04651486128568649\n","Iteration 2404: Training Loss = 57.96430969238281 Training Accuracy = 0.04795565456151962\n","Iteration 2405: Training Loss = 57.892391204833984 Training Accuracy = 0.04694049805402756\n","Iteration 2406: Training Loss = 57.77005386352539 Training Accuracy = 0.04753443971276283\n","Iteration 2407: Training Loss = 57.70759963989258 Training Accuracy = 0.046906717121601105\n","Iteration 2408: Training Loss = 57.665374755859375 Training Accuracy = 0.04730960726737976\n","Iteration 2409: Training Loss = 57.646060943603516 Training Accuracy = 0.046934206038713455\n","Iteration 2410: Training Loss = 57.63414764404297 Training Accuracy = 0.04722219333052635\n","Iteration 2411: Training Loss = 57.630619049072266 Training Accuracy = 0.04730286821722984\n","Iteration 2412: Training Loss = 57.634674072265625 Training Accuracy = 0.04722372442483902\n","Iteration 2413: Training Loss = 57.644683837890625 Training Accuracy = 0.04769394174218178\n","Iteration 2414: Training Loss = 57.67113494873047 Training Accuracy = 0.04734491929411888\n","Iteration 2415: Training Loss = 57.713077545166016 Training Accuracy = 0.047967858612537384\n","Iteration 2416: Training Loss = 57.81591796875 Training Accuracy = 0.04728684201836586\n","Iteration 2417: Training Loss = 57.9223747253418 Training Accuracy = 0.048384226858615875\n","Iteration 2418: Training Loss = 58.22895431518555 Training Accuracy = 0.046982694417238235\n","Iteration 2419: Training Loss = 58.320377349853516 Training Accuracy = 0.048897258937358856\n","Iteration 2420: Training Loss = 58.772090911865234 Training Accuracy = 0.0468941330909729\n","Iteration 2421: Training Loss = 58.3885498046875 Training Accuracy = 0.04903309792280197\n","Iteration 2422: Training Loss = 58.35536575317383 Training Accuracy = 0.047219280153512955\n","Iteration 2423: Training Loss = 57.945621490478516 Training Accuracy = 0.048614710569381714\n","Iteration 2424: Training Loss = 57.76705551147461 Training Accuracy = 0.04738590493798256\n","Iteration 2425: Training Loss = 57.63814163208008 Training Accuracy = 0.04786917567253113\n","Iteration 2426: Training Loss = 57.59346008300781 Training Accuracy = 0.047359056770801544\n","Iteration 2427: Training Loss = 57.592002868652344 Training Accuracy = 0.04734162241220474\n","Iteration 2428: Training Loss = 57.616302490234375 Training Accuracy = 0.04748063534498215\n","Iteration 2429: Training Loss = 57.67118453979492 Training Accuracy = 0.047104381024837494\n","Iteration 2430: Training Loss = 57.74069595336914 Training Accuracy = 0.04792714864015579\n","Iteration 2431: Training Loss = 57.89790344238281 Training Accuracy = 0.04700601473450661\n","Iteration 2432: Training Loss = 57.96973419189453 Training Accuracy = 0.0484369695186615\n","Iteration 2433: Training Loss = 58.197776794433594 Training Accuracy = 0.046941496431827545\n","Iteration 2434: Training Loss = 58.07966613769531 Training Accuracy = 0.04866407439112663\n","Iteration 2435: Training Loss = 58.15058517456055 Training Accuracy = 0.04704366624355316\n","Iteration 2436: Training Loss = 57.93324279785156 Training Accuracy = 0.04843924194574356\n","Iteration 2437: Training Loss = 57.87070083618164 Training Accuracy = 0.04722967743873596\n","Iteration 2438: Training Loss = 57.72376251220703 Training Accuracy = 0.04809613153338432\n","Iteration 2439: Training Loss = 57.65740966796875 Training Accuracy = 0.04731794074177742\n","Iteration 2440: Training Loss = 57.595550537109375 Training Accuracy = 0.0479181744158268\n","Iteration 2441: Training Loss = 57.56955337524414 Training Accuracy = 0.0474453866481781\n","Iteration 2442: Training Loss = 57.55121612548828 Training Accuracy = 0.047881294041872025\n","Iteration 2443: Training Loss = 57.540428161621094 Training Accuracy = 0.04754079505801201\n","Iteration 2444: Training Loss = 57.52784729003906 Training Accuracy = 0.047867029905319214\n","Iteration 2445: Training Loss = 57.517433166503906 Training Accuracy = 0.047678060829639435\n","Iteration 2446: Training Loss = 57.51020431518555 Training Accuracy = 0.047889627516269684\n","Iteration 2447: Training Loss = 57.50761795043945 Training Accuracy = 0.047810353338718414\n","Iteration 2448: Training Loss = 57.508548736572266 Training Accuracy = 0.04804893583059311\n","Iteration 2449: Training Loss = 57.512840270996094 Training Accuracy = 0.04793741554021835\n","Iteration 2450: Training Loss = 57.521026611328125 Training Accuracy = 0.048287540674209595\n","Iteration 2451: Training Loss = 57.54669189453125 Training Accuracy = 0.04801991954445839\n","Iteration 2452: Training Loss = 57.59669876098633 Training Accuracy = 0.04866841062903404\n","Iteration 2453: Training Loss = 57.74384307861328 Training Accuracy = 0.0478920079767704\n","Iteration 2454: Training Loss = 57.940181732177734 Training Accuracy = 0.0492427721619606\n","Iteration 2455: Training Loss = 58.51829528808594 Training Accuracy = 0.047544047236442566\n","Iteration 2456: Training Loss = 58.557838439941406 Training Accuracy = 0.049881886690855026\n","Iteration 2457: Training Loss = 59.15837860107422 Training Accuracy = 0.04740608111023903\n","Iteration 2458: Training Loss = 58.29182815551758 Training Accuracy = 0.049868516623973846\n","Iteration 2459: Training Loss = 57.94927978515625 Training Accuracy = 0.04803907498717308\n","Iteration 2460: Training Loss = 57.59867477416992 Training Accuracy = 0.04868327081203461\n","Iteration 2461: Training Loss = 57.47630310058594 Training Accuracy = 0.04810357093811035\n","Iteration 2462: Training Loss = 57.48638153076172 Training Accuracy = 0.04765522852540016\n","Iteration 2463: Training Loss = 57.592342376708984 Training Accuracy = 0.04800231754779816\n","Iteration 2464: Training Loss = 57.80705261230469 Training Accuracy = 0.047217559069395065\n","Iteration 2465: Training Loss = 57.92949295043945 Training Accuracy = 0.04837489500641823\n","Iteration 2466: Training Loss = 58.21543502807617 Training Accuracy = 0.04696318134665489\n","Iteration 2467: Training Loss = 58.03040313720703 Training Accuracy = 0.048903465270996094\n","Iteration 2468: Training Loss = 58.010467529296875 Training Accuracy = 0.04717957228422165\n","Iteration 2469: Training Loss = 57.71249771118164 Training Accuracy = 0.048652127385139465\n","Iteration 2470: Training Loss = 57.56599426269531 Training Accuracy = 0.04763881862163544\n","Iteration 2471: Training Loss = 57.46139907836914 Training Accuracy = 0.04785021394491196\n","Iteration 2472: Training Loss = 57.426631927490234 Training Accuracy = 0.047687966376543045\n","Iteration 2473: Training Loss = 57.444637298583984 Training Accuracy = 0.04748537391424179\n","Iteration 2474: Training Loss = 57.49846649169922 Training Accuracy = 0.04787376523017883\n","Iteration 2475: Training Loss = 57.59820556640625 Training Accuracy = 0.04740157350897789\n","Iteration 2476: Training Loss = 57.678890228271484 Training Accuracy = 0.04851381853222847\n","Iteration 2477: Training Loss = 57.86370849609375 Training Accuracy = 0.04730250686407089\n","Iteration 2478: Training Loss = 57.87277603149414 Training Accuracy = 0.04895967245101929\n","Iteration 2479: Training Loss = 58.030433654785156 Training Accuracy = 0.047468557953834534\n","Iteration 2480: Training Loss = 57.8526496887207 Training Accuracy = 0.04892927408218384\n","Iteration 2481: Training Loss = 57.827659606933594 Training Accuracy = 0.04758601263165474\n","Iteration 2482: Training Loss = 57.63888168334961 Training Accuracy = 0.048597682267427444\n","Iteration 2483: Training Loss = 57.55780792236328 Training Accuracy = 0.047548022121191025\n","Iteration 2484: Training Loss = 57.47114562988281 Training Accuracy = 0.048275742679834366\n","Iteration 2485: Training Loss = 57.42807388305664 Training Accuracy = 0.04766096919775009\n","Iteration 2486: Training Loss = 57.398406982421875 Training Accuracy = 0.048083480447530746\n","Iteration 2487: Training Loss = 57.38175582885742 Training Accuracy = 0.0479363314807415\n","Iteration 2488: Training Loss = 57.36985778808594 Training Accuracy = 0.04811447858810425\n","Iteration 2489: Training Loss = 57.36334991455078 Training Accuracy = 0.04815297573804855\n","Iteration 2490: Training Loss = 57.36119079589844 Training Accuracy = 0.04823660850524902\n","Iteration 2491: Training Loss = 57.361854553222656 Training Accuracy = 0.04838775470852852\n","Iteration 2492: Training Loss = 57.36808395385742 Training Accuracy = 0.048233356326818466\n","Iteration 2493: Training Loss = 57.38123321533203 Training Accuracy = 0.048666007816791534\n","Iteration 2494: Training Loss = 57.41447830200195 Training Accuracy = 0.0481991283595562\n","Iteration 2495: Training Loss = 57.47092056274414 Training Accuracy = 0.04895276203751564\n","Iteration 2496: Training Loss = 57.625450134277344 Training Accuracy = 0.04814574867486954\n","Iteration 2497: Training Loss = 57.79920959472656 Training Accuracy = 0.049471426755189896\n","Iteration 2498: Training Loss = 58.3144416809082 Training Accuracy = 0.04786830395460129\n","Iteration 2499: Training Loss = 58.320980072021484 Training Accuracy = 0.050101425498723984\n","Iteration 2500: Training Loss = 58.80679702758789 Training Accuracy = 0.04778101667761803\n","Iteration 2501: Training Loss = 58.0834846496582 Training Accuracy = 0.050091687589883804\n","Iteration 2502: Training Loss = 57.79777526855469 Training Accuracy = 0.04829547181725502\n","Iteration 2503: Training Loss = 57.46815872192383 Training Accuracy = 0.04905204102396965\n","Iteration 2504: Training Loss = 57.341590881347656 Training Accuracy = 0.04837939888238907\n","Iteration 2505: Training Loss = 57.320804595947266 Training Accuracy = 0.048140838742256165\n","Iteration 2506: Training Loss = 57.378292083740234 Training Accuracy = 0.04827176779508591\n","Iteration 2507: Training Loss = 57.50996017456055 Training Accuracy = 0.047721683979034424\n","Iteration 2508: Training Loss = 57.626953125 Training Accuracy = 0.04863760620355606\n","Iteration 2509: Training Loss = 57.8702507019043 Training Accuracy = 0.04745446518063545\n","Iteration 2510: Training Loss = 57.837310791015625 Training Accuracy = 0.0492008700966835\n","Iteration 2511: Training Loss = 57.9530143737793 Training Accuracy = 0.0475323349237442\n","Iteration 2512: Training Loss = 57.68670654296875 Training Accuracy = 0.04918207973241806\n","Iteration 2513: Training Loss = 57.5696907043457 Training Accuracy = 0.0479154959321022\n","Iteration 2514: Training Loss = 57.40304946899414 Training Accuracy = 0.04859970137476921\n","Iteration 2515: Training Loss = 57.322444915771484 Training Accuracy = 0.048042260110378265\n","Iteration 2516: Training Loss = 57.280487060546875 Training Accuracy = 0.04819876700639725\n","Iteration 2517: Training Loss = 57.27311706542969 Training Accuracy = 0.048151638358831406\n","Iteration 2518: Training Loss = 57.28852462768555 Training Accuracy = 0.04813586175441742\n","Iteration 2519: Training Loss = 57.32173156738281 Training Accuracy = 0.04860084876418114\n","Iteration 2520: Training Loss = 57.39153289794922 Training Accuracy = 0.04805867373943329\n","Iteration 2521: Training Loss = 57.466400146484375 Training Accuracy = 0.04908023029565811\n","Iteration 2522: Training Loss = 57.63601303100586 Training Accuracy = 0.048035457730293274\n","Iteration 2523: Training Loss = 57.69999313354492 Training Accuracy = 0.0494181327521801\n","Iteration 2524: Training Loss = 57.946170806884766 Training Accuracy = 0.04793227091431618\n","Iteration 2525: Training Loss = 57.8126106262207 Training Accuracy = 0.04956135153770447\n","Iteration 2526: Training Loss = 57.88782501220703 Training Accuracy = 0.047906313091516495\n","Iteration 2527: Training Loss = 57.636383056640625 Training Accuracy = 0.04939481243491173\n","Iteration 2528: Training Loss = 57.55004119873047 Training Accuracy = 0.048053041100502014\n","Iteration 2529: Training Loss = 57.395599365234375 Training Accuracy = 0.04896232858300209\n","Iteration 2530: Training Loss = 57.325626373291016 Training Accuracy = 0.048234228044748306\n","Iteration 2531: Training Loss = 57.26734161376953 Training Accuracy = 0.048659227788448334\n","Iteration 2532: Training Loss = 57.23806381225586 Training Accuracy = 0.04831719771027565\n","Iteration 2533: Training Loss = 57.220829010009766 Training Accuracy = 0.048616476356983185\n","Iteration 2534: Training Loss = 57.211158752441406 Training Accuracy = 0.04845476150512695\n","Iteration 2535: Training Loss = 57.20566177368164 Training Accuracy = 0.04859081655740738\n","Iteration 2536: Training Loss = 57.20414352416992 Training Accuracy = 0.04873550310730934\n","Iteration 2537: Training Loss = 57.20735168457031 Training Accuracy = 0.04859670624136925\n","Iteration 2538: Training Loss = 57.216678619384766 Training Accuracy = 0.04897691309452057\n","Iteration 2539: Training Loss = 57.24082565307617 Training Accuracy = 0.04866156354546547\n","Iteration 2540: Training Loss = 57.28203201293945 Training Accuracy = 0.04927518963813782\n","Iteration 2541: Training Loss = 57.388427734375 Training Accuracy = 0.04860057309269905\n","Iteration 2542: Training Loss = 57.51902770996094 Training Accuracy = 0.04972434043884277\n","Iteration 2543: Training Loss = 57.89336395263672 Training Accuracy = 0.048355378210544586\n","Iteration 2544: Training Loss = 58.02053451538086 Training Accuracy = 0.05034885182976723\n","Iteration 2545: Training Loss = 58.60457992553711 Training Accuracy = 0.048169493675231934\n","Iteration 2546: Training Loss = 58.04429244995117 Training Accuracy = 0.050552889704704285\n","Iteration 2547: Training Loss = 57.90308380126953 Training Accuracy = 0.04862380772829056\n","Iteration 2548: Training Loss = 57.461090087890625 Training Accuracy = 0.04981764405965805\n","Iteration 2549: Training Loss = 57.268558502197266 Training Accuracy = 0.04880034178495407\n","Iteration 2550: Training Loss = 57.175559997558594 Training Accuracy = 0.04885068163275719\n","Iteration 2551: Training Loss = 57.17363739013672 Training Accuracy = 0.048676997423172\n","Iteration 2552: Training Loss = 57.23500061035156 Training Accuracy = 0.04836851730942726\n","Iteration 2553: Training Loss = 57.32470703125 Training Accuracy = 0.0488116480410099\n","Iteration 2554: Training Loss = 57.48835754394531 Training Accuracy = 0.04807242751121521\n","Iteration 2555: Training Loss = 57.558448791503906 Training Accuracy = 0.04938896745443344\n","Iteration 2556: Training Loss = 57.77202606201172 Training Accuracy = 0.04792674258351326\n","Iteration 2557: Training Loss = 57.64022445678711 Training Accuracy = 0.04974459856748581\n","Iteration 2558: Training Loss = 57.6461181640625 Training Accuracy = 0.048222873359918594\n","Iteration 2559: Training Loss = 57.411930084228516 Training Accuracy = 0.049456845968961716\n","Iteration 2560: Training Loss = 57.30414581298828 Training Accuracy = 0.04845527186989784\n","Iteration 2561: Training Loss = 57.1963005065918 Training Accuracy = 0.04895978048443794\n","Iteration 2562: Training Loss = 57.14607620239258 Training Accuracy = 0.04853588342666626\n","Iteration 2563: Training Loss = 57.122161865234375 Training Accuracy = 0.048758313059806824\n","Iteration 2564: Training Loss = 57.116947174072266 Training Accuracy = 0.04878290742635727\n","Iteration 2565: Training Loss = 57.12643814086914 Training Accuracy = 0.048703357577323914\n","Iteration 2566: Training Loss = 57.150882720947266 Training Accuracy = 0.049227338284254074\n","Iteration 2567: Training Loss = 57.203125 Training Accuracy = 0.04869617521762848\n","Iteration 2568: Training Loss = 57.26789474487305 Training Accuracy = 0.04954385757446289\n","Iteration 2569: Training Loss = 57.42041015625 Training Accuracy = 0.04864154011011124\n","Iteration 2570: Training Loss = 57.514766693115234 Training Accuracy = 0.04986424371600151\n","Iteration 2571: Training Loss = 57.81170654296875 Training Accuracy = 0.04840616509318352\n","Iteration 2572: Training Loss = 57.74439239501953 Training Accuracy = 0.05020316690206528\n","Iteration 2573: Training Loss = 57.93844985961914 Training Accuracy = 0.048367153853178024\n","Iteration 2574: Training Loss = 57.616336822509766 Training Accuracy = 0.050168879330158234\n","Iteration 2575: Training Loss = 57.52488327026367 Training Accuracy = 0.048636544495821\n","Iteration 2576: Training Loss = 57.29654312133789 Training Accuracy = 0.049655865877866745\n","Iteration 2577: Training Loss = 57.19383239746094 Training Accuracy = 0.0487973652780056\n","Iteration 2578: Training Loss = 57.11298370361328 Training Accuracy = 0.049201276153326035\n","Iteration 2579: Training Loss = 57.07663345336914 Training Accuracy = 0.048799723386764526\n","Iteration 2580: Training Loss = 57.06200408935547 Training Accuracy = 0.049021024256944656\n","Iteration 2581: Training Loss = 57.061668395996094 Training Accuracy = 0.04899530112743378\n","Iteration 2582: Training Loss = 57.07225799560547 Training Accuracy = 0.048906441777944565\n","Iteration 2583: Training Loss = 57.09469223022461 Training Accuracy = 0.04933120682835579\n","Iteration 2584: Training Loss = 57.14175796508789 Training Accuracy = 0.048857949674129486\n","Iteration 2585: Training Loss = 57.20146179199219 Training Accuracy = 0.04970512166619301\n","Iteration 2586: Training Loss = 57.346439361572266 Training Accuracy = 0.04881611466407776\n","Iteration 2587: Training Loss = 57.44486618041992 Training Accuracy = 0.05010782182216644\n","Iteration 2588: Training Loss = 57.750484466552734 Training Accuracy = 0.0486556775867939\n","Iteration 2589: Training Loss = 57.701393127441406 Training Accuracy = 0.050487350672483444\n","Iteration 2590: Training Loss = 57.935081481933594 Training Accuracy = 0.048613306134939194\n","Iteration 2591: Training Loss = 57.61137390136719 Training Accuracy = 0.05047544464468956\n","Iteration 2592: Training Loss = 57.539306640625 Training Accuracy = 0.048925042152404785\n","Iteration 2593: Training Loss = 57.28310012817383 Training Accuracy = 0.04995875805616379\n","Iteration 2594: Training Loss = 57.16508483886719 Training Accuracy = 0.04903216287493706\n","Iteration 2595: Training Loss = 57.06279754638672 Training Accuracy = 0.049439605325460434\n","Iteration 2596: Training Loss = 57.022735595703125 Training Accuracy = 0.04899289831519127\n","Iteration 2597: Training Loss = 57.00648498535156 Training Accuracy = 0.04920218884944916\n","Iteration 2598: Training Loss = 57.0042839050293 Training Accuracy = 0.04907183349132538\n","Iteration 2599: Training Loss = 57.01483154296875 Training Accuracy = 0.04904921352863312\n","Iteration 2600: Training Loss = 57.03459167480469 Training Accuracy = 0.04935833439230919\n","Iteration 2601: Training Loss = 57.07289505004883 Training Accuracy = 0.04894619435071945\n","Iteration 2602: Training Loss = 57.122840881347656 Training Accuracy = 0.04975206032395363\n","Iteration 2603: Training Loss = 57.254581451416016 Training Accuracy = 0.04893754422664642\n","Iteration 2604: Training Loss = 57.35369873046875 Training Accuracy = 0.0502273365855217\n","Iteration 2605: Training Loss = 57.64138412475586 Training Accuracy = 0.04880918189883232\n","Iteration 2606: Training Loss = 57.60323715209961 Training Accuracy = 0.05062283203005791\n","Iteration 2607: Training Loss = 57.833099365234375 Training Accuracy = 0.04879162460565567\n","Iteration 2608: Training Loss = 57.54278564453125 Training Accuracy = 0.05060102045536041\n","Iteration 2609: Training Loss = 57.50624465942383 Training Accuracy = 0.04901957884430885\n","Iteration 2610: Training Loss = 57.25907897949219 Training Accuracy = 0.05014092102646828\n","Iteration 2611: Training Loss = 57.15169906616211 Training Accuracy = 0.04919974505901337\n","Iteration 2612: Training Loss = 57.036338806152344 Training Accuracy = 0.049702081829309464\n","Iteration 2613: Training Loss = 56.98334503173828 Training Accuracy = 0.04916800558567047\n","Iteration 2614: Training Loss = 56.957115173339844 Training Accuracy = 0.04946096986532211\n","Iteration 2615: Training Loss = 56.949363708496094 Training Accuracy = 0.049165986478328705\n","Iteration 2616: Training Loss = 56.94820022583008 Training Accuracy = 0.04932712763547897\n","Iteration 2617: Training Loss = 56.948368072509766 Training Accuracy = 0.04935144633054733\n","Iteration 2618: Training Loss = 56.95086669921875 Training Accuracy = 0.04927944391965866\n","Iteration 2619: Training Loss = 56.959442138671875 Training Accuracy = 0.04967661574482918\n","Iteration 2620: Training Loss = 56.99013900756836 Training Accuracy = 0.04932423308491707\n","Iteration 2621: Training Loss = 57.03938674926758 Training Accuracy = 0.05004264414310455\n","Iteration 2622: Training Loss = 57.167240142822266 Training Accuracy = 0.04926658049225807\n","Iteration 2623: Training Loss = 57.28513717651367 Training Accuracy = 0.050534628331661224\n","Iteration 2624: Training Loss = 57.61991500854492 Training Accuracy = 0.04904670640826225\n","Iteration 2625: Training Loss = 57.63998794555664 Training Accuracy = 0.05098479986190796\n","Iteration 2626: Training Loss = 58.014137268066406 Training Accuracy = 0.04901181906461716\n","Iteration 2627: Training Loss = 57.6391716003418 Training Accuracy = 0.051023490726947784\n","Iteration 2628: Training Loss = 57.590335845947266 Training Accuracy = 0.049310438334941864\n","Iteration 2629: Training Loss = 57.23105239868164 Training Accuracy = 0.05048677697777748\n","Iteration 2630: Training Loss = 57.064170837402344 Training Accuracy = 0.04943150654435158\n","Iteration 2631: Training Loss = 56.93449020385742 Training Accuracy = 0.04979168623685837\n","Iteration 2632: Training Loss = 56.90087127685547 Training Accuracy = 0.0492987260222435\n","Iteration 2633: Training Loss = 56.894615173339844 Training Accuracy = 0.049450382590293884\n","Iteration 2634: Training Loss = 56.90898513793945 Training Accuracy = 0.04936177656054497\n","Iteration 2635: Training Loss = 56.94011306762695 Training Accuracy = 0.049188412725925446\n","Iteration 2636: Training Loss = 56.97380065917969 Training Accuracy = 0.04967661574482918\n","Iteration 2637: Training Loss = 57.05771255493164 Training Accuracy = 0.0490005724132061\n","Iteration 2638: Training Loss = 57.13800048828125 Training Accuracy = 0.050169069319963455\n","Iteration 2639: Training Loss = 57.35615539550781 Training Accuracy = 0.048925936222076416\n","Iteration 2640: Training Loss = 57.35657501220703 Training Accuracy = 0.05056789889931679\n","Iteration 2641: Training Loss = 57.53049850463867 Training Accuracy = 0.049026403576135635\n","Iteration 2642: Training Loss = 57.32600021362305 Training Accuracy = 0.05065952241420746\n","Iteration 2643: Training Loss = 57.3224983215332 Training Accuracy = 0.04916662350296974\n","Iteration 2644: Training Loss = 57.14604187011719 Training Accuracy = 0.05033186823129654\n","Iteration 2645: Training Loss = 57.089351654052734 Training Accuracy = 0.04929991438984871\n","Iteration 2646: Training Loss = 56.98185729980469 Training Accuracy = 0.050019919872283936\n","Iteration 2647: Training Loss = 56.933712005615234 Training Accuracy = 0.049393028020858765\n","Iteration 2648: Training Loss = 56.890018463134766 Training Accuracy = 0.04994198679924011\n","Iteration 2649: Training Loss = 56.882476806640625 Training Accuracy = 0.049417708069086075\n","Iteration 2650: Training Loss = 56.87403869628906 Training Accuracy = 0.04998037964105606\n","Iteration 2651: Training Loss = 56.87617492675781 Training Accuracy = 0.04936237260699272\n","Iteration 2652: Training Loss = 56.87015151977539 Training Accuracy = 0.0499626062810421\n","Iteration 2653: Training Loss = 56.8834342956543 Training Accuracy = 0.04946213960647583\n","Iteration 2654: Training Loss = 56.898441314697266 Training Accuracy = 0.050135161727666855\n","Iteration 2655: Training Loss = 56.96455764770508 Training Accuracy = 0.049511563032865524\n","Iteration 2656: Training Loss = 57.03437042236328 Training Accuracy = 0.050488393753767014\n","Iteration 2657: Training Loss = 57.22996139526367 Training Accuracy = 0.04945065826177597\n","Iteration 2658: Training Loss = 57.325443267822266 Training Accuracy = 0.050945643335580826\n","Iteration 2659: Training Loss = 57.69004440307617 Training Accuracy = 0.049288518726825714\n","Iteration 2660: Training Loss = 57.528560638427734 Training Accuracy = 0.05127517133951187\n","Iteration 2661: Training Loss = 57.66670227050781 Training Accuracy = 0.04935210570693016\n","Iteration 2662: Training Loss = 57.26692199707031 Training Accuracy = 0.05105406045913696\n","Iteration 2663: Training Loss = 57.116943359375 Training Accuracy = 0.049575190991163254\n","Iteration 2664: Training Loss = 56.917137145996094 Training Accuracy = 0.05035429447889328\n","Iteration 2665: Training Loss = 56.826637268066406 Training Accuracy = 0.049615178257226944\n","Iteration 2666: Training Loss = 56.775917053222656 Training Accuracy = 0.049791473895311356\n","Iteration 2667: Training Loss = 56.7600212097168 Training Accuracy = 0.04965612292289734\n","Iteration 2668: Training Loss = 56.76629638671875 Training Accuracy = 0.04961039498448372\n","Iteration 2669: Training Loss = 56.78933334350586 Training Accuracy = 0.04987374693155289\n","Iteration 2670: Training Loss = 56.836578369140625 Training Accuracy = 0.0495309941470623\n","Iteration 2671: Training Loss = 56.895668029785156 Training Accuracy = 0.05029842630028725\n","Iteration 2672: Training Loss = 57.02449417114258 Training Accuracy = 0.04944666102528572\n","Iteration 2673: Training Loss = 57.10838317871094 Training Accuracy = 0.05077312886714935\n","Iteration 2674: Training Loss = 57.34712219238281 Training Accuracy = 0.04940576106309891\n","Iteration 2675: Training Loss = 57.304256439208984 Training Accuracy = 0.051073066890239716\n","Iteration 2676: Training Loss = 57.474884033203125 Training Accuracy = 0.049434565007686615\n","Iteration 2677: Training Loss = 57.2332649230957 Training Accuracy = 0.05105892941355705\n","Iteration 2678: Training Loss = 57.1917724609375 Training Accuracy = 0.04960491135716438\n","Iteration 2679: Training Loss = 56.99166488647461 Training Accuracy = 0.05073724314570427\n","Iteration 2680: Training Loss = 56.91047668457031 Training Accuracy = 0.04972085356712341\n","Iteration 2681: Training Loss = 56.81843185424805 Training Accuracy = 0.05033331364393234\n","Iteration 2682: Training Loss = 56.776634216308594 Training Accuracy = 0.04978188872337341\n","Iteration 2683: Training Loss = 56.74508285522461 Training Accuracy = 0.05014343187212944\n","Iteration 2684: Training Loss = 56.72964096069336 Training Accuracy = 0.04979313164949417\n","Iteration 2685: Training Loss = 56.718143463134766 Training Accuracy = 0.05014545097947121\n","Iteration 2686: Training Loss = 56.709720611572266 Training Accuracy = 0.04992438480257988\n","Iteration 2687: Training Loss = 56.70218276977539 Training Accuracy = 0.05018654465675354\n","Iteration 2688: Training Loss = 56.69660186767578 Training Accuracy = 0.05014961585402489\n","Iteration 2689: Training Loss = 56.69259262084961 Training Accuracy = 0.05030157417058945\n","Iteration 2690: Training Loss = 56.689762115478516 Training Accuracy = 0.050339072942733765\n","Iteration 2691: Training Loss = 56.68801498413086 Training Accuracy = 0.05050882324576378\n","Iteration 2692: Training Loss = 56.687835693359375 Training Accuracy = 0.05046549811959267\n","Iteration 2693: Training Loss = 56.68976593017578 Training Accuracy = 0.05073903128504753\n","Iteration 2694: Training Loss = 56.698516845703125 Training Accuracy = 0.050571706146001816\n","Iteration 2695: Training Loss = 56.71977615356445 Training Accuracy = 0.051008887588977814\n","Iteration 2696: Training Loss = 56.78107833862305 Training Accuracy = 0.05057215318083763\n","Iteration 2697: Training Loss = 56.89063262939453 Training Accuracy = 0.051429104059934616\n","Iteration 2698: Training Loss = 57.204036712646484 Training Accuracy = 0.05029262229800224\n","Iteration 2699: Training Loss = 57.481285095214844 Training Accuracy = 0.05209145322442055\n","Iteration 2700: Training Loss = 58.32828903198242 Training Accuracy = 0.049819983541965485\n","Iteration 2701: Training Loss = 57.803462982177734 Training Accuracy = 0.05251028761267662\n","Iteration 2702: Training Loss = 57.82472229003906 Training Accuracy = 0.05012686923146248\n","Iteration 2703: Training Loss = 57.15278625488281 Training Accuracy = 0.05177973955869675\n","Iteration 2704: Training Loss = 56.87327194213867 Training Accuracy = 0.050484757870435715\n","Iteration 2705: Training Loss = 56.69877243041992 Training Accuracy = 0.05048499256372452\n","Iteration 2706: Training Loss = 56.652809143066406 Training Accuracy = 0.05026679486036301\n","Iteration 2707: Training Loss = 56.69219207763672 Training Accuracy = 0.04996315762400627\n","Iteration 2708: Training Loss = 56.79171371459961 Training Accuracy = 0.05026179924607277\n","Iteration 2709: Training Loss = 56.96965026855469 Training Accuracy = 0.0497293584048748\n","Iteration 2710: Training Loss = 57.07270431518555 Training Accuracy = 0.05078939348459244\n","Iteration 2711: Training Loss = 57.34739303588867 Training Accuracy = 0.04938837140798569\n","Iteration 2712: Training Loss = 57.23468017578125 Training Accuracy = 0.05135973542928696\n","Iteration 2713: Training Loss = 57.277069091796875 Training Accuracy = 0.04958745837211609\n","Iteration 2714: Training Loss = 56.965789794921875 Training Accuracy = 0.051046259701251984\n","Iteration 2715: Training Loss = 56.81585693359375 Training Accuracy = 0.049974530935287476\n","Iteration 2716: Training Loss = 56.679229736328125 Training Accuracy = 0.05030087009072304\n","Iteration 2717: Training Loss = 56.62481689453125 Training Accuracy = 0.05003618076443672\n","Iteration 2718: Training Loss = 56.623191833496094 Training Accuracy = 0.05000769719481468\n","Iteration 2719: Training Loss = 56.660160064697266 Training Accuracy = 0.05025758966803551\n","Iteration 2720: Training Loss = 56.73408889770508 Training Accuracy = 0.0499199815094471\n","Iteration 2721: Training Loss = 56.80975341796875 Training Accuracy = 0.05081811174750328\n","Iteration 2722: Training Loss = 56.96998596191406 Training Accuracy = 0.04976598545908928\n","Iteration 2723: Training Loss = 57.01698303222656 Training Accuracy = 0.05128088966012001\n","Iteration 2724: Training Loss = 57.192832946777344 Training Accuracy = 0.049842920154333115\n","Iteration 2725: Training Loss = 57.0686149597168 Training Accuracy = 0.05132434144616127\n","Iteration 2726: Training Loss = 57.08961486816406 Training Accuracy = 0.04996422305703163\n","Iteration 2727: Training Loss = 56.89045715332031 Training Accuracy = 0.051062796264886856\n","Iteration 2728: Training Loss = 56.8093376159668 Training Accuracy = 0.05001164972782135\n","Iteration 2729: Training Loss = 56.699832916259766 Training Accuracy = 0.050783440470695496\n","Iteration 2730: Training Loss = 56.6477165222168 Training Accuracy = 0.05006785690784454\n","Iteration 2731: Training Loss = 56.6085319519043 Training Accuracy = 0.050586819648742676\n","Iteration 2732: Training Loss = 56.58932876586914 Training Accuracy = 0.05030125379562378\n","Iteration 2733: Training Loss = 56.57609558105469 Training Accuracy = 0.05053329095244408\n","Iteration 2734: Training Loss = 56.567344665527344 Training Accuracy = 0.05050278455018997\n","Iteration 2735: Training Loss = 56.560951232910156 Training Accuracy = 0.050649043172597885\n","Iteration 2736: Training Loss = 56.55659866333008 Training Accuracy = 0.0506838858127594\n","Iteration 2737: Training Loss = 56.554725646972656 Training Accuracy = 0.05073054879903793\n","Iteration 2738: Training Loss = 56.555198669433594 Training Accuracy = 0.05090863257646561\n","Iteration 2739: Training Loss = 56.56081771850586 Training Accuracy = 0.05075669661164284\n","Iteration 2740: Training Loss = 56.573951721191406 Training Accuracy = 0.0512094609439373\n","Iteration 2741: Training Loss = 56.60763931274414 Training Accuracy = 0.050774574279785156\n","Iteration 2742: Training Loss = 56.663902282714844 Training Accuracy = 0.05155274271965027\n","Iteration 2743: Training Loss = 56.81499099731445 Training Accuracy = 0.05072421208024025\n","Iteration 2744: Training Loss = 56.98627853393555 Training Accuracy = 0.05202261731028557\n","Iteration 2745: Training Loss = 57.47964859008789 Training Accuracy = 0.05042263865470886\n","Iteration 2746: Training Loss = 57.51310348510742 Training Accuracy = 0.05260539799928665\n","Iteration 2747: Training Loss = 58.00290298461914 Training Accuracy = 0.05028773471713066\n","Iteration 2748: Training Loss = 57.32608413696289 Training Accuracy = 0.05258750170469284\n","Iteration 2749: Training Loss = 57.078758239746094 Training Accuracy = 0.05072782561182976\n","Iteration 2750: Training Loss = 56.7223014831543 Training Accuracy = 0.05156096816062927\n","Iteration 2751: Training Loss = 56.57866668701172 Training Accuracy = 0.050742559134960175\n","Iteration 2752: Training Loss = 56.520233154296875 Training Accuracy = 0.05065053328871727\n","Iteration 2753: Training Loss = 56.53142547607422 Training Accuracy = 0.05060238018631935\n","Iteration 2754: Training Loss = 56.592063903808594 Training Accuracy = 0.05038730800151825\n","Iteration 2755: Training Loss = 56.67448043823242 Training Accuracy = 0.05084073171019554\n","Iteration 2756: Training Loss = 56.83512878417969 Training Accuracy = 0.05016181990504265\n","Iteration 2757: Training Loss = 56.921958923339844 Training Accuracy = 0.051433779299259186\n","Iteration 2758: Training Loss = 57.16958999633789 Training Accuracy = 0.049939073622226715\n","Iteration 2759: Training Loss = 57.0540885925293 Training Accuracy = 0.05182795599102974\n","Iteration 2760: Training Loss = 57.088600158691406 Training Accuracy = 0.05018794536590576\n","Iteration 2761: Training Loss = 56.81684875488281 Training Accuracy = 0.05150201916694641\n","Iteration 2762: Training Loss = 56.694053649902344 Training Accuracy = 0.0504312701523304\n","Iteration 2763: Training Loss = 56.56635665893555 Training Accuracy = 0.05095106363296509\n","Iteration 2764: Training Loss = 56.5074577331543 Training Accuracy = 0.050491370260715485\n","Iteration 2765: Training Loss = 56.48124694824219 Training Accuracy = 0.050718940794467926\n","Iteration 2766: Training Loss = 56.47848892211914 Training Accuracy = 0.05073915794491768\n","Iteration 2767: Training Loss = 56.493614196777344 Training Accuracy = 0.05063486471772194\n","Iteration 2768: Training Loss = 56.5252685546875 Training Accuracy = 0.05113628879189491\n","Iteration 2769: Training Loss = 56.590179443359375 Training Accuracy = 0.050561755895614624\n","Iteration 2770: Training Loss = 56.6590461730957 Training Accuracy = 0.05153384432196617\n","Iteration 2771: Training Loss = 56.81489944458008 Training Accuracy = 0.05053681880235672\n","Iteration 2772: Training Loss = 56.880130767822266 Training Accuracy = 0.05181232839822769\n","Iteration 2773: Training Loss = 57.10710144042969 Training Accuracy = 0.0503988079726696\n","Iteration 2774: Training Loss = 57.00139236450195 Training Accuracy = 0.052016858011484146\n","Iteration 2775: Training Loss = 57.086082458496094 Training Accuracy = 0.0504012331366539\n","Iteration 2776: Training Loss = 56.8587760925293 Training Accuracy = 0.051924191415309906\n","Iteration 2777: Training Loss = 56.79433822631836 Training Accuracy = 0.0505705364048481\n","Iteration 2778: Training Loss = 56.63855743408203 Training Accuracy = 0.05153496935963631\n","Iteration 2779: Training Loss = 56.5755615234375 Training Accuracy = 0.05068707466125488\n","Iteration 2780: Training Loss = 56.51073455810547 Training Accuracy = 0.05122661590576172\n","Iteration 2781: Training Loss = 56.48196792602539 Training Accuracy = 0.050727084279060364\n","Iteration 2782: Training Loss = 56.459442138671875 Training Accuracy = 0.05120754614472389\n","Iteration 2783: Training Loss = 56.44889831542969 Training Accuracy = 0.05079779028892517\n","Iteration 2784: Training Loss = 56.440364837646484 Training Accuracy = 0.05123641714453697\n","Iteration 2785: Training Loss = 56.437965393066406 Training Accuracy = 0.050968706607818604\n","Iteration 2786: Training Loss = 56.43849182128906 Training Accuracy = 0.051364243030548096\n","Iteration 2787: Training Loss = 56.44844055175781 Training Accuracy = 0.05110371857881546\n","Iteration 2788: Training Loss = 56.46448516845703 Training Accuracy = 0.05158909410238266\n","Iteration 2789: Training Loss = 56.5083122253418 Training Accuracy = 0.05108743533492088\n","Iteration 2790: Training Loss = 56.565284729003906 Training Accuracy = 0.051961202174425125\n","Iteration 2791: Training Loss = 56.71524429321289 Training Accuracy = 0.05101026967167854\n","Iteration 2792: Training Loss = 56.8348503112793 Training Accuracy = 0.05235986411571503\n","Iteration 2793: Training Loss = 57.19452667236328 Training Accuracy = 0.050847236067056656\n","Iteration 2794: Training Loss = 57.15742492675781 Training Accuracy = 0.05276515707373619\n","Iteration 2795: Training Loss = 57.4374885559082 Training Accuracy = 0.05078548192977905\n","Iteration 2796: Training Loss = 57.03185272216797 Training Accuracy = 0.052706293761730194\n","Iteration 2797: Training Loss = 56.917083740234375 Training Accuracy = 0.05098539590835571\n","Iteration 2798: Training Loss = 56.636417388916016 Training Accuracy = 0.05200301855802536\n","Iteration 2799: Training Loss = 56.514854431152344 Training Accuracy = 0.05102478712797165\n","Iteration 2800: Training Loss = 56.423851013183594 Training Accuracy = 0.05133131518959999\n","Iteration 2801: Training Loss = 56.3839111328125 Training Accuracy = 0.050986308604478836\n","Iteration 2802: Training Loss = 56.3701171875 Training Accuracy = 0.0511600561439991\n","Iteration 2803: Training Loss = 56.37229537963867 Training Accuracy = 0.05107500031590462\n","Iteration 2804: Training Loss = 56.38283157348633 Training Accuracy = 0.05114179477095604\n","Iteration 2805: Training Loss = 56.400455474853516 Training Accuracy = 0.05138767138123512\n","Iteration 2806: Training Loss = 56.436851501464844 Training Accuracy = 0.0510699599981308\n","Iteration 2807: Training Loss = 56.484249114990234 Training Accuracy = 0.05186932533979416\n","Iteration 2808: Training Loss = 56.595211029052734 Training Accuracy = 0.05100236088037491\n","Iteration 2809: Training Loss = 56.68304443359375 Training Accuracy = 0.05220969393849373\n","Iteration 2810: Training Loss = 56.92521667480469 Training Accuracy = 0.0509219616651535\n","Iteration 2811: Training Loss = 56.90712356567383 Training Accuracy = 0.05258282274007797\n","Iteration 2812: Training Loss = 57.09904861450195 Training Accuracy = 0.050842516124248505\n","Iteration 2813: Training Loss = 56.870418548583984 Training Accuracy = 0.05258656293153763\n","Iteration 2814: Training Loss = 56.85296630859375 Training Accuracy = 0.051006950438022614\n","Iteration 2815: Training Loss = 56.64689254760742 Training Accuracy = 0.05219053849577904\n","Iteration 2816: Training Loss = 56.57475280761719 Training Accuracy = 0.05113399401307106\n","Iteration 2817: Training Loss = 56.46580123901367 Training Accuracy = 0.0517546571791172\n","Iteration 2818: Training Loss = 56.41554641723633 Training Accuracy = 0.05117638036608696\n","Iteration 2819: Training Loss = 56.37289047241211 Training Accuracy = 0.05162561684846878\n","Iteration 2820: Training Loss = 56.3570556640625 Training Accuracy = 0.05112140625715256\n","Iteration 2821: Training Loss = 56.34878921508789 Training Accuracy = 0.05164868012070656\n","Iteration 2822: Training Loss = 56.35116958618164 Training Accuracy = 0.051158417016267776\n","Iteration 2823: Training Loss = 56.35098648071289 Training Accuracy = 0.05170537903904915\n","Iteration 2824: Training Loss = 56.35948181152344 Training Accuracy = 0.051305122673511505\n","Iteration 2825: Training Loss = 56.362998962402344 Training Accuracy = 0.051868196576833725\n","Iteration 2826: Training Loss = 56.38881301879883 Training Accuracy = 0.05137091875076294\n","Iteration 2827: Training Loss = 56.42030715942383 Training Accuracy = 0.052199020981788635\n","Iteration 2828: Training Loss = 56.51631546020508 Training Accuracy = 0.05135142430663109\n","Iteration 2829: Training Loss = 56.60597610473633 Training Accuracy = 0.05255984142422676\n","Iteration 2830: Training Loss = 56.86114501953125 Training Accuracy = 0.05127491429448128\n","Iteration 2831: Training Loss = 56.921024322509766 Training Accuracy = 0.05293720215559006\n","Iteration 2832: Training Loss = 57.25320053100586 Training Accuracy = 0.051154591143131256\n","Iteration 2833: Training Loss = 56.985069274902344 Training Accuracy = 0.05307557433843613\n","Iteration 2834: Training Loss = 56.99213790893555 Training Accuracy = 0.051242388784885406\n","Iteration 2835: Training Loss = 56.65897750854492 Training Accuracy = 0.052689265459775925\n","Iteration 2836: Training Loss = 56.52461624145508 Training Accuracy = 0.05134768411517143\n","Iteration 2837: Training Loss = 56.381290435791016 Training Accuracy = 0.0519748292863369\n","Iteration 2838: Training Loss = 56.317359924316406 Training Accuracy = 0.051317665725946426\n","Iteration 2839: Training Loss = 56.2772216796875 Training Accuracy = 0.05159706622362137\n","Iteration 2840: Training Loss = 56.25963592529297 Training Accuracy = 0.05132501944899559\n","Iteration 2841: Training Loss = 56.252899169921875 Training Accuracy = 0.051554400473833084\n","Iteration 2842: Training Loss = 56.253841400146484 Training Accuracy = 0.05156545341014862\n","Iteration 2843: Training Loss = 56.25975799560547 Training Accuracy = 0.05156532675027847\n","Iteration 2844: Training Loss = 56.27234649658203 Training Accuracy = 0.05193694680929184\n","Iteration 2845: Training Loss = 56.304176330566406 Training Accuracy = 0.05154447257518768\n","Iteration 2846: Training Loss = 56.348426818847656 Training Accuracy = 0.05233626812696457\n","Iteration 2847: Training Loss = 56.45091247558594 Training Accuracy = 0.05152317136526108\n","Iteration 2848: Training Loss = 56.538963317871094 Training Accuracy = 0.05269508808851242\n","Iteration 2849: Training Loss = 56.7794075012207 Training Accuracy = 0.05139332637190819\n","Iteration 2850: Training Loss = 56.80495071411133 Training Accuracy = 0.05308333411812782\n","Iteration 2851: Training Loss = 57.069725036621094 Training Accuracy = 0.05130206048488617\n","Iteration 2852: Training Loss = 56.85258483886719 Training Accuracy = 0.05320459231734276\n","Iteration 2853: Training Loss = 56.880043029785156 Training Accuracy = 0.05145784467458725\n","Iteration 2854: Training Loss = 56.61264419555664 Training Accuracy = 0.052784863859415054\n","Iteration 2855: Training Loss = 56.51718521118164 Training Accuracy = 0.05161730572581291\n","Iteration 2856: Training Loss = 56.367820739746094 Training Accuracy = 0.05227723345160484\n","Iteration 2857: Training Loss = 56.299068450927734 Training Accuracy = 0.05160550773143768\n","Iteration 2858: Training Loss = 56.24605941772461 Training Accuracy = 0.05201764404773712\n","Iteration 2859: Training Loss = 56.22634506225586 Training Accuracy = 0.05149625986814499\n","Iteration 2860: Training Loss = 56.21611022949219 Training Accuracy = 0.05196668952703476\n","Iteration 2861: Training Loss = 56.21152877807617 Training Accuracy = 0.05159763991832733\n","Iteration 2862: Training Loss = 56.20408248901367 Training Accuracy = 0.05197691172361374\n","Iteration 2863: Training Loss = 56.19632339477539 Training Accuracy = 0.051874298602342606\n","Iteration 2864: Training Loss = 56.185951232910156 Training Accuracy = 0.05208363011479378\n","Iteration 2865: Training Loss = 56.17713928222656 Training Accuracy = 0.05206992104649544\n","Iteration 2866: Training Loss = 56.17146682739258 Training Accuracy = 0.05231266841292381\n","Iteration 2867: Training Loss = 56.169410705566406 Training Accuracy = 0.05220465734601021\n","Iteration 2868: Training Loss = 56.17047119140625 Training Accuracy = 0.052455950528383255\n","Iteration 2869: Training Loss = 56.174930572509766 Training Accuracy = 0.05236107483506203\n","Iteration 2870: Training Loss = 56.18384552001953 Training Accuracy = 0.05264509096741676\n","Iteration 2871: Training Loss = 56.206268310546875 Training Accuracy = 0.05246911197900772\n","Iteration 2872: Training Loss = 56.24510955810547 Training Accuracy = 0.052941836416721344\n","Iteration 2873: Training Loss = 56.34728240966797 Training Accuracy = 0.05240478366613388\n","Iteration 2874: Training Loss = 56.49054718017578 Training Accuracy = 0.053464245051145554\n","Iteration 2875: Training Loss = 56.893985748291016 Training Accuracy = 0.052024830132722855\n","Iteration 2876: Training Loss = 57.0716552734375 Training Accuracy = 0.05408648028969765\n","Iteration 2877: Training Loss = 57.7220573425293 Training Accuracy = 0.05164893716573715\n","Iteration 2878: Training Loss = 57.06244659423828 Training Accuracy = 0.05416959896683693\n","Iteration 2879: Training Loss = 56.865577697753906 Training Accuracy = 0.05201549828052521\n","Iteration 2880: Training Loss = 56.4381217956543 Training Accuracy = 0.05313337594270706\n","Iteration 2881: Training Loss = 56.26178741455078 Training Accuracy = 0.05217893421649933\n","Iteration 2882: Training Loss = 56.15939712524414 Training Accuracy = 0.05214717239141464\n","Iteration 2883: Training Loss = 56.127681732177734 Training Accuracy = 0.052099596709012985\n","Iteration 2884: Training Loss = 56.14509963989258 Training Accuracy = 0.05185038223862648\n","Iteration 2885: Training Loss = 56.20534896850586 Training Accuracy = 0.052220579236745834\n","Iteration 2886: Training Loss = 56.323486328125 Training Accuracy = 0.05174177139997482\n","Iteration 2887: Training Loss = 56.43277359008789 Training Accuracy = 0.052663031965494156\n","Iteration 2888: Training Loss = 56.67643356323242 Training Accuracy = 0.05153229087591171\n","Iteration 2889: Training Loss = 56.68593978881836 Training Accuracy = 0.05325299873948097\n","Iteration 2890: Training Loss = 56.86590576171875 Training Accuracy = 0.05145210400223732\n","Iteration 2891: Training Loss = 56.608001708984375 Training Accuracy = 0.053252507001161575\n","Iteration 2892: Training Loss = 56.522300720214844 Training Accuracy = 0.05169721692800522\n","Iteration 2893: Training Loss = 56.3028678894043 Training Accuracy = 0.05264627933502197\n","Iteration 2894: Training Loss = 56.19997024536133 Training Accuracy = 0.05182221531867981\n","Iteration 2895: Training Loss = 56.1258430480957 Training Accuracy = 0.05221811309456825\n","Iteration 2896: Training Loss = 56.09493637084961 Training Accuracy = 0.05191175639629364\n","Iteration 2897: Training Loss = 56.08769607543945 Training Accuracy = 0.05213180184364319\n","Iteration 2898: Training Loss = 56.0954704284668 Training Accuracy = 0.05223837122321129\n","Iteration 2899: Training Loss = 56.11887741088867 Training Accuracy = 0.05209638550877571\n","Iteration 2900: Training Loss = 56.154605865478516 Training Accuracy = 0.05268866941332817\n","Iteration 2901: Training Loss = 56.22548294067383 Training Accuracy = 0.05201292410492897\n","Iteration 2902: Training Loss = 56.2917366027832 Training Accuracy = 0.053054485470056534\n","Iteration 2903: Training Loss = 56.44704055786133 Training Accuracy = 0.0518888384103775\n","Iteration 2904: Training Loss = 56.49364471435547 Training Accuracy = 0.05329124256968498\n","Iteration 2905: Training Loss = 56.69771957397461 Training Accuracy = 0.05182113125920296\n","Iteration 2906: Training Loss = 56.590576171875 Training Accuracy = 0.05344776809215546\n","Iteration 2907: Training Loss = 56.666351318359375 Training Accuracy = 0.051812414079904556\n","Iteration 2908: Training Loss = 56.4692497253418 Training Accuracy = 0.0533207468688488\n","Iteration 2909: Training Loss = 56.42546844482422 Training Accuracy = 0.051981762051582336\n","Iteration 2910: Training Loss = 56.28046798706055 Training Accuracy = 0.052972447127103806\n","Iteration 2911: Training Loss = 56.22536849975586 Training Accuracy = 0.05209806561470032\n","Iteration 2912: Training Loss = 56.15409851074219 Training Accuracy = 0.0527225136756897\n","Iteration 2913: Training Loss = 56.12620544433594 Training Accuracy = 0.05211498588323593\n","Iteration 2914: Training Loss = 56.09916687011719 Training Accuracy = 0.05269034951925278\n","Iteration 2915: Training Loss = 56.09242248535156 Training Accuracy = 0.052105631679296494\n","Iteration 2916: Training Loss = 56.08481216430664 Training Accuracy = 0.052756164222955704\n","Iteration 2917: Training Loss = 56.08997344970703 Training Accuracy = 0.05220697447657585\n","Iteration 2918: Training Loss = 56.091983795166016 Training Accuracy = 0.052847087383270264\n","Iteration 2919: Training Loss = 56.112911224365234 Training Accuracy = 0.05231156572699547\n","Iteration 2920: Training Loss = 56.13006591796875 Training Accuracy = 0.05305369943380356\n","Iteration 2921: Training Loss = 56.18951416015625 Training Accuracy = 0.05231520161032677\n","Iteration 2922: Training Loss = 56.23910903930664 Training Accuracy = 0.053375255316495895\n","Iteration 2923: Training Loss = 56.39131546020508 Training Accuracy = 0.05223109945654869\n","Iteration 2924: Training Loss = 56.46027755737305 Training Accuracy = 0.053699299693107605\n","Iteration 2925: Training Loss = 56.725826263427734 Training Accuracy = 0.0521463006734848\n","Iteration 2926: Training Loss = 56.65467834472656 Training Accuracy = 0.05392778664827347\n","Iteration 2927: Training Loss = 56.81727600097656 Training Accuracy = 0.05215068161487579\n","Iteration 2928: Training Loss = 56.53363037109375 Training Accuracy = 0.05378756299614906\n","Iteration 2929: Training Loss = 56.45895767211914 Training Accuracy = 0.05224670469760895\n","Iteration 2930: Training Loss = 56.243934631347656 Training Accuracy = 0.0532991923391819\n","Iteration 2931: Training Loss = 56.15303039550781 Training Accuracy = 0.05223318561911583\n","Iteration 2932: Training Loss = 56.06761169433594 Training Accuracy = 0.05281447619199753\n","Iteration 2933: Training Loss = 56.02922439575195 Training Accuracy = 0.052236225455999374\n","Iteration 2934: Training Loss = 56.00354766845703 Training Accuracy = 0.052621323615312576\n","Iteration 2935: Training Loss = 55.99074935913086 Training Accuracy = 0.05234915018081665\n","Iteration 2936: Training Loss = 55.982322692871094 Training Accuracy = 0.05265820398926735\n","Iteration 2937: Training Loss = 55.9764404296875 Training Accuracy = 0.05261228606104851\n","Iteration 2938: Training Loss = 55.972877502441406 Training Accuracy = 0.0527372881770134\n","Iteration 2939: Training Loss = 55.971805572509766 Training Accuracy = 0.0528884157538414\n","Iteration 2940: Training Loss = 55.97523498535156 Training Accuracy = 0.05278743803501129\n","Iteration 2941: Training Loss = 55.984493255615234 Training Accuracy = 0.05316188186407089\n","Iteration 2942: Training Loss = 56.00858688354492 Training Accuracy = 0.05277489498257637\n","Iteration 2943: Training Loss = 56.04629898071289 Training Accuracy = 0.053461648523807526\n","Iteration 2944: Training Loss = 56.13750457763672 Training Accuracy = 0.05270759016275406\n","Iteration 2945: Training Loss = 56.238704681396484 Training Accuracy = 0.053862202912569046\n","Iteration 2946: Training Loss = 56.509037017822266 Training Accuracy = 0.05251900479197502\n","Iteration 2947: Training Loss = 56.607749938964844 Training Accuracy = 0.054339583963155746\n","Iteration 2948: Training Loss = 57.01708221435547 Training Accuracy = 0.052321769297122955\n","Iteration 2949: Training Loss = 56.74287796020508 Training Accuracy = 0.05450727045536041\n","Iteration 2950: Training Loss = 56.78624725341797 Training Accuracy = 0.052483800798654556\n","Iteration 2951: Training Loss = 56.400081634521484 Training Accuracy = 0.05392655357718468\n","Iteration 2952: Training Loss = 56.24274444580078 Training Accuracy = 0.052626870572566986\n","Iteration 2953: Training Loss = 56.062076568603516 Training Accuracy = 0.053113583475351334\n","Iteration 2954: Training Loss = 55.97893142700195 Training Accuracy = 0.052580591291189194\n","Iteration 2955: Training Loss = 55.933719635009766 Training Accuracy = 0.052765391767024994\n","Iteration 2956: Training Loss = 55.923824310302734 Training Accuracy = 0.05253029242157936\n","Iteration 2957: Training Loss = 55.93240737915039 Training Accuracy = 0.052765220403671265\n","Iteration 2958: Training Loss = 55.94740676879883 Training Accuracy = 0.05267893150448799\n","Iteration 2959: Training Loss = 55.969242095947266 Training Accuracy = 0.0526873953640461\n","Iteration 2960: Training Loss = 55.992225646972656 Training Accuracy = 0.05312138423323631\n","Iteration 2961: Training Loss = 56.044273376464844 Training Accuracy = 0.05258261039853096\n","Iteration 2962: Training Loss = 56.096466064453125 Training Accuracy = 0.053536415100097656\n","Iteration 2963: Training Loss = 56.23796844482422 Training Accuracy = 0.052476637065410614\n","Iteration 2964: Training Loss = 56.30841827392578 Training Accuracy = 0.05389999970793724\n","Iteration 2965: Training Loss = 56.54914855957031 Training Accuracy = 0.052309565246105194\n","Iteration 2966: Training Loss = 56.4542350769043 Training Accuracy = 0.05411073565483093\n","Iteration 2967: Training Loss = 56.55436325073242 Training Accuracy = 0.052297789603471756\n","Iteration 2968: Training Loss = 56.32755661010742 Training Accuracy = 0.053928229957818985\n","Iteration 2969: Training Loss = 56.28661346435547 Training Accuracy = 0.05243539437651634\n","Iteration 2970: Training Loss = 56.13261795043945 Training Accuracy = 0.053466156125068665\n","Iteration 2971: Training Loss = 56.0744514465332 Training Accuracy = 0.05259145423769951\n","Iteration 2972: Training Loss = 55.98931121826172 Training Accuracy = 0.05311458185315132\n","Iteration 2973: Training Loss = 55.94683074951172 Training Accuracy = 0.052637286484241486\n","Iteration 2974: Training Loss = 55.90837860107422 Training Accuracy = 0.05301352217793465\n","Iteration 2975: Training Loss = 55.8945198059082 Training Accuracy = 0.05255807936191559\n","Iteration 2976: Training Loss = 55.888126373291016 Training Accuracy = 0.053106993436813354\n","Iteration 2977: Training Loss = 55.891239166259766 Training Accuracy = 0.0525871179997921\n","Iteration 2978: Training Loss = 55.89191818237305 Training Accuracy = 0.05320076644420624\n","Iteration 2979: Training Loss = 55.899208068847656 Training Accuracy = 0.05276554077863693\n","Iteration 2980: Training Loss = 55.90181350708008 Training Accuracy = 0.05334613099694252\n","Iteration 2981: Training Loss = 55.92230987548828 Training Accuracy = 0.052856121212244034\n","Iteration 2982: Training Loss = 55.94662094116211 Training Accuracy = 0.05361105501651764\n","Iteration 2983: Training Loss = 56.02503204345703 Training Accuracy = 0.05282127857208252\n","Iteration 2984: Training Loss = 56.106815338134766 Training Accuracy = 0.05400082841515541\n","Iteration 2985: Training Loss = 56.33263397216797 Training Accuracy = 0.052701447159051895\n","Iteration 2986: Training Loss = 56.4273796081543 Training Accuracy = 0.0543815903365612\n","Iteration 2987: Training Loss = 56.78580093383789 Training Accuracy = 0.052592985332012177\n","Iteration 2988: Training Loss = 56.572025299072266 Training Accuracy = 0.054580315947532654\n","Iteration 2989: Training Loss = 56.63349151611328 Training Accuracy = 0.052623022347688675\n","Iteration 2990: Training Loss = 56.27049255371094 Training Accuracy = 0.05422423407435417\n","Iteration 2991: Training Loss = 56.129024505615234 Training Accuracy = 0.05268741399049759\n","Iteration 2992: Training Loss = 55.955326080322266 Training Accuracy = 0.05351024493575096\n","Iteration 2993: Training Loss = 55.87771987915039 Training Accuracy = 0.052706822752952576\n","Iteration 2994: Training Loss = 55.828617095947266 Training Accuracy = 0.053006887435913086\n","Iteration 2995: Training Loss = 55.807655334472656 Training Accuracy = 0.05278582125902176\n","Iteration 2996: Training Loss = 55.80099868774414 Training Accuracy = 0.05293762683868408\n","Iteration 2997: Training Loss = 55.804542541503906 Training Accuracy = 0.05300191417336464\n","Iteration 2998: Training Loss = 55.817840576171875 Training Accuracy = 0.05296870693564415\n","Iteration 2999: Training Loss = 55.8401985168457 Training Accuracy = 0.05338858440518379\n","Iteration 3000: Training Loss = 55.88787078857422 Training Accuracy = 0.05294755473732948\n","Iteration 3001: Training Loss = 55.943790435791016 Training Accuracy = 0.05382025986909866\n","Iteration 3002: Training Loss = 56.07056427001953 Training Accuracy = 0.05281154438853264\n","Iteration 3003: Training Loss = 56.149349212646484 Training Accuracy = 0.054231591522693634\n","Iteration 3004: Training Loss = 56.38161087036133 Training Accuracy = 0.05269351601600647\n","Iteration 3005: Training Loss = 56.35322189331055 Training Accuracy = 0.05446915328502655\n","Iteration 3006: Training Loss = 56.53453826904297 Training Accuracy = 0.05268154665827751\n","Iteration 3007: Training Loss = 56.3140983581543 Training Accuracy = 0.0544169656932354\n","Iteration 3008: Training Loss = 56.298465728759766 Training Accuracy = 0.05280512198805809\n","Iteration 3009: Training Loss = 56.08673858642578 Training Accuracy = 0.05403073877096176\n","Iteration 3010: Training Loss = 56.00495910644531 Training Accuracy = 0.052912838757038116\n","Iteration 3011: Training Loss = 55.892539978027344 Training Accuracy = 0.05357179045677185\n","Iteration 3012: Training Loss = 55.84056854248047 Training Accuracy = 0.05295291170477867\n","Iteration 3013: Training Loss = 55.79774856567383 Training Accuracy = 0.05335954576730728\n","Iteration 3014: Training Loss = 55.77864074707031 Training Accuracy = 0.052935436367988586\n","Iteration 3015: Training Loss = 55.76682662963867 Training Accuracy = 0.05338197201490402\n","Iteration 3016: Training Loss = 55.76129913330078 Training Accuracy = 0.05300208181142807\n","Iteration 3017: Training Loss = 55.754940032958984 Training Accuracy = 0.0534675158560276\n","Iteration 3018: Training Loss = 55.74910354614258 Training Accuracy = 0.05322499945759773\n","Iteration 3019: Training Loss = 55.742713928222656 Training Accuracy = 0.0535561665892601\n","Iteration 3020: Training Loss = 55.73800277709961 Training Accuracy = 0.05344534292817116\n","Iteration 3021: Training Loss = 55.73455047607422 Training Accuracy = 0.05370986461639404\n","Iteration 3022: Training Loss = 55.735801696777344 Training Accuracy = 0.05355769395828247\n","Iteration 3023: Training Loss = 55.743011474609375 Training Accuracy = 0.053943876177072525\n","Iteration 3024: Training Loss = 55.76463317871094 Training Accuracy = 0.05360979959368706\n","Iteration 3025: Training Loss = 55.80262756347656 Training Accuracy = 0.054263606667518616\n","Iteration 3026: Training Loss = 55.896732330322266 Training Accuracy = 0.053570281714200974\n","Iteration 3027: Training Loss = 56.019771575927734 Training Accuracy = 0.05467034503817558\n","Iteration 3028: Training Loss = 56.34747314453125 Training Accuracy = 0.0533546544611454\n","Iteration 3029: Training Loss = 56.499671936035156 Training Accuracy = 0.05522079020738602\n","Iteration 3030: Training Loss = 57.04413986206055 Training Accuracy = 0.05298979580402374\n","Iteration 3031: Training Loss = 56.60551834106445 Training Accuracy = 0.05542249232530594\n","Iteration 3032: Training Loss = 56.5426139831543 Training Accuracy = 0.0531231090426445\n","Iteration 3033: Training Loss = 56.09574890136719 Training Accuracy = 0.0546385832130909\n","Iteration 3034: Training Loss = 55.90921401977539 Training Accuracy = 0.05330824851989746\n","Iteration 3035: Training Loss = 55.76670455932617 Training Accuracy = 0.05358894541859627\n","Iteration 3036: Training Loss = 55.705867767333984 Training Accuracy = 0.053315624594688416\n","Iteration 3037: Training Loss = 55.683082580566406 Training Accuracy = 0.05321664363145828\n","Iteration 3038: Training Loss = 55.69203186035156 Training Accuracy = 0.053382378071546555\n","Iteration 3039: Training Loss = 55.72856140136719 Training Accuracy = 0.05326411500573158\n","Iteration 3040: Training Loss = 55.781166076660156 Training Accuracy = 0.053629420697689056\n","Iteration 3041: Training Loss = 55.87920379638672 Training Accuracy = 0.053156737238168716\n","Iteration 3042: Training Loss = 55.95742416381836 Training Accuracy = 0.054160311818122864\n","Iteration 3043: Training Loss = 56.148433685302734 Training Accuracy = 0.05290348455309868\n","Iteration 3044: Training Loss = 56.16964340209961 Training Accuracy = 0.05462959036231041\n","Iteration 3045: Training Loss = 56.34402084350586 Training Accuracy = 0.05285465717315674\n","Iteration 3046: Training Loss = 56.15618896484375 Training Accuracy = 0.054610393941402435\n","Iteration 3047: Training Loss = 56.133548736572266 Training Accuracy = 0.053016453981399536\n","Iteration 3048: Training Loss = 55.92848205566406 Training Accuracy = 0.054201383143663406\n","Iteration 3049: Training Loss = 55.843570709228516 Training Accuracy = 0.05315780267119408\n","Iteration 3050: Training Loss = 55.74701690673828 Training Accuracy = 0.053875043988227844\n","Iteration 3051: Training Loss = 55.70109939575195 Training Accuracy = 0.05323509871959686\n","Iteration 3052: Training Loss = 55.666866302490234 Training Accuracy = 0.05373711884021759\n","Iteration 3053: Training Loss = 55.647823333740234 Training Accuracy = 0.05344921350479126\n","Iteration 3054: Training Loss = 55.63507843017578 Training Accuracy = 0.0537126287817955\n","Iteration 3055: Training Loss = 55.627567291259766 Training Accuracy = 0.053656142204999924\n","Iteration 3056: Training Loss = 55.62268829345703 Training Accuracy = 0.05372140556573868\n","Iteration 3057: Training Loss = 55.61915588378906 Training Accuracy = 0.05380928888916969\n","Iteration 3058: Training Loss = 55.61651611328125 Training Accuracy = 0.053861819207668304\n","Iteration 3059: Training Loss = 55.614959716796875 Training Accuracy = 0.05396830290555954\n","Iteration 3060: Training Loss = 55.61465835571289 Training Accuracy = 0.05401511490345001\n","Iteration 3061: Training Loss = 55.616477966308594 Training Accuracy = 0.054193176329135895\n","Iteration 3062: Training Loss = 55.62334060668945 Training Accuracy = 0.05411428585648537\n","Iteration 3063: Training Loss = 55.63833999633789 Training Accuracy = 0.05449517443776131\n","Iteration 3064: Training Loss = 55.677757263183594 Training Accuracy = 0.05408390611410141\n","Iteration 3065: Training Loss = 55.744232177734375 Training Accuracy = 0.05491020530462265\n","Iteration 3066: Training Loss = 55.91520309448242 Training Accuracy = 0.05392387509346008\n","Iteration 3067: Training Loss = 56.09954071044922 Training Accuracy = 0.05544942617416382\n","Iteration 3068: Training Loss = 56.60993194580078 Training Accuracy = 0.05359179526567459\n","Iteration 3069: Training Loss = 56.574459075927734 Training Accuracy = 0.05594661831855774\n","Iteration 3070: Training Loss = 56.941139221191406 Training Accuracy = 0.053437329828739166\n","Iteration 3071: Training Loss = 56.332176208496094 Training Accuracy = 0.055672746151685715\n","Iteration 3072: Training Loss = 56.111175537109375 Training Accuracy = 0.053741879761219025\n","Iteration 3073: Training Loss = 55.796958923339844 Training Accuracy = 0.05456947162747383\n","Iteration 3074: Training Loss = 55.6623649597168 Training Accuracy = 0.05380963161587715\n","Iteration 3075: Training Loss = 55.59265899658203 Training Accuracy = 0.05369621515274048\n","Iteration 3076: Training Loss = 55.57569122314453 Training Accuracy = 0.053892072290182114\n","Iteration 3077: Training Loss = 55.59827423095703 Training Accuracy = 0.053551848977804184\n","Iteration 3078: Training Loss = 55.65607833862305 Training Accuracy = 0.053988903760910034\n","Iteration 3079: Training Loss = 55.76669692993164 Training Accuracy = 0.05352504178881645\n","Iteration 3080: Training Loss = 55.860538482666016 Training Accuracy = 0.054348744451999664\n","Iteration 3081: Training Loss = 56.06497573852539 Training Accuracy = 0.05334334447979927\n","Iteration 3082: Training Loss = 56.07382583618164 Training Accuracy = 0.05491923913359642\n","Iteration 3083: Training Loss = 56.23702621459961 Training Accuracy = 0.05317491665482521\n","Iteration 3084: Training Loss = 56.04317855834961 Training Accuracy = 0.05503207817673683\n","Iteration 3085: Training Loss = 56.02252197265625 Training Accuracy = 0.05327010899782181\n","Iteration 3086: Training Loss = 55.85610580444336 Training Accuracy = 0.054532311856746674\n","Iteration 3087: Training Loss = 55.80692672729492 Training Accuracy = 0.05350291356444359\n","Iteration 3088: Training Loss = 55.697330474853516 Training Accuracy = 0.05412423610687256\n","Iteration 3089: Training Loss = 55.646873474121094 Training Accuracy = 0.05336611345410347\n","Iteration 3090: Training Loss = 55.598575592041016 Training Accuracy = 0.05407803878188133\n","Iteration 3091: Training Loss = 55.573570251464844 Training Accuracy = 0.053634416311979294\n","Iteration 3092: Training Loss = 55.55521774291992 Training Accuracy = 0.05419902130961418\n","Iteration 3093: Training Loss = 55.5415153503418 Training Accuracy = 0.05380529165267944\n","Iteration 3094: Training Loss = 55.530887603759766 Training Accuracy = 0.054038096219301224\n","Iteration 3095: Training Loss = 55.527137756347656 Training Accuracy = 0.05398992449045181\n","Iteration 3096: Training Loss = 55.52376937866211 Training Accuracy = 0.05410616472363472\n","Iteration 3097: Training Loss = 55.518157958984375 Training Accuracy = 0.05409944802522659\n","Iteration 3098: Training Loss = 55.5161018371582 Training Accuracy = 0.05431636795401573\n","Iteration 3099: Training Loss = 55.51527786254883 Training Accuracy = 0.05421288311481476\n","Iteration 3100: Training Loss = 55.51636505126953 Training Accuracy = 0.054625317454338074\n","Iteration 3101: Training Loss = 55.52203369140625 Training Accuracy = 0.0542469397187233\n","Iteration 3102: Training Loss = 55.5335693359375 Training Accuracy = 0.05489311367273331\n","Iteration 3103: Training Loss = 55.56604766845703 Training Accuracy = 0.05438986048102379\n","Iteration 3104: Training Loss = 55.61900329589844 Training Accuracy = 0.05514292046427727\n","Iteration 3105: Training Loss = 55.75886917114258 Training Accuracy = 0.0542832687497139\n","Iteration 3106: Training Loss = 55.904720306396484 Training Accuracy = 0.05559683218598366\n","Iteration 3107: Training Loss = 56.32122039794922 Training Accuracy = 0.05391819775104523\n","Iteration 3108: Training Loss = 56.368263244628906 Training Accuracy = 0.056157186627388\n","Iteration 3109: Training Loss = 56.79587936401367 Training Accuracy = 0.0536457896232605\n","Iteration 3110: Training Loss = 56.29559326171875 Training Accuracy = 0.05601741001009941\n","Iteration 3111: Training Loss = 56.16331481933594 Training Accuracy = 0.054012052714824677\n","Iteration 3112: Training Loss = 55.790191650390625 Training Accuracy = 0.055005546659231186\n","Iteration 3113: Training Loss = 55.630191802978516 Training Accuracy = 0.054061371833086014\n","Iteration 3114: Training Loss = 55.516117095947266 Training Accuracy = 0.05411260575056076\n","Iteration 3115: Training Loss = 55.46763229370117 Training Accuracy = 0.05406360700726509\n","Iteration 3116: Training Loss = 55.45950698852539 Training Accuracy = 0.0539451539516449\n","Iteration 3117: Training Loss = 55.4840087890625 Training Accuracy = 0.05401315912604332\n","Iteration 3118: Training Loss = 55.537445068359375 Training Accuracy = 0.05395688861608505\n","Iteration 3119: Training Loss = 55.59674072265625 Training Accuracy = 0.05432123690843582\n","Iteration 3120: Training Loss = 55.70618438720703 Training Accuracy = 0.05382068455219269\n","Iteration 3121: Training Loss = 55.771549224853516 Training Accuracy = 0.054893430322408676\n","Iteration 3122: Training Loss = 55.951663970947266 Training Accuracy = 0.05356558412313461\n","Iteration 3123: Training Loss = 55.932647705078125 Training Accuracy = 0.05531319975852966\n","Iteration 3124: Training Loss = 56.07802963256836 Training Accuracy = 0.05349930003285408\n","Iteration 3125: Training Loss = 55.88608932495117 Training Accuracy = 0.05527704209089279\n","Iteration 3126: Training Loss = 55.850975036621094 Training Accuracy = 0.05368201434612274\n","Iteration 3127: Training Loss = 55.66725158691406 Training Accuracy = 0.05482587218284607\n","Iteration 3128: Training Loss = 55.58694839477539 Training Accuracy = 0.05380014702677727\n","Iteration 3129: Training Loss = 55.5020637512207 Training Accuracy = 0.05447935685515404\n","Iteration 3130: Training Loss = 55.45891189575195 Training Accuracy = 0.053879909217357635\n","Iteration 3131: Training Loss = 55.4317626953125 Training Accuracy = 0.05439045652747154\n","Iteration 3132: Training Loss = 55.419612884521484 Training Accuracy = 0.05413560941815376\n","Iteration 3133: Training Loss = 55.406776428222656 Training Accuracy = 0.05435909703373909\n","Iteration 3134: Training Loss = 55.40030288696289 Training Accuracy = 0.054357439279556274\n","Iteration 3135: Training Loss = 55.39672088623047 Training Accuracy = 0.054344046860933304\n","Iteration 3136: Training Loss = 55.393001556396484 Training Accuracy = 0.054517049342393875\n","Iteration 3137: Training Loss = 55.38711166381836 Training Accuracy = 0.054462775588035583\n","Iteration 3138: Training Loss = 55.38555145263672 Training Accuracy = 0.05465446412563324\n","Iteration 3139: Training Loss = 55.3856315612793 Training Accuracy = 0.05465325340628624\n","Iteration 3140: Training Loss = 55.386253356933594 Training Accuracy = 0.05487597733736038\n","Iteration 3141: Training Loss = 55.393890380859375 Training Accuracy = 0.05476462468504906\n","Iteration 3142: Training Loss = 55.411170959472656 Training Accuracy = 0.055165454745292664\n","Iteration 3143: Training Loss = 55.455501556396484 Training Accuracy = 0.05468475818634033\n","Iteration 3144: Training Loss = 55.52998352050781 Training Accuracy = 0.055593304336071014\n","Iteration 3145: Training Loss = 55.72425079345703 Training Accuracy = 0.05450816452503204\n","Iteration 3146: Training Loss = 55.921966552734375 Training Accuracy = 0.056176744401454926\n","Iteration 3147: Training Loss = 56.462894439697266 Training Accuracy = 0.05417204648256302\n","Iteration 3148: Training Loss = 56.36842727661133 Training Accuracy = 0.056588031351566315\n","Iteration 3149: Training Loss = 56.67083740234375 Training Accuracy = 0.05407561734318733\n","Iteration 3150: Training Loss = 56.05803298950195 Training Accuracy = 0.05620935559272766\n","Iteration 3151: Training Loss = 55.82497787475586 Training Accuracy = 0.05432381108403206\n","Iteration 3152: Training Loss = 55.53985595703125 Training Accuracy = 0.05504870414733887\n","Iteration 3153: Training Loss = 55.4175910949707 Training Accuracy = 0.05436592176556587\n","Iteration 3154: Training Loss = 55.358665466308594 Training Accuracy = 0.05428735166788101\n","Iteration 3155: Training Loss = 55.348663330078125 Training Accuracy = 0.0544772744178772\n","Iteration 3156: Training Loss = 55.37510681152344 Training Accuracy = 0.054080549627542496\n","Iteration 3157: Training Loss = 55.432865142822266 Training Accuracy = 0.054595980793237686\n","Iteration 3158: Training Loss = 55.54515075683594 Training Accuracy = 0.054058801382780075\n","Iteration 3159: Training Loss = 55.63937759399414 Training Accuracy = 0.05497153475880623\n","Iteration 3160: Training Loss = 55.846588134765625 Training Accuracy = 0.05392115190625191\n","Iteration 3161: Training Loss = 55.84352493286133 Training Accuracy = 0.05550437793135643\n","Iteration 3162: Training Loss = 55.99154281616211 Training Accuracy = 0.053758587688207626\n","Iteration 3163: Training Loss = 55.797245025634766 Training Accuracy = 0.05559011548757553\n","Iteration 3164: Training Loss = 55.7412109375 Training Accuracy = 0.05386590212583542\n","Iteration 3165: Training Loss = 55.55086135864258 Training Accuracy = 0.05508733168244362\n","Iteration 3166: Training Loss = 55.46631622314453 Training Accuracy = 0.05405248701572418\n","Iteration 3167: Training Loss = 55.383949279785156 Training Accuracy = 0.05457538366317749\n","Iteration 3168: Training Loss = 55.34259033203125 Training Accuracy = 0.05421740934252739\n","Iteration 3169: Training Loss = 55.317054748535156 Training Accuracy = 0.05454530194401741\n","Iteration 3170: Training Loss = 55.306251525878906 Training Accuracy = 0.05438307672739029\n","Iteration 3171: Training Loss = 55.303340911865234 Training Accuracy = 0.05463365092873573\n","Iteration 3172: Training Loss = 55.304691314697266 Training Accuracy = 0.05461413785815239\n","Iteration 3173: Training Loss = 55.311309814453125 Training Accuracy = 0.054557375609874725\n","Iteration 3174: Training Loss = 55.325721740722656 Training Accuracy = 0.054934438318014145\n","Iteration 3175: Training Loss = 55.3576774597168 Training Accuracy = 0.05443039909005165\n","Iteration 3176: Training Loss = 55.40132141113281 Training Accuracy = 0.05530518665909767\n","Iteration 3177: Training Loss = 55.49679183959961 Training Accuracy = 0.05435888469219208\n","Iteration 3178: Training Loss = 55.58515167236328 Training Accuracy = 0.05568133667111397\n","Iteration 3179: Training Loss = 55.816097259521484 Training Accuracy = 0.05429213494062424\n","Iteration 3180: Training Loss = 55.860225677490234 Training Accuracy = 0.056027211248874664\n","Iteration 3181: Training Loss = 56.13651657104492 Training Accuracy = 0.054168663918972015\n","Iteration 3182: Training Loss = 55.92704772949219 Training Accuracy = 0.05612563714385033\n","Iteration 3183: Training Loss = 55.96058654785156 Training Accuracy = 0.05421600863337517\n","Iteration 3184: Training Loss = 55.691246032714844 Training Accuracy = 0.05573790520429611\n","Iteration 3185: Training Loss = 55.59436798095703 Training Accuracy = 0.054382652044296265\n","Iteration 3186: Training Loss = 55.44170379638672 Training Accuracy = 0.055115729570388794\n","Iteration 3187: Training Loss = 55.37157440185547 Training Accuracy = 0.054468028247356415\n","Iteration 3188: Training Loss = 55.31247329711914 Training Accuracy = 0.05472327768802643\n","Iteration 3189: Training Loss = 55.28287124633789 Training Accuracy = 0.05445316806435585\n","Iteration 3190: Training Loss = 55.26656723022461 Training Accuracy = 0.054712627083063126\n","Iteration 3191: Training Loss = 55.261802673339844 Training Accuracy = 0.054435182362794876\n","Iteration 3192: Training Loss = 55.261775970458984 Training Accuracy = 0.05489173159003258\n","Iteration 3193: Training Loss = 55.262813568115234 Training Accuracy = 0.05459119752049446\n","Iteration 3194: Training Loss = 55.26279067993164 Training Accuracy = 0.054993048310279846\n","Iteration 3195: Training Loss = 55.262474060058594 Training Accuracy = 0.05484657734632492\n","Iteration 3196: Training Loss = 55.261314392089844 Training Accuracy = 0.05504647269845009\n","Iteration 3197: Training Loss = 55.25811004638672 Training Accuracy = 0.05506575107574463\n","Iteration 3198: Training Loss = 55.253238677978516 Training Accuracy = 0.055157460272312164\n","Iteration 3199: Training Loss = 55.24778366088867 Training Accuracy = 0.05524693801999092\n","Iteration 3200: Training Loss = 55.24628448486328 Training Accuracy = 0.05529234558343887\n","Iteration 3201: Training Loss = 55.249881744384766 Training Accuracy = 0.055450424551963806\n","Iteration 3202: Training Loss = 55.26770782470703 Training Accuracy = 0.05532259866595268\n","Iteration 3203: Training Loss = 55.30414962768555 Training Accuracy = 0.05579474940896034\n","Iteration 3204: Training Loss = 55.405208587646484 Training Accuracy = 0.055112436413764954\n","Iteration 3205: Training Loss = 55.55046844482422 Training Accuracy = 0.056351784616708755\n","Iteration 3206: Training Loss = 55.9373664855957 Training Accuracy = 0.054686736315488815\n","Iteration 3207: Training Loss = 56.09992218017578 Training Accuracy = 0.056942496448755264\n","Iteration 3208: Training Loss = 56.67402267456055 Training Accuracy = 0.05436037480831146\n","Iteration 3209: Training Loss = 56.12316131591797 Training Accuracy = 0.05692523345351219\n","Iteration 3210: Training Loss = 55.99229049682617 Training Accuracy = 0.05469325929880142\n","Iteration 3211: Training Loss = 55.5744743347168 Training Accuracy = 0.05585592985153198\n","Iteration 3212: Training Loss = 55.40012741088867 Training Accuracy = 0.0548299103975296\n","Iteration 3213: Training Loss = 55.27075958251953 Training Accuracy = 0.054842475801706314\n","Iteration 3214: Training Loss = 55.215885162353516 Training Accuracy = 0.05486747622489929\n","Iteration 3215: Training Loss = 55.204872131347656 Training Accuracy = 0.054529231041669846\n","Iteration 3216: Training Loss = 55.23535919189453 Training Accuracy = 0.054929208010435104\n","Iteration 3217: Training Loss = 55.312225341796875 Training Accuracy = 0.05448618158698082\n","Iteration 3218: Training Loss = 55.40435028076172 Training Accuracy = 0.05517859384417534\n","Iteration 3219: Training Loss = 55.58730697631836 Training Accuracy = 0.054383162409067154\n","Iteration 3220: Training Loss = 55.64826202392578 Training Accuracy = 0.05573514103889465\n","Iteration 3221: Training Loss = 55.85601806640625 Training Accuracy = 0.05414627864956856\n","Iteration 3222: Training Loss = 55.722015380859375 Training Accuracy = 0.05601096898317337\n","Iteration 3223: Training Loss = 55.741973876953125 Training Accuracy = 0.05418173968791962\n","Iteration 3224: Training Loss = 55.51005172729492 Training Accuracy = 0.05568709596991539\n","Iteration 3225: Training Loss = 55.41132354736328 Training Accuracy = 0.05431775003671646\n","Iteration 3226: Training Loss = 55.28679275512695 Training Accuracy = 0.05511840805411339\n","Iteration 3227: Training Loss = 55.22692108154297 Training Accuracy = 0.05447682738304138\n","Iteration 3228: Training Loss = 55.18540573120117 Training Accuracy = 0.05491315945982933\n","Iteration 3229: Training Loss = 55.166500091552734 Training Accuracy = 0.05468945577740669\n","Iteration 3230: Training Loss = 55.1601676940918 Training Accuracy = 0.054957952350378036\n","Iteration 3231: Training Loss = 55.1609992980957 Training Accuracy = 0.05492772161960602\n","Iteration 3232: Training Loss = 55.168251037597656 Training Accuracy = 0.05491343513131142\n","Iteration 3233: Training Loss = 55.182498931884766 Training Accuracy = 0.0552234910428524\n","Iteration 3234: Training Loss = 55.21310043334961 Training Accuracy = 0.054781846702098846\n","Iteration 3235: Training Loss = 55.25867462158203 Training Accuracy = 0.05564780905842781\n","Iteration 3236: Training Loss = 55.36040115356445 Training Accuracy = 0.054720111191272736\n","Iteration 3237: Training Loss = 55.45166015625 Training Accuracy = 0.05602128058671951\n","Iteration 3238: Training Loss = 55.68045425415039 Training Accuracy = 0.05462215095758438\n","Iteration 3239: Training Loss = 55.70457077026367 Training Accuracy = 0.05636892095208168\n","Iteration 3240: Training Loss = 55.939476013183594 Training Accuracy = 0.05449745059013367\n","Iteration 3241: Training Loss = 55.73361587524414 Training Accuracy = 0.05642714723944664\n","Iteration 3242: Training Loss = 55.74687194824219 Training Accuracy = 0.05458386614918709\n","Iteration 3243: Training Loss = 55.50829315185547 Training Accuracy = 0.05600937455892563\n","Iteration 3244: Training Loss = 55.42206573486328 Training Accuracy = 0.05474500358104706\n","Iteration 3245: Training Loss = 55.2879638671875 Training Accuracy = 0.055449895560741425\n","Iteration 3246: Training Loss = 55.22592544555664 Training Accuracy = 0.0548526793718338\n","Iteration 3247: Training Loss = 55.17155838012695 Training Accuracy = 0.05511981248855591\n","Iteration 3248: Training Loss = 55.14442825317383 Training Accuracy = 0.054861053824424744\n","Iteration 3249: Training Loss = 55.128665924072266 Training Accuracy = 0.055189263075590134\n","Iteration 3250: Training Loss = 55.12455368041992 Training Accuracy = 0.05483473464846611\n","Iteration 3251: Training Loss = 55.12394332885742 Training Accuracy = 0.055354125797748566\n","Iteration 3252: Training Loss = 55.122928619384766 Training Accuracy = 0.054980549961328506\n","Iteration 3253: Training Loss = 55.119895935058594 Training Accuracy = 0.055470239371061325\n","Iteration 3254: Training Loss = 55.116458892822266 Training Accuracy = 0.05524090304970741\n","Iteration 3255: Training Loss = 55.11200714111328 Training Accuracy = 0.05552978441119194\n","Iteration 3256: Training Loss = 55.107643127441406 Training Accuracy = 0.05544266477227211\n","Iteration 3257: Training Loss = 55.103240966796875 Training Accuracy = 0.05572096258401871\n","Iteration 3258: Training Loss = 55.10315704345703 Training Accuracy = 0.05552087724208832\n","Iteration 3259: Training Loss = 55.10935592651367 Training Accuracy = 0.05597376823425293\n","Iteration 3260: Training Loss = 55.13139343261719 Training Accuracy = 0.05550401657819748\n","Iteration 3261: Training Loss = 55.173423767089844 Training Accuracy = 0.05630754679441452\n","Iteration 3262: Training Loss = 55.279659271240234 Training Accuracy = 0.055413030087947845\n","Iteration 3263: Training Loss = 55.42353820800781 Training Accuracy = 0.05671092867851257\n","Iteration 3264: Training Loss = 55.804298400878906 Training Accuracy = 0.055155932903289795\n","Iteration 3265: Training Loss = 55.94525146484375 Training Accuracy = 0.057213690131902695\n","Iteration 3266: Training Loss = 56.4919548034668 Training Accuracy = 0.05482582747936249\n","Iteration 3267: Training Loss = 55.97395706176758 Training Accuracy = 0.05729496106505394\n","Iteration 3268: Training Loss = 55.83879089355469 Training Accuracy = 0.05499019846320152\n","Iteration 3269: Training Loss = 55.41306686401367 Training Accuracy = 0.05634961649775505\n","Iteration 3270: Training Loss = 55.23006820678711 Training Accuracy = 0.055134885013103485\n","Iteration 3271: Training Loss = 55.111793518066406 Training Accuracy = 0.05527178943157196\n","Iteration 3272: Training Loss = 55.06870651245117 Training Accuracy = 0.05521075800061226\n","Iteration 3273: Training Loss = 55.06504821777344 Training Accuracy = 0.054926615208387375\n","Iteration 3274: Training Loss = 55.09425354003906 Training Accuracy = 0.0552949421107769\n","Iteration 3275: Training Loss = 55.1611328125 Training Accuracy = 0.054978422820568085\n","Iteration 3276: Training Loss = 55.23969268798828 Training Accuracy = 0.05554034933447838\n","Iteration 3277: Training Loss = 55.391441345214844 Training Accuracy = 0.05490931123495102\n","Iteration 3278: Training Loss = 55.450748443603516 Training Accuracy = 0.05606900528073311\n","Iteration 3279: Training Loss = 55.646480560302734 Training Accuracy = 0.05464649200439453\n","Iteration 3280: Training Loss = 55.5637092590332 Training Accuracy = 0.056465815752744675\n","Iteration 3281: Training Loss = 55.61943817138672 Training Accuracy = 0.05454770475625992\n","Iteration 3282: Training Loss = 55.41014862060547 Training Accuracy = 0.05624581128358841\n","Iteration 3283: Training Loss = 55.32610321044922 Training Accuracy = 0.05473186820745468\n","Iteration 3284: Training Loss = 55.18793487548828 Training Accuracy = 0.05563335493206978\n","Iteration 3285: Training Loss = 55.12388229370117 Training Accuracy = 0.05495259538292885\n","Iteration 3286: Training Loss = 55.06855010986328 Training Accuracy = 0.055347129702568054\n","Iteration 3287: Training Loss = 55.04066848754883 Training Accuracy = 0.055126339197158813\n","Iteration 3288: Training Loss = 55.02642059326172 Training Accuracy = 0.055456142872571945\n","Iteration 3289: Training Loss = 55.02046203613281 Training Accuracy = 0.05525720492005348\n","Iteration 3290: Training Loss = 55.01753234863281 Training Accuracy = 0.05546288192272186\n","Iteration 3291: Training Loss = 55.01700973510742 Training Accuracy = 0.05550036206841469\n","Iteration 3292: Training Loss = 55.021305084228516 Training Accuracy = 0.055385395884513855\n","Iteration 3293: Training Loss = 55.03215026855469 Training Accuracy = 0.05585048720240593\n","Iteration 3294: Training Loss = 55.055397033691406 Training Accuracy = 0.05535348504781723\n","Iteration 3295: Training Loss = 55.09025573730469 Training Accuracy = 0.05621596425771713\n","Iteration 3296: Training Loss = 55.167911529541016 Training Accuracy = 0.05537472292780876\n","Iteration 3297: Training Loss = 55.25619125366211 Training Accuracy = 0.05660371854901314\n","Iteration 3298: Training Loss = 55.47919464111328 Training Accuracy = 0.05529287829995155\n","Iteration 3299: Training Loss = 55.570640563964844 Training Accuracy = 0.05697742477059364\n","Iteration 3300: Training Loss = 55.91157531738281 Training Accuracy = 0.055073853582143784\n","Iteration 3301: Training Loss = 55.729190826416016 Training Accuracy = 0.05719158053398132\n","Iteration 3302: Training Loss = 55.81416702270508 Training Accuracy = 0.055098555982112885\n","Iteration 3303: Training Loss = 55.480167388916016 Training Accuracy = 0.05678199231624603\n","Iteration 3304: Training Loss = 55.35717010498047 Training Accuracy = 0.05524257943034172\n","Iteration 3305: Training Loss = 55.16872024536133 Training Accuracy = 0.055987223982810974\n","Iteration 3306: Training Loss = 55.08372116088867 Training Accuracy = 0.055329252034425735\n","Iteration 3307: Training Loss = 55.01938247680664 Training Accuracy = 0.05550452694296837\n","Iteration 3308: Training Loss = 54.988197326660156 Training Accuracy = 0.05531923845410347\n","Iteration 3309: Training Loss = 54.974937438964844 Training Accuracy = 0.05551347881555557\n","Iteration 3310: Training Loss = 54.97574996948242 Training Accuracy = 0.05530969426035881\n","Iteration 3311: Training Loss = 54.9832878112793 Training Accuracy = 0.05568607524037361\n","Iteration 3312: Training Loss = 54.99185562133789 Training Accuracy = 0.05553148314356804\n","Iteration 3313: Training Loss = 55.00213623046875 Training Accuracy = 0.05571451783180237\n","Iteration 3314: Training Loss = 55.012184143066406 Training Accuracy = 0.05588207766413689\n","Iteration 3315: Training Loss = 55.036746978759766 Training Accuracy = 0.055635202676057816\n","Iteration 3316: Training Loss = 55.06443405151367 Training Accuracy = 0.056280527263879776\n","Iteration 3317: Training Loss = 55.141632080078125 Training Accuracy = 0.055521853268146515\n","Iteration 3318: Training Loss = 55.221431732177734 Training Accuracy = 0.05671641230583191\n","Iteration 3319: Training Loss = 55.44152069091797 Training Accuracy = 0.05532076954841614\n","Iteration 3320: Training Loss = 55.512245178222656 Training Accuracy = 0.05708124861121178\n","Iteration 3321: Training Loss = 55.80413055419922 Training Accuracy = 0.05510178580880165\n","Iteration 3322: Training Loss = 55.58795166015625 Training Accuracy = 0.057188328355550766\n","Iteration 3323: Training Loss = 55.601402282714844 Training Accuracy = 0.05513286590576172\n","Iteration 3324: Training Loss = 55.33107376098633 Training Accuracy = 0.05668681859970093\n","Iteration 3325: Training Loss = 55.24077606201172 Training Accuracy = 0.05528605356812477\n","Iteration 3326: Training Loss = 55.102195739746094 Training Accuracy = 0.05600268021225929\n","Iteration 3327: Training Loss = 55.0399169921875 Training Accuracy = 0.05542565882205963\n","Iteration 3328: Training Loss = 54.980613708496094 Training Accuracy = 0.055590178817510605\n","Iteration 3329: Training Loss = 54.94637680053711 Training Accuracy = 0.05552278831601143\n","Iteration 3330: Training Loss = 54.922428131103516 Training Accuracy = 0.05559160187840462\n","Iteration 3331: Training Loss = 54.91089630126953 Training Accuracy = 0.05553567036986351\n","Iteration 3332: Training Loss = 54.90793991088867 Training Accuracy = 0.055842410773038864\n","Iteration 3333: Training Loss = 54.9091682434082 Training Accuracy = 0.05566033348441124\n","Iteration 3334: Training Loss = 54.91189956665039 Training Accuracy = 0.055975403636693954\n","Iteration 3335: Training Loss = 54.91414260864258 Training Accuracy = 0.05587415024638176\n","Iteration 3336: Training Loss = 54.917049407958984 Training Accuracy = 0.05604984983801842\n","Iteration 3337: Training Loss = 54.9188346862793 Training Accuracy = 0.05611496418714523\n","Iteration 3338: Training Loss = 54.92335891723633 Training Accuracy = 0.0561080127954483\n","Iteration 3339: Training Loss = 54.928871154785156 Training Accuracy = 0.05639687553048134\n","Iteration 3340: Training Loss = 54.950111389160156 Training Accuracy = 0.05611858144402504\n","Iteration 3341: Training Loss = 54.9881591796875 Training Accuracy = 0.05670478194952011\n","Iteration 3342: Training Loss = 55.09581756591797 Training Accuracy = 0.056003741919994354\n","Iteration 3343: Training Loss = 55.22874069213867 Training Accuracy = 0.05715455114841461\n","Iteration 3344: Training Loss = 55.589073181152344 Training Accuracy = 0.055627550929784775\n","Iteration 3345: Training Loss = 55.6988525390625 Training Accuracy = 0.05774058401584625\n","Iteration 3346: Training Loss = 56.15250015258789 Training Accuracy = 0.05520208179950714\n","Iteration 3347: Training Loss = 55.69436264038086 Training Accuracy = 0.05770884454250336\n","Iteration 3348: Training Loss = 55.57426071166992 Training Accuracy = 0.05546434968709946\n","Iteration 3349: Training Loss = 55.21123504638672 Training Accuracy = 0.056716836988925934\n","Iteration 3350: Training Loss = 55.062564849853516 Training Accuracy = 0.05567402392625809\n","Iteration 3351: Training Loss = 54.94394302368164 Training Accuracy = 0.05571817606687546\n","Iteration 3352: Training Loss = 54.88677978515625 Training Accuracy = 0.05577727407217026\n","Iteration 3353: Training Loss = 54.86219024658203 Training Accuracy = 0.05545484647154808\n","Iteration 3354: Training Loss = 54.87080764770508 Training Accuracy = 0.05580131709575653\n","Iteration 3355: Training Loss = 54.913761138916016 Training Accuracy = 0.05552699789404869\n","Iteration 3356: Training Loss = 54.97768783569336 Training Accuracy = 0.05596926063299179\n","Iteration 3357: Training Loss = 55.097190856933594 Training Accuracy = 0.055499445647001266\n","Iteration 3358: Training Loss = 55.173667907714844 Training Accuracy = 0.05651532858610153\n","Iteration 3359: Training Loss = 55.36854934692383 Training Accuracy = 0.055261798202991486\n","Iteration 3360: Training Loss = 55.358211517333984 Training Accuracy = 0.057019516825675964\n","Iteration 3361: Training Loss = 55.49137496948242 Training Accuracy = 0.05511762201786041\n","Iteration 3362: Training Loss = 55.30223083496094 Training Accuracy = 0.05692874267697334\n","Iteration 3363: Training Loss = 55.25502014160156 Training Accuracy = 0.0552334189414978\n","Iteration 3364: Training Loss = 55.070167541503906 Training Accuracy = 0.05636522173881531\n","Iteration 3365: Training Loss = 54.98995590209961 Training Accuracy = 0.055380355566740036\n","Iteration 3366: Training Loss = 54.90327453613281 Training Accuracy = 0.055982738733291626\n","Iteration 3367: Training Loss = 54.86093521118164 Training Accuracy = 0.055523745715618134\n","Iteration 3368: Training Loss = 54.83462905883789 Training Accuracy = 0.055943600833415985\n","Iteration 3369: Training Loss = 54.82179260253906 Training Accuracy = 0.055657077580690384\n","Iteration 3370: Training Loss = 54.81449508666992 Training Accuracy = 0.055966347455978394\n","Iteration 3371: Training Loss = 54.81037902832031 Training Accuracy = 0.055872298777103424\n","Iteration 3372: Training Loss = 54.80936813354492 Training Accuracy = 0.055909693241119385\n","Iteration 3373: Training Loss = 54.812232971191406 Training Accuracy = 0.056130144745111465\n","Iteration 3374: Training Loss = 54.82072830200195 Training Accuracy = 0.05588890239596367\n","Iteration 3375: Training Loss = 54.83601379394531 Training Accuracy = 0.05646447837352753\n","Iteration 3376: Training Loss = 54.86894607543945 Training Accuracy = 0.05594438686966896\n","Iteration 3377: Training Loss = 54.91817092895508 Training Accuracy = 0.056809309870004654\n","Iteration 3378: Training Loss = 55.03255844116211 Training Accuracy = 0.05589885264635086\n","Iteration 3379: Training Loss = 55.143550872802734 Training Accuracy = 0.057212330400943756\n","Iteration 3380: Training Loss = 55.43494415283203 Training Accuracy = 0.05569576844573021\n","Iteration 3381: Training Loss = 55.47904586791992 Training Accuracy = 0.05765929073095322\n","Iteration 3382: Training Loss = 55.787960052490234 Training Accuracy = 0.05552200227975845\n","Iteration 3383: Training Loss = 55.49553298950195 Training Accuracy = 0.05766747519373894\n","Iteration 3384: Training Loss = 55.461631774902344 Training Accuracy = 0.05561723932623863\n","Iteration 3385: Training Loss = 55.154842376708984 Training Accuracy = 0.05696067214012146\n","Iteration 3386: Training Loss = 55.02753448486328 Training Accuracy = 0.05574466288089752\n","Iteration 3387: Training Loss = 54.89247512817383 Training Accuracy = 0.05615788698196411\n","Iteration 3388: Training Loss = 54.82820510864258 Training Accuracy = 0.05581209436058998\n","Iteration 3389: Training Loss = 54.78779220581055 Training Accuracy = 0.0558578222990036\n","Iteration 3390: Training Loss = 54.77033996582031 Training Accuracy = 0.05580420792102814\n","Iteration 3391: Training Loss = 54.76820373535156 Training Accuracy = 0.05594600364565849\n","Iteration 3392: Training Loss = 54.776039123535156 Training Accuracy = 0.05586600676178932\n","Iteration 3393: Training Loss = 54.7902946472168 Training Accuracy = 0.05607353150844574\n","Iteration 3394: Training Loss = 54.806434631347656 Training Accuracy = 0.05611288174986839\n","Iteration 3395: Training Loss = 54.83393096923828 Training Accuracy = 0.056050170212984085\n","Iteration 3396: Training Loss = 54.86540985107422 Training Accuracy = 0.056529611349105835\n","Iteration 3397: Training Loss = 54.941322326660156 Training Accuracy = 0.055933717638254166\n","Iteration 3398: Training Loss = 55.01033401489258 Training Accuracy = 0.05702117457985878\n","Iteration 3399: Training Loss = 55.192169189453125 Training Accuracy = 0.05569930002093315\n","Iteration 3400: Training Loss = 55.25695037841797 Training Accuracy = 0.057396408170461655\n","Iteration 3401: Training Loss = 55.5115852355957 Training Accuracy = 0.055491793900728226\n","Iteration 3402: Training Loss = 55.35630798339844 Training Accuracy = 0.057465966790914536\n","Iteration 3403: Training Loss = 55.397010803222656 Training Accuracy = 0.055553339421749115\n","Iteration 3404: Training Loss = 55.1358757019043 Training Accuracy = 0.057100147008895874\n","Iteration 3405: Training Loss = 55.04385757446289 Training Accuracy = 0.055665094405412674\n","Iteration 3406: Training Loss = 54.90449142456055 Training Accuracy = 0.05649232491850853\n","Iteration 3407: Training Loss = 54.84201431274414 Training Accuracy = 0.05576137453317642\n","Iteration 3408: Training Loss = 54.78635025024414 Training Accuracy = 0.056106142699718475\n","Iteration 3409: Training Loss = 54.75718307495117 Training Accuracy = 0.055914584547281265\n","Iteration 3410: Training Loss = 54.73588180541992 Training Accuracy = 0.055980272591114044\n","Iteration 3411: Training Loss = 54.723384857177734 Training Accuracy = 0.05601086467504501\n","Iteration 3412: Training Loss = 54.71608352661133 Training Accuracy = 0.056104931980371475\n","Iteration 3413: Training Loss = 54.71273422241211 Training Accuracy = 0.056136585772037506\n","Iteration 3414: Training Loss = 54.712154388427734 Training Accuracy = 0.056281931698322296\n","Iteration 3415: Training Loss = 54.713748931884766 Training Accuracy = 0.05631362646818161\n","Iteration 3416: Training Loss = 54.71786117553711 Training Accuracy = 0.056436117738485336\n","Iteration 3417: Training Loss = 54.723854064941406 Training Accuracy = 0.05648446083068848\n","Iteration 3418: Training Loss = 54.735652923583984 Training Accuracy = 0.05652227997779846\n","Iteration 3419: Training Loss = 54.752864837646484 Training Accuracy = 0.056719474494457245\n","Iteration 3420: Training Loss = 54.79256820678711 Training Accuracy = 0.05650263652205467\n","Iteration 3421: Training Loss = 54.84739303588867 Training Accuracy = 0.057099852710962296\n","Iteration 3422: Training Loss = 54.98848342895508 Training Accuracy = 0.05631539225578308\n","Iteration 3423: Training Loss = 55.12948989868164 Training Accuracy = 0.05762002617120743\n","Iteration 3424: Training Loss = 55.52030563354492 Training Accuracy = 0.055951595306396484\n","Iteration 3425: Training Loss = 55.54767990112305 Training Accuracy = 0.058130037039518356\n","Iteration 3426: Training Loss = 55.89854049682617 Training Accuracy = 0.05563928559422493\n","Iteration 3427: Training Loss = 55.4116325378418 Training Accuracy = 0.057983461767435074\n","Iteration 3428: Training Loss = 55.232784271240234 Training Accuracy = 0.055807165801525116\n","Iteration 3429: Training Loss = 54.92544937133789 Training Accuracy = 0.05689810961484909\n","Iteration 3430: Training Loss = 54.7952766418457 Training Accuracy = 0.055925022810697556\n","Iteration 3431: Training Loss = 54.714996337890625 Training Accuracy = 0.05598703399300575\n","Iteration 3432: Training Loss = 54.684261322021484 Training Accuracy = 0.0560801662504673\n","Iteration 3433: Training Loss = 54.681522369384766 Training Accuracy = 0.05575067922472954\n","Iteration 3434: Training Loss = 54.70389938354492 Training Accuracy = 0.056278400123119354\n","Iteration 3435: Training Loss = 54.75741958618164 Training Accuracy = 0.055782269686460495\n","Iteration 3436: Training Loss = 54.826637268066406 Training Accuracy = 0.0565543994307518\n","Iteration 3437: Training Loss = 54.965450286865234 Training Accuracy = 0.055729273706674576\n","Iteration 3438: Training Loss = 55.034488677978516 Training Accuracy = 0.05700312554836273\n","Iteration 3439: Training Loss = 55.22763442993164 Training Accuracy = 0.055560652166604996\n","Iteration 3440: Training Loss = 55.1716194152832 Training Accuracy = 0.05734506621956825\n","Iteration 3441: Training Loss = 55.251346588134766 Training Accuracy = 0.05554436519742012\n","Iteration 3442: Training Loss = 55.05860137939453 Training Accuracy = 0.057179782539606094\n","Iteration 3443: Training Loss = 54.99409103393555 Training Accuracy = 0.055674128234386444\n","Iteration 3444: Training Loss = 54.84642791748047 Training Accuracy = 0.056632187217473984\n","Iteration 3445: Training Loss = 54.78221130371094 Training Accuracy = 0.05583001673221588\n","Iteration 3446: Training Loss = 54.71609115600586 Training Accuracy = 0.05629953369498253\n","Iteration 3447: Training Loss = 54.6836051940918 Training Accuracy = 0.055928315967321396\n","Iteration 3448: Training Loss = 54.66077423095703 Training Accuracy = 0.05633050575852394\n","Iteration 3449: Training Loss = 54.65040969848633 Training Accuracy = 0.05594392120838165\n","Iteration 3450: Training Loss = 54.64474105834961 Training Accuracy = 0.05644309148192406\n","Iteration 3451: Training Loss = 54.641151428222656 Training Accuracy = 0.05604313313961029\n","Iteration 3452: Training Loss = 54.638160705566406 Training Accuracy = 0.05652155727148056\n","Iteration 3453: Training Loss = 54.635746002197266 Training Accuracy = 0.05623367428779602\n","Iteration 3454: Training Loss = 54.63460159301758 Training Accuracy = 0.05660901218652725\n","Iteration 3455: Training Loss = 54.63487243652344 Training Accuracy = 0.05642640218138695\n","Iteration 3456: Training Loss = 54.63529586791992 Training Accuracy = 0.05679068714380264\n","Iteration 3457: Training Loss = 54.6379508972168 Training Accuracy = 0.05651377514004707\n","Iteration 3458: Training Loss = 54.64345932006836 Training Accuracy = 0.0570334829390049\n","Iteration 3459: Training Loss = 54.65748596191406 Training Accuracy = 0.056492749601602554\n","Iteration 3460: Training Loss = 54.68156814575195 Training Accuracy = 0.057340286672115326\n","Iteration 3461: Training Loss = 54.738525390625 Training Accuracy = 0.05646698549389839\n","Iteration 3462: Training Loss = 54.82127380371094 Training Accuracy = 0.05767584964632988\n","Iteration 3463: Training Loss = 55.03992462158203 Training Accuracy = 0.05631881207227707\n","Iteration 3464: Training Loss = 55.207401275634766 Training Accuracy = 0.058055613189935684\n","Iteration 3465: Training Loss = 55.69710159301758 Training Accuracy = 0.05604366585612297\n","Iteration 3466: Training Loss = 55.541015625 Training Accuracy = 0.05833090841770172\n","Iteration 3467: Training Loss = 55.74060821533203 Training Accuracy = 0.0559639036655426\n","Iteration 3468: Training Loss = 55.24480056762695 Training Accuracy = 0.057857271283864975\n","Iteration 3469: Training Loss = 55.04490661621094 Training Accuracy = 0.05611513555049896\n","Iteration 3470: Training Loss = 54.78322219848633 Training Accuracy = 0.05675289034843445\n","Iteration 3471: Training Loss = 54.67138671875 Training Accuracy = 0.056173406541347504\n","Iteration 3472: Training Loss = 54.61134338378906 Training Accuracy = 0.056006185710430145\n","Iteration 3473: Training Loss = 54.59209060668945 Training Accuracy = 0.056118302047252655\n","Iteration 3474: Training Loss = 54.600746154785156 Training Accuracy = 0.05597181245684624\n","Iteration 3475: Training Loss = 54.6325569152832 Training Accuracy = 0.05612512677907944\n","Iteration 3476: Training Loss = 54.69219970703125 Training Accuracy = 0.056151848286390305\n","Iteration 3477: Training Loss = 54.7486572265625 Training Accuracy = 0.05646902695298195\n","Iteration 3478: Training Loss = 54.851524353027344 Training Accuracy = 0.05605592951178551\n","Iteration 3479: Training Loss = 54.895042419433594 Training Accuracy = 0.057011160999536514\n","Iteration 3480: Training Loss = 55.0524787902832 Training Accuracy = 0.05573573708534241\n","Iteration 3481: Training Loss = 55.040164947509766 Training Accuracy = 0.057481590658426285\n","Iteration 3482: Training Loss = 55.14813995361328 Training Accuracy = 0.055560074746608734\n","Iteration 3483: Training Loss = 54.99394989013672 Training Accuracy = 0.05742514878511429\n","Iteration 3484: Training Loss = 54.958709716796875 Training Accuracy = 0.05571819841861725\n","Iteration 3485: Training Loss = 54.79824447631836 Training Accuracy = 0.05683650076389313\n","Iteration 3486: Training Loss = 54.734169006347656 Training Accuracy = 0.05593690648674965\n","Iteration 3487: Training Loss = 54.6546516418457 Training Accuracy = 0.056495197117328644\n","Iteration 3488: Training Loss = 54.618629455566406 Training Accuracy = 0.05602240562438965\n","Iteration 3489: Training Loss = 54.59180450439453 Training Accuracy = 0.05647006630897522\n","Iteration 3490: Training Loss = 54.57892990112305 Training Accuracy = 0.05607232078909874\n","Iteration 3491: Training Loss = 54.56690979003906 Training Accuracy = 0.05649255961179733\n","Iteration 3492: Training Loss = 54.558624267578125 Training Accuracy = 0.056271642446517944\n","Iteration 3493: Training Loss = 54.552581787109375 Training Accuracy = 0.05650899186730385\n","Iteration 3494: Training Loss = 54.55009841918945 Training Accuracy = 0.05646989867091179\n","Iteration 3495: Training Loss = 54.550045013427734 Training Accuracy = 0.056671492755413055\n","Iteration 3496: Training Loss = 54.55445098876953 Training Accuracy = 0.05659972503781319\n","Iteration 3497: Training Loss = 54.56237030029297 Training Accuracy = 0.0569726824760437\n","Iteration 3498: Training Loss = 54.58544921875 Training Accuracy = 0.056555695831775665\n","Iteration 3499: Training Loss = 54.622352600097656 Training Accuracy = 0.05734039098024368\n","Iteration 3500: Training Loss = 54.72746276855469 Training Accuracy = 0.056437987834215164\n","Iteration 3501: Training Loss = 54.8787956237793 Training Accuracy = 0.05790848284959793\n","Iteration 3502: Training Loss = 55.238433837890625 Training Accuracy = 0.05601254105567932\n","Iteration 3503: Training Loss = 55.32585906982422 Training Accuracy = 0.05834275111556053\n","Iteration 3504: Training Loss = 55.70844268798828 Training Accuracy = 0.05609723553061485\n","Iteration 3505: Training Loss = 55.26810073852539 Training Accuracy = 0.058139923959970474\n","Iteration 3506: Training Loss = 55.13774108886719 Training Accuracy = 0.056295428425073624\n","Iteration 3507: Training Loss = 54.812259674072266 Training Accuracy = 0.05720522999763489\n","Iteration 3508: Training Loss = 54.663700103759766 Training Accuracy = 0.05628480017185211\n","Iteration 3509: Training Loss = 54.55801773071289 Training Accuracy = 0.056331567466259\n","Iteration 3510: Training Loss = 54.51731491088867 Training Accuracy = 0.056371577084064484\n","Iteration 3511: Training Loss = 54.51575469970703 Training Accuracy = 0.05600737780332565\n","Iteration 3512: Training Loss = 54.54644012451172 Training Accuracy = 0.056520961225032806\n","Iteration 3513: Training Loss = 54.61479187011719 Training Accuracy = 0.05603665113449097\n","Iteration 3514: Training Loss = 54.695343017578125 Training Accuracy = 0.05676054581999779\n","Iteration 3515: Training Loss = 54.85005187988281 Training Accuracy = 0.05606347694993019\n","Iteration 3516: Training Loss = 54.901126861572266 Training Accuracy = 0.057249765843153\n","Iteration 3517: Training Loss = 55.07243728637695 Training Accuracy = 0.05586513504385948\n","Iteration 3518: Training Loss = 54.97893142700195 Training Accuracy = 0.057539306581020355\n","Iteration 3519: Training Loss = 55.011802673339844 Training Accuracy = 0.05578654259443283\n","Iteration 3520: Training Loss = 54.83148193359375 Training Accuracy = 0.05737823247909546\n","Iteration 3521: Training Loss = 54.757568359375 Training Accuracy = 0.05589009448885918\n","Iteration 3522: Training Loss = 54.634464263916016 Training Accuracy = 0.056858547031879425\n","Iteration 3523: Training Loss = 54.578826904296875 Training Accuracy = 0.05599528178572655\n","Iteration 3524: Training Loss = 54.52875900268555 Training Accuracy = 0.056577783077955246\n","Iteration 3525: Training Loss = 54.50299835205078 Training Accuracy = 0.056255463510751724\n","Iteration 3526: Training Loss = 54.48508834838867 Training Accuracy = 0.05655195564031601\n","Iteration 3527: Training Loss = 54.47527313232422 Training Accuracy = 0.05635930970311165\n","Iteration 3528: Training Loss = 54.470123291015625 Training Accuracy = 0.05662814527750015\n","Iteration 3529: Training Loss = 54.46634292602539 Training Accuracy = 0.05649957433342934\n","Iteration 3530: Training Loss = 54.46375274658203 Training Accuracy = 0.05672893300652504\n","Iteration 3531: Training Loss = 54.4608268737793 Training Accuracy = 0.05668057128787041\n","Iteration 3532: Training Loss = 54.45931625366211 Training Accuracy = 0.056764837354421616\n","Iteration 3533: Training Loss = 54.45960235595703 Training Accuracy = 0.05700408294796944\n","Iteration 3534: Training Loss = 54.46300506591797 Training Accuracy = 0.05683360993862152\n","Iteration 3535: Training Loss = 54.47047805786133 Training Accuracy = 0.057268962264060974\n","Iteration 3536: Training Loss = 54.488861083984375 Training Accuracy = 0.05687472224235535\n","Iteration 3537: Training Loss = 54.52256393432617 Training Accuracy = 0.05754370614886284\n","Iteration 3538: Training Loss = 54.60569763183594 Training Accuracy = 0.056853506714105606\n","Iteration 3539: Training Loss = 54.719261169433594 Training Accuracy = 0.05797434225678444\n","Iteration 3540: Training Loss = 55.0057373046875 Training Accuracy = 0.05658905208110809\n","Iteration 3541: Training Loss = 55.15716552734375 Training Accuracy = 0.058501552790403366\n","Iteration 3542: Training Loss = 55.62799835205078 Training Accuracy = 0.05628199502825737\n","Iteration 3543: Training Loss = 55.31500244140625 Training Accuracy = 0.05868873372673988\n","Iteration 3544: Training Loss = 55.31627655029297 Training Accuracy = 0.056269727647304535\n","Iteration 3545: Training Loss = 54.898983001708984 Training Accuracy = 0.05788163095712662\n","Iteration 3546: Training Loss = 54.724693298339844 Training Accuracy = 0.05638832971453667\n","Iteration 3547: Training Loss = 54.557472229003906 Training Accuracy = 0.05678645893931389\n","Iteration 3548: Training Loss = 54.48567581176758 Training Accuracy = 0.05651335045695305\n","Iteration 3549: Training Loss = 54.44487762451172 Training Accuracy = 0.056255463510751724\n","Iteration 3550: Training Loss = 54.42873001098633 Training Accuracy = 0.05657891184091568\n","Iteration 3551: Training Loss = 54.433319091796875 Training Accuracy = 0.05634940415620804\n","Iteration 3552: Training Loss = 54.45863723754883 Training Accuracy = 0.05663998797535896\n","Iteration 3553: Training Loss = 54.51105499267578 Training Accuracy = 0.05651196837425232\n","Iteration 3554: Training Loss = 54.56794738769531 Training Accuracy = 0.0569332055747509\n","Iteration 3555: Training Loss = 54.6760139465332 Training Accuracy = 0.05642196163535118\n","Iteration 3556: Training Loss = 54.734336853027344 Training Accuracy = 0.05740261450409889\n","Iteration 3557: Training Loss = 54.904823303222656 Training Accuracy = 0.056192196905612946\n","Iteration 3558: Training Loss = 54.89436340332031 Training Accuracy = 0.057817112654447556\n","Iteration 3559: Training Loss = 55.019325256347656 Training Accuracy = 0.05597221478819847\n","Iteration 3560: Training Loss = 54.86714553833008 Training Accuracy = 0.05783522501587868\n","Iteration 3561: Training Loss = 54.84661102294922 Training Accuracy = 0.05607421323657036\n","Iteration 3562: Training Loss = 54.67438888549805 Training Accuracy = 0.057335905730724335\n","Iteration 3563: Training Loss = 54.60639190673828 Training Accuracy = 0.05623820051550865\n","Iteration 3564: Training Loss = 54.51034927368164 Training Accuracy = 0.056903570890426636\n","Iteration 3565: Training Loss = 54.46442794799805 Training Accuracy = 0.05629264563322067\n","Iteration 3566: Training Loss = 54.42782211303711 Training Accuracy = 0.05680945888161659\n","Iteration 3567: Training Loss = 54.41078186035156 Training Accuracy = 0.05629438906908035\n","Iteration 3568: Training Loss = 54.39948654174805 Training Accuracy = 0.05685214698314667\n","Iteration 3569: Training Loss = 54.39235305786133 Training Accuracy = 0.05644997954368591\n","Iteration 3570: Training Loss = 54.38697052001953 Training Accuracy = 0.05689544975757599\n","Iteration 3571: Training Loss = 54.38398361206055 Training Accuracy = 0.056641966104507446\n","Iteration 3572: Training Loss = 54.38336181640625 Training Accuracy = 0.05699462071061134\n","Iteration 3573: Training Loss = 54.387142181396484 Training Accuracy = 0.05683554336428642\n","Iteration 3574: Training Loss = 54.39354705810547 Training Accuracy = 0.05722391605377197\n","Iteration 3575: Training Loss = 54.41066360473633 Training Accuracy = 0.05684814974665642\n","Iteration 3576: Training Loss = 54.43687057495117 Training Accuracy = 0.057537734508514404\n","Iteration 3577: Training Loss = 54.50076675415039 Training Accuracy = 0.056783970445394516\n","Iteration 3578: Training Loss = 54.58122634887695 Training Accuracy = 0.05799848958849907\n","Iteration 3579: Training Loss = 54.77267074584961 Training Accuracy = 0.05661385878920555\n","Iteration 3580: Training Loss = 54.891624450683594 Training Accuracy = 0.05843990296125412\n","Iteration 3581: Training Loss = 55.237762451171875 Training Accuracy = 0.05642206594347954\n","Iteration 3582: Training Loss = 55.11899948120117 Training Accuracy = 0.058550361543893814\n","Iteration 3583: Training Loss = 55.270652770996094 Training Accuracy = 0.056449998170137405\n","Iteration 3584: Training Loss = 54.91749954223633 Training Accuracy = 0.05805537849664688\n","Iteration 3585: Training Loss = 54.80081558227539 Training Accuracy = 0.056513458490371704\n","Iteration 3586: Training Loss = 54.579471588134766 Training Accuracy = 0.057272981852293015\n","Iteration 3587: Training Loss = 54.47696304321289 Training Accuracy = 0.056612223386764526\n","Iteration 3588: Training Loss = 54.396514892578125 Training Accuracy = 0.05668012425303459\n","Iteration 3589: Training Loss = 54.35771179199219 Training Accuracy = 0.056660693138837814\n","Iteration 3590: Training Loss = 54.33836364746094 Training Accuracy = 0.056533802300691605\n","Iteration 3591: Training Loss = 54.335105895996094 Training Accuracy = 0.056672535836696625\n","Iteration 3592: Training Loss = 54.34451675415039 Training Accuracy = 0.0566839724779129\n","Iteration 3593: Training Loss = 54.36481857299805 Training Accuracy = 0.05683290958404541\n","Iteration 3594: Training Loss = 54.40131759643555 Training Accuracy = 0.05681300908327103\n","Iteration 3595: Training Loss = 54.44081497192383 Training Accuracy = 0.05713552236557007\n","Iteration 3596: Training Loss = 54.517818450927734 Training Accuracy = 0.056787073612213135\n","Iteration 3597: Training Loss = 54.5770378112793 Training Accuracy = 0.057556718587875366\n","Iteration 3598: Training Loss = 54.734989166259766 Training Accuracy = 0.056563518941402435\n","Iteration 3599: Training Loss = 54.78132629394531 Training Accuracy = 0.05805503949522972\n","Iteration 3600: Training Loss = 54.989776611328125 Training Accuracy = 0.056283652782440186\n","Iteration 3601: Training Loss = 54.88972091674805 Training Accuracy = 0.058309778571128845\n","Iteration 3602: Training Loss = 54.94873046875 Training Accuracy = 0.056246791034936905\n","Iteration 3603: Training Loss = 54.71711730957031 Training Accuracy = 0.05794370919466019\n","Iteration 3604: Training Loss = 54.628475189208984 Training Accuracy = 0.056380271911621094\n","Iteration 3605: Training Loss = 54.47797393798828 Training Accuracy = 0.057268984615802765\n","Iteration 3606: Training Loss = 54.41038513183594 Training Accuracy = 0.056460630148649216\n","Iteration 3607: Training Loss = 54.35469055175781 Training Accuracy = 0.056936778128147125\n","Iteration 3608: Training Loss = 54.32748794555664 Training Accuracy = 0.056577168405056\n","Iteration 3609: Training Loss = 54.31028366088867 Training Accuracy = 0.056886523962020874\n","Iteration 3610: Training Loss = 54.301265716552734 Training Accuracy = 0.05678008124232292\n","Iteration 3611: Training Loss = 54.29553985595703 Training Accuracy = 0.05693142116069794\n","Iteration 3612: Training Loss = 54.291934967041016 Training Accuracy = 0.057026468217372894\n","Iteration 3613: Training Loss = 54.29014205932617 Training Accuracy = 0.05699319764971733\n","Iteration 3614: Training Loss = 54.29063415527344 Training Accuracy = 0.057288628071546555\n","Iteration 3615: Training Loss = 54.294307708740234 Training Accuracy = 0.05705316737294197\n","Iteration 3616: Training Loss = 54.30238723754883 Training Accuracy = 0.05759647116065025\n","Iteration 3617: Training Loss = 54.31914520263672 Training Accuracy = 0.05706239491701126\n","Iteration 3618: Training Loss = 54.34735870361328 Training Accuracy = 0.057925041764974594\n","Iteration 3619: Training Loss = 54.408905029296875 Training Accuracy = 0.057061776518821716\n","Iteration 3620: Training Loss = 54.48960494995117 Training Accuracy = 0.05825980007648468\n","Iteration 3621: Training Loss = 54.682395935058594 Training Accuracy = 0.05694679170846939\n","Iteration 3622: Training Loss = 54.803321838378906 Training Accuracy = 0.058627232909202576\n","Iteration 3623: Training Loss = 55.15996170043945 Training Accuracy = 0.05669047683477402\n","Iteration 3624: Training Loss = 55.05040740966797 Training Accuracy = 0.058840878307819366\n","Iteration 3625: Training Loss = 55.219730377197266 Training Accuracy = 0.056601617485284805\n","Iteration 3626: Training Loss = 54.87044143676758 Training Accuracy = 0.058410778641700745\n","Iteration 3627: Training Loss = 54.756771087646484 Training Accuracy = 0.05673520267009735\n","Iteration 3628: Training Loss = 54.51466751098633 Training Accuracy = 0.05751475319266319\n","Iteration 3629: Training Loss = 54.406959533691406 Training Accuracy = 0.05685102194547653\n","Iteration 3630: Training Loss = 54.32174301147461 Training Accuracy = 0.05683465301990509\n","Iteration 3631: Training Loss = 54.27989196777344 Training Accuracy = 0.056884586811065674\n","Iteration 3632: Training Loss = 54.257347106933594 Training Accuracy = 0.05673573538661003\n","Iteration 3633: Training Loss = 54.25354766845703 Training Accuracy = 0.05681311711668968\n","Iteration 3634: Training Loss = 54.265995025634766 Training Accuracy = 0.05695790797472\n","Iteration 3635: Training Loss = 54.289730072021484 Training Accuracy = 0.056960225105285645\n","Iteration 3636: Training Loss = 54.325740814208984 Training Accuracy = 0.05710386857390404\n","Iteration 3637: Training Loss = 54.35810470581055 Training Accuracy = 0.05727170407772064\n","Iteration 3638: Training Loss = 54.42436599731445 Training Accuracy = 0.05701266974210739\n","Iteration 3639: Training Loss = 54.47532272338867 Training Accuracy = 0.05775484815239906\n","Iteration 3640: Training Loss = 54.61729431152344 Training Accuracy = 0.056711990386247635\n","Iteration 3641: Training Loss = 54.66889953613281 Training Accuracy = 0.058260563760995865\n","Iteration 3642: Training Loss = 54.86476135253906 Training Accuracy = 0.05643894523382187\n","Iteration 3643: Training Loss = 54.789188385009766 Training Accuracy = 0.05848480015993118\n","Iteration 3644: Training Loss = 54.86534881591797 Training Accuracy = 0.05651430785655975\n","Iteration 3645: Training Loss = 54.64176559448242 Training Accuracy = 0.058123279362916946\n","Iteration 3646: Training Loss = 54.56328201293945 Training Accuracy = 0.05662398040294647\n","Iteration 3647: Training Loss = 54.41012954711914 Training Accuracy = 0.05751900374889374\n","Iteration 3648: Training Loss = 54.34428787231445 Training Accuracy = 0.05664774775505066\n","Iteration 3649: Training Loss = 54.28666687011719 Training Accuracy = 0.05719902366399765\n","Iteration 3650: Training Loss = 54.25853729248047 Training Accuracy = 0.056784991174936295\n","Iteration 3651: Training Loss = 54.237491607666016 Training Accuracy = 0.05711521953344345\n","Iteration 3652: Training Loss = 54.22553253173828 Training Accuracy = 0.05700724944472313\n","Iteration 3653: Training Loss = 54.21657943725586 Training Accuracy = 0.05713054910302162\n","Iteration 3654: Training Loss = 54.210758209228516 Training Accuracy = 0.05722172558307648\n","Iteration 3655: Training Loss = 54.20661926269531 Training Accuracy = 0.057235267013311386\n","Iteration 3656: Training Loss = 54.203636169433594 Training Accuracy = 0.05740954354405403\n","Iteration 3657: Training Loss = 54.201324462890625 Training Accuracy = 0.057339880615472794\n","Iteration 3658: Training Loss = 54.19980239868164 Training Accuracy = 0.057602912187576294\n","Iteration 3659: Training Loss = 54.19968795776367 Training Accuracy = 0.057460885494947433\n","Iteration 3660: Training Loss = 54.20210266113281 Training Accuracy = 0.0578114353120327\n","Iteration 3661: Training Loss = 54.21040725708008 Training Accuracy = 0.057573702186346054\n","Iteration 3662: Training Loss = 54.22815704345703 Training Accuracy = 0.05806766450405121\n","Iteration 3663: Training Loss = 54.27137756347656 Training Accuracy = 0.05755950137972832\n","Iteration 3664: Training Loss = 54.341461181640625 Training Accuracy = 0.05843590572476387\n","Iteration 3665: Training Loss = 54.51344680786133 Training Accuracy = 0.057309649884700775\n","Iteration 3666: Training Loss = 54.68461608886719 Training Accuracy = 0.05895225331187248\n","Iteration 3667: Training Loss = 55.126869201660156 Training Accuracy = 0.05690516531467438\n","Iteration 3668: Training Loss = 55.10026931762695 Training Accuracy = 0.059336140751838684\n","Iteration 3669: Training Loss = 55.397090911865234 Training Accuracy = 0.056805018335580826\n","Iteration 3670: Training Loss = 54.89852523803711 Training Accuracy = 0.05891228839755058\n","Iteration 3671: Training Loss = 54.72220993041992 Training Accuracy = 0.05699509009718895\n","Iteration 3672: Training Loss = 54.42597961425781 Training Accuracy = 0.05776258558034897\n","Iteration 3673: Training Loss = 54.29695510864258 Training Accuracy = 0.05711120367050171\n","Iteration 3674: Training Loss = 54.21765899658203 Training Accuracy = 0.056867919862270355\n","Iteration 3675: Training Loss = 54.18754959106445 Training Accuracy = 0.057260800153017044\n","Iteration 3676: Training Loss = 54.1851806640625 Training Accuracy = 0.05667863413691521\n","Iteration 3677: Training Loss = 54.21293640136719 Training Accuracy = 0.05729321762919426\n","Iteration 3678: Training Loss = 54.28231430053711 Training Accuracy = 0.05683709681034088\n","Iteration 3679: Training Loss = 54.361572265625 Training Accuracy = 0.05756073445081711\n","Iteration 3680: Training Loss = 54.51787185668945 Training Accuracy = 0.0568523183465004\n","Iteration 3681: Training Loss = 54.56450653076172 Training Accuracy = 0.05798256769776344\n","Iteration 3682: Training Loss = 54.728729248046875 Training Accuracy = 0.056647323071956635\n","Iteration 3683: Training Loss = 54.63280487060547 Training Accuracy = 0.0582321435213089\n","Iteration 3684: Training Loss = 54.6612434387207 Training Accuracy = 0.056536585092544556\n","Iteration 3685: Training Loss = 54.49028778076172 Training Accuracy = 0.058096788823604584\n","Iteration 3686: Training Loss = 54.419769287109375 Training Accuracy = 0.05658339709043503\n","Iteration 3687: Training Loss = 54.30340576171875 Training Accuracy = 0.05765858665108681\n","Iteration 3688: Training Loss = 54.25407028198242 Training Accuracy = 0.05681464821100235\n","Iteration 3689: Training Loss = 54.206642150878906 Training Accuracy = 0.05738014355301857\n","Iteration 3690: Training Loss = 54.184165954589844 Training Accuracy = 0.05697208642959595\n","Iteration 3691: Training Loss = 54.16476821899414 Training Accuracy = 0.05739200860261917\n","Iteration 3692: Training Loss = 54.15529251098633 Training Accuracy = 0.057029612362384796\n","Iteration 3693: Training Loss = 54.15116500854492 Training Accuracy = 0.0575760193169117\n","Iteration 3694: Training Loss = 54.149925231933594 Training Accuracy = 0.057065412402153015\n","Iteration 3695: Training Loss = 54.14780044555664 Training Accuracy = 0.057754356414079666\n","Iteration 3696: Training Loss = 54.143341064453125 Training Accuracy = 0.05725493282079697\n","Iteration 3697: Training Loss = 54.13959884643555 Training Accuracy = 0.05781349912285805\n","Iteration 3698: Training Loss = 54.13890838623047 Training Accuracy = 0.05747544765472412\n","Iteration 3699: Training Loss = 54.14169692993164 Training Accuracy = 0.05792706087231636\n","Iteration 3700: Training Loss = 54.150840759277344 Training Accuracy = 0.05762111023068428\n","Iteration 3701: Training Loss = 54.16483688354492 Training Accuracy = 0.058191750198602676\n","Iteration 3702: Training Loss = 54.202083587646484 Training Accuracy = 0.0575965978205204\n","Iteration 3703: Training Loss = 54.25680923461914 Training Accuracy = 0.058598171919584274\n","Iteration 3704: Training Loss = 54.39189910888672 Training Accuracy = 0.0574180893599987\n","Iteration 3705: Training Loss = 54.52974319458008 Training Accuracy = 0.05906645581126213\n","Iteration 3706: Training Loss = 54.8785285949707 Training Accuracy = 0.05715795233845711\n","Iteration 3707: Training Loss = 54.91851806640625 Training Accuracy = 0.059362903237342834\n","Iteration 3708: Training Loss = 55.2612190246582 Training Accuracy = 0.05704929679632187\n","Iteration 3709: Training Loss = 54.855709075927734 Training Accuracy = 0.059053272008895874\n","Iteration 3710: Training Loss = 54.748252868652344 Training Accuracy = 0.05719519406557083\n","Iteration 3711: Training Loss = 54.41986083984375 Training Accuracy = 0.05812978371977806\n","Iteration 3712: Training Loss = 54.27376937866211 Training Accuracy = 0.05720875784754753\n","Iteration 3713: Training Loss = 54.161033630371094 Training Accuracy = 0.05734961852431297\n","Iteration 3714: Training Loss = 54.112159729003906 Training Accuracy = 0.057293303310871124\n","Iteration 3715: Training Loss = 54.09423065185547 Training Accuracy = 0.057106249034404755\n","Iteration 3716: Training Loss = 54.09930419921875 Training Accuracy = 0.05732417106628418\n","Iteration 3717: Training Loss = 54.12528610229492 Training Accuracy = 0.057231441140174866\n","Iteration 3718: Training Loss = 54.16697311401367 Training Accuracy = 0.05754604563117027\n","Iteration 3719: Training Loss = 54.24100112915039 Training Accuracy = 0.05734366551041603\n","Iteration 3720: Training Loss = 54.30124282836914 Training Accuracy = 0.0578739158809185\n","Iteration 3721: Training Loss = 54.42897415161133 Training Accuracy = 0.057205356657505035\n","Iteration 3722: Training Loss = 54.46112823486328 Training Accuracy = 0.058292217552661896\n","Iteration 3723: Training Loss = 54.615272521972656 Training Accuracy = 0.056941285729408264\n","Iteration 3724: Training Loss = 54.5531120300293 Training Accuracy = 0.05863724648952484\n","Iteration 3725: Training Loss = 54.61756896972656 Training Accuracy = 0.056749746203422546\n","Iteration 3726: Training Loss = 54.45970153808594 Training Accuracy = 0.05857653170824051\n","Iteration 3727: Training Loss = 54.40651321411133 Training Accuracy = 0.05687597766518593\n","Iteration 3728: Training Loss = 54.271629333496094 Training Accuracy = 0.05803396925330162\n","Iteration 3729: Training Loss = 54.216190338134766 Training Accuracy = 0.0570903904736042\n","Iteration 3730: Training Loss = 54.15024948120117 Training Accuracy = 0.05765550583600998\n","Iteration 3731: Training Loss = 54.11909866333008 Training Accuracy = 0.05718248337507248\n","Iteration 3732: Training Loss = 54.09379959106445 Training Accuracy = 0.057633865624666214\n","Iteration 3733: Training Loss = 54.08202362060547 Training Accuracy = 0.057307034730911255\n","Iteration 3734: Training Loss = 54.073814392089844 Training Accuracy = 0.05772536247968674\n","Iteration 3735: Training Loss = 54.070133209228516 Training Accuracy = 0.05747510492801666\n","Iteration 3736: Training Loss = 54.06722640991211 Training Accuracy = 0.05783390626311302\n","Iteration 3737: Training Loss = 54.067684173583984 Training Accuracy = 0.057644322514534\n","Iteration 3738: Training Loss = 54.07021713256836 Training Accuracy = 0.05797230079770088\n","Iteration 3739: Training Loss = 54.081321716308594 Training Accuracy = 0.057762011885643005\n","Iteration 3740: Training Loss = 54.09836959838867 Training Accuracy = 0.05818624421954155\n","Iteration 3741: Training Loss = 54.14077377319336 Training Accuracy = 0.05778203532099724\n","Iteration 3742: Training Loss = 54.19357681274414 Training Accuracy = 0.0585307814180851\n","Iteration 3743: Training Loss = 54.32667922973633 Training Accuracy = 0.057650912553071976\n","Iteration 3744: Training Loss = 54.43474578857422 Training Accuracy = 0.05899897962808609\n","Iteration 3745: Training Loss = 54.73105239868164 Training Accuracy = 0.057302847504615784\n","Iteration 3746: Training Loss = 54.74345397949219 Training Accuracy = 0.059434693306684494\n","Iteration 3747: Training Loss = 54.97953796386719 Training Accuracy = 0.05708567053079605\n","Iteration 3748: Training Loss = 54.66983413696289 Training Accuracy = 0.05922536179423332\n","Iteration 3749: Training Loss = 54.58150863647461 Training Accuracy = 0.05722315236926079\n","Iteration 3750: Training Loss = 54.31590270996094 Training Accuracy = 0.058295492082834244\n","Iteration 3751: Training Loss = 54.204219818115234 Training Accuracy = 0.057314563542604446\n","Iteration 3752: Training Loss = 54.105838775634766 Training Accuracy = 0.057594068348407745\n","Iteration 3753: Training Loss = 54.06032943725586 Training Accuracy = 0.05748562887310982\n","Iteration 3754: Training Loss = 54.03290557861328 Training Accuracy = 0.057375915348529816\n","Iteration 3755: Training Loss = 54.02110290527344 Training Accuracy = 0.057688627392053604\n","Iteration 3756: Training Loss = 54.02103805541992 Training Accuracy = 0.05742533877491951\n","Iteration 3757: Training Loss = 54.03219223022461 Training Accuracy = 0.05787525326013565\n","Iteration 3758: Training Loss = 54.05781173706055 Training Accuracy = 0.05750873684883118\n","Iteration 3759: Training Loss = 54.092018127441406 Training Accuracy = 0.058188583701848984\n","Iteration 3760: Training Loss = 54.16105651855469 Training Accuracy = 0.057474467903375626\n","Iteration 3761: Training Loss = 54.22458267211914 Training Accuracy = 0.05858541652560234\n","Iteration 3762: Training Loss = 54.37224197387695 Training Accuracy = 0.05731915310025215\n","Iteration 3763: Training Loss = 54.42896270751953 Training Accuracy = 0.05896894261240959\n","Iteration 3764: Training Loss = 54.61880874633789 Training Accuracy = 0.057181164622306824\n","Iteration 3765: Training Loss = 54.53965759277344 Training Accuracy = 0.059078190475702286\n","Iteration 3766: Training Loss = 54.614891052246094 Training Accuracy = 0.05725276470184326\n","Iteration 3767: Training Loss = 54.41701126098633 Training Accuracy = 0.05874449387192726\n","Iteration 3768: Training Loss = 54.36375427246094 Training Accuracy = 0.05740223079919815\n","Iteration 3769: Training Loss = 54.21342849731445 Training Accuracy = 0.058187905699014664\n","Iteration 3770: Training Loss = 54.15072250366211 Training Accuracy = 0.057455357164144516\n","Iteration 3771: Training Loss = 54.0797004699707 Training Accuracy = 0.05781437084078789\n","Iteration 3772: Training Loss = 54.04484939575195 Training Accuracy = 0.0575878843665123\n","Iteration 3773: Training Loss = 54.01580810546875 Training Accuracy = 0.05769691616296768\n","Iteration 3774: Training Loss = 53.999000549316406 Training Accuracy = 0.05769181624054909\n","Iteration 3775: Training Loss = 53.98695373535156 Training Accuracy = 0.05778875574469566\n","Iteration 3776: Training Loss = 53.97952651977539 Training Accuracy = 0.05780019238591194\n","Iteration 3777: Training Loss = 53.974178314208984 Training Accuracy = 0.057965900748968124\n","Iteration 3778: Training Loss = 53.97040557861328 Training Accuracy = 0.057962097227573395\n","Iteration 3779: Training Loss = 53.967254638671875 Training Accuracy = 0.05814530327916145\n","Iteration 3780: Training Loss = 53.96467590332031 Training Accuracy = 0.05810926854610443\n","Iteration 3781: Training Loss = 53.962581634521484 Training Accuracy = 0.058316707611083984\n","Iteration 3782: Training Loss = 53.96112060546875 Training Accuracy = 0.0582677498459816\n","Iteration 3783: Training Loss = 53.96070861816406 Training Accuracy = 0.058504633605480194\n","Iteration 3784: Training Loss = 53.96206283569336 Training Accuracy = 0.058365389704704285\n","Iteration 3785: Training Loss = 53.96619415283203 Training Accuracy = 0.05873720347881317\n","Iteration 3786: Training Loss = 53.97624206542969 Training Accuracy = 0.05835608020424843\n","Iteration 3787: Training Loss = 53.99546813964844 Training Accuracy = 0.05904294177889824\n","Iteration 3788: Training Loss = 54.02846908569336 Training Accuracy = 0.05823403596878052\n","Iteration 3789: Training Loss = 54.07524490356445 Training Accuracy = 0.059365030378103256\n","Iteration 3790: Training Loss = 54.160858154296875 Training Accuracy = 0.058096323162317276\n","Iteration 3791: Training Loss = 54.25288772583008 Training Accuracy = 0.05963059142231941\n","Iteration 3792: Training Loss = 54.49610900878906 Training Accuracy = 0.05792636051774025\n","Iteration 3793: Training Loss = 54.607601165771484 Training Accuracy = 0.05987047031521797\n","Iteration 3794: Training Loss = 55.06235885620117 Training Accuracy = 0.05774608999490738\n","Iteration 3795: Training Loss = 54.89285659790039 Training Accuracy = 0.06000844016671181\n","Iteration 3796: Training Loss = 55.09773254394531 Training Accuracy = 0.05770242214202881\n","Iteration 3797: Training Loss = 54.609031677246094 Training Accuracy = 0.059426743537187576\n","Iteration 3798: Training Loss = 54.39902877807617 Training Accuracy = 0.05782134458422661\n","Iteration 3799: Training Loss = 54.13290786743164 Training Accuracy = 0.058245621621608734\n","Iteration 3800: Training Loss = 54.03266525268555 Training Accuracy = 0.057892199605703354\n","Iteration 3801: Training Loss = 53.975921630859375 Training Accuracy = 0.05753292888402939\n","Iteration 3802: Training Loss = 53.953948974609375 Training Accuracy = 0.057652849704027176\n","Iteration 3803: Training Loss = 53.958038330078125 Training Accuracy = 0.057661160826683044\n","Iteration 3804: Training Loss = 53.9874153137207 Training Accuracy = 0.05749944597482681\n","Iteration 3805: Training Loss = 54.04184341430664 Training Accuracy = 0.05790552869439125\n","Iteration 3806: Training Loss = 54.08253479003906 Training Accuracy = 0.05785731226205826\n","Iteration 3807: Training Loss = 54.15665054321289 Training Accuracy = 0.05771787837147713\n","Iteration 3808: Training Loss = 54.18940734863281 Training Accuracy = 0.058509841561317444\n","Iteration 3809: Training Loss = 54.329036712646484 Training Accuracy = 0.05732952803373337\n","Iteration 3810: Training Loss = 54.356117248535156 Training Accuracy = 0.05906366929411888\n","Iteration 3811: Training Loss = 54.47683334350586 Training Accuracy = 0.057084821164608\n","Iteration 3812: Training Loss = 54.35878372192383 Training Accuracy = 0.05912119522690773\n","Iteration 3813: Training Loss = 54.34947967529297 Training Accuracy = 0.05730384960770607\n","Iteration 3814: Training Loss = 54.192588806152344 Training Accuracy = 0.0585031658411026\n","Iteration 3815: Training Loss = 54.14205551147461 Training Accuracy = 0.05762793496251106\n","Iteration 3816: Training Loss = 54.048824310302734 Training Accuracy = 0.05809793621301651\n","Iteration 3817: Training Loss = 54.0099983215332 Training Accuracy = 0.05773884057998657\n","Iteration 3818: Training Loss = 53.970558166503906 Training Accuracy = 0.058043792843818665\n","Iteration 3819: Training Loss = 53.95243453979492 Training Accuracy = 0.057735055685043335\n","Iteration 3820: Training Loss = 53.933475494384766 Training Accuracy = 0.05810142308473587\n","Iteration 3821: Training Loss = 53.919490814208984 Training Accuracy = 0.057835906744003296\n","Iteration 3822: Training Loss = 53.90848159790039 Training Accuracy = 0.058154888451099396\n","Iteration 3823: Training Loss = 53.90522384643555 Training Accuracy = 0.05801256373524666\n","Iteration 3824: Training Loss = 53.906429290771484 Training Accuracy = 0.05830937623977661\n","Iteration 3825: Training Loss = 53.91559982299805 Training Accuracy = 0.05806924030184746\n","Iteration 3826: Training Loss = 53.928466796875 Training Accuracy = 0.05862399935722351\n","Iteration 3827: Training Loss = 53.95823669433594 Training Accuracy = 0.057987626641988754\n","Iteration 3828: Training Loss = 53.99654006958008 Training Accuracy = 0.059008970856666565\n","Iteration 3829: Training Loss = 54.08079147338867 Training Accuracy = 0.0578751266002655\n","Iteration 3830: Training Loss = 54.163211822509766 Training Accuracy = 0.05939887464046478\n","Iteration 3831: Training Loss = 54.36579132080078 Training Accuracy = 0.05781568959355354\n","Iteration 3832: Training Loss = 54.4461669921875 Training Accuracy = 0.05970004200935364\n","Iteration 3833: Training Loss = 54.75096893310547 Training Accuracy = 0.05775820463895798\n","Iteration 3834: Training Loss = 54.5827751159668 Training Accuracy = 0.05964241176843643\n","Iteration 3835: Training Loss = 54.65347671508789 Training Accuracy = 0.057782165706157684\n","Iteration 3836: Training Loss = 54.3349609375 Training Accuracy = 0.05916188284754753\n","Iteration 3837: Training Loss = 54.20858383178711 Training Accuracy = 0.05769570544362068\n","Iteration 3838: Training Loss = 54.03156661987305 Training Accuracy = 0.058510906994342804\n","Iteration 3839: Training Loss = 53.95322036743164 Training Accuracy = 0.05768297240138054\n","Iteration 3840: Training Loss = 53.90100860595703 Training Accuracy = 0.05810131877660751\n","Iteration 3841: Training Loss = 53.88072967529297 Training Accuracy = 0.05772453173995018\n","Iteration 3842: Training Loss = 53.87202453613281 Training Accuracy = 0.05809817090630531\n","Iteration 3843: Training Loss = 53.871463775634766 Training Accuracy = 0.05782640352845192\n","Iteration 3844: Training Loss = 53.87433624267578 Training Accuracy = 0.058272745460271835\n","Iteration 3845: Training Loss = 53.87745666503906 Training Accuracy = 0.05802147090435028\n","Iteration 3846: Training Loss = 53.88303756713867 Training Accuracy = 0.05833054706454277\n","Iteration 3847: Training Loss = 53.890037536621094 Training Accuracy = 0.05834883078932762\n","Iteration 3848: Training Loss = 53.912567138671875 Training Accuracy = 0.058239392936229706\n","Iteration 3849: Training Loss = 53.94499206542969 Training Accuracy = 0.05876866355538368\n","Iteration 3850: Training Loss = 54.026832580566406 Training Accuracy = 0.058075424283742905\n","Iteration 3851: Training Loss = 54.1094856262207 Training Accuracy = 0.05929100885987282\n","Iteration 3852: Training Loss = 54.311851501464844 Training Accuracy = 0.05778016522526741\n","Iteration 3853: Training Loss = 54.3836669921875 Training Accuracy = 0.05972835794091225\n","Iteration 3854: Training Loss = 54.62980651855469 Training Accuracy = 0.05764270946383476\n","Iteration 3855: Training Loss = 54.461002349853516 Training Accuracy = 0.05971156433224678\n","Iteration 3856: Training Loss = 54.48584747314453 Training Accuracy = 0.05772257596254349\n","Iteration 3857: Training Loss = 54.22658157348633 Training Accuracy = 0.05910557135939598\n","Iteration 3858: Training Loss = 54.13284683227539 Training Accuracy = 0.057806845754384995\n","Iteration 3859: Training Loss = 53.99134826660156 Training Accuracy = 0.05845862999558449\n","Iteration 3860: Training Loss = 53.925540924072266 Training Accuracy = 0.0578836090862751\n","Iteration 3861: Training Loss = 53.87094497680664 Training Accuracy = 0.0581231489777565\n","Iteration 3862: Training Loss = 53.84229278564453 Training Accuracy = 0.0580337792634964\n","Iteration 3863: Training Loss = 53.82407760620117 Training Accuracy = 0.05808407813310623\n","Iteration 3864: Training Loss = 53.814544677734375 Training Accuracy = 0.05817572399973869\n","Iteration 3865: Training Loss = 53.8105583190918 Training Accuracy = 0.058249808847904205\n","Iteration 3866: Training Loss = 53.81098175048828 Training Accuracy = 0.058326978236436844\n","Iteration 3867: Training Loss = 53.815128326416016 Training Accuracy = 0.05846279859542847\n","Iteration 3868: Training Loss = 53.8223876953125 Training Accuracy = 0.05848454684019089\n","Iteration 3869: Training Loss = 53.834815979003906 Training Accuracy = 0.05858416110277176\n","Iteration 3870: Training Loss = 53.85124969482422 Training Accuracy = 0.058694154024124146\n","Iteration 3871: Training Loss = 53.88556671142578 Training Accuracy = 0.05860678106546402\n","Iteration 3872: Training Loss = 53.929473876953125 Training Accuracy = 0.05906450003385544\n","Iteration 3873: Training Loss = 54.037681579589844 Training Accuracy = 0.058448322117328644\n","Iteration 3874: Training Loss = 54.14076232910156 Training Accuracy = 0.05956349894404411\n","Iteration 3875: Training Loss = 54.41622543334961 Training Accuracy = 0.05809234827756882\n","Iteration 3876: Training Loss = 54.480194091796875 Training Accuracy = 0.060076892375946045\n","Iteration 3877: Training Loss = 54.78022384643555 Training Accuracy = 0.05776107683777809\n","Iteration 3878: Training Loss = 54.49278259277344 Training Accuracy = 0.06005476042628288\n","Iteration 3879: Training Loss = 54.42312240600586 Training Accuracy = 0.05781024694442749\n","Iteration 3880: Training Loss = 54.115047454833984 Training Accuracy = 0.059205569326877594\n","Iteration 3881: Training Loss = 53.984283447265625 Training Accuracy = 0.057923831045627594\n","Iteration 3882: Training Loss = 53.86805725097656 Training Accuracy = 0.05838771164417267\n","Iteration 3883: Training Loss = 53.8150749206543 Training Accuracy = 0.05806541070342064\n","Iteration 3884: Training Loss = 53.787269592285156 Training Accuracy = 0.058121342211961746\n","Iteration 3885: Training Loss = 53.7785758972168 Training Accuracy = 0.05832574516534805\n","Iteration 3886: Training Loss = 53.78327560424805 Training Accuracy = 0.05811098963022232\n","Iteration 3887: Training Loss = 53.800048828125 Training Accuracy = 0.05862901732325554\n","Iteration 3888: Training Loss = 53.8343505859375 Training Accuracy = 0.058158714324235916\n","Iteration 3889: Training Loss = 53.879878997802734 Training Accuracy = 0.058972131460905075\n","Iteration 3890: Training Loss = 53.971588134765625 Training Accuracy = 0.05809476971626282\n","Iteration 3891: Training Loss = 54.04469680786133 Training Accuracy = 0.0593629889190197\n","Iteration 3892: Training Loss = 54.21762466430664 Training Accuracy = 0.057934947311878204\n","Iteration 3893: Training Loss = 54.241519927978516 Training Accuracy = 0.059654463082551956\n","Iteration 3894: Training Loss = 54.4033088684082 Training Accuracy = 0.05785669758915901\n","Iteration 3895: Training Loss = 54.26755142211914 Training Accuracy = 0.05963960289955139\n","Iteration 3896: Training Loss = 54.27874755859375 Training Accuracy = 0.05793694779276848\n","Iteration 3897: Training Loss = 54.094688415527344 Training Accuracy = 0.05926866456866264\n","Iteration 3898: Training Loss = 54.03038024902344 Training Accuracy = 0.05809779092669487\n","Iteration 3899: Training Loss = 53.914894104003906 Training Accuracy = 0.058779358863830566\n","Iteration 3900: Training Loss = 53.86417007446289 Training Accuracy = 0.0581810362637043\n","Iteration 3901: Training Loss = 53.81378173828125 Training Accuracy = 0.0585164949297905\n","Iteration 3902: Training Loss = 53.788230895996094 Training Accuracy = 0.058258991688489914\n","Iteration 3903: Training Loss = 53.76795959472656 Training Accuracy = 0.058507759124040604\n","Iteration 3904: Training Loss = 53.75651550292969 Training Accuracy = 0.0583823136985302\n","Iteration 3905: Training Loss = 53.74863815307617 Training Accuracy = 0.058664415031671524\n","Iteration 3906: Training Loss = 53.74464416503906 Training Accuracy = 0.058506738394498825\n","Iteration 3907: Training Loss = 53.74276351928711 Training Accuracy = 0.0588894784450531\n","Iteration 3908: Training Loss = 53.743064880371094 Training Accuracy = 0.0585932619869709\n","Iteration 3909: Training Loss = 53.74594497680664 Training Accuracy = 0.059122875332832336\n","Iteration 3910: Training Loss = 53.75288009643555 Training Accuracy = 0.058657206594944\n","Iteration 3911: Training Loss = 53.7661247253418 Training Accuracy = 0.05941026657819748\n","Iteration 3912: Training Loss = 53.79429244995117 Training Accuracy = 0.0586811862885952\n","Iteration 3913: Training Loss = 53.83544158935547 Training Accuracy = 0.059723787009716034\n","Iteration 3914: Training Loss = 53.92259979248047 Training Accuracy = 0.05859534442424774\n","Iteration 3915: Training Loss = 54.01116180419922 Training Accuracy = 0.06006135046482086\n","Iteration 3916: Training Loss = 54.231178283691406 Training Accuracy = 0.05841052159667015\n","Iteration 3917: Training Loss = 54.31425857543945 Training Accuracy = 0.060345154255628586\n","Iteration 3918: Training Loss = 54.635555267333984 Training Accuracy = 0.05827510729432106\n","Iteration 3919: Training Loss = 54.46649932861328 Training Accuracy = 0.060319684445858\n","Iteration 3920: Training Loss = 54.55599594116211 Training Accuracy = 0.05831477418541908\n","Iteration 3921: Training Loss = 54.221153259277344 Training Accuracy = 0.05975361540913582\n","Iteration 3922: Training Loss = 54.0894660949707 Training Accuracy = 0.058406952768564224\n","Iteration 3923: Training Loss = 53.897743225097656 Training Accuracy = 0.05893365293741226\n","Iteration 3924: Training Loss = 53.81264877319336 Training Accuracy = 0.05852138623595238\n","Iteration 3925: Training Loss = 53.75111389160156 Training Accuracy = 0.0583958774805069\n","Iteration 3926: Training Loss = 53.71979522705078 Training Accuracy = 0.05850880220532417\n","Iteration 3927: Training Loss = 53.70299530029297 Training Accuracy = 0.05841349810361862\n","Iteration 3928: Training Loss = 53.70085906982422 Training Accuracy = 0.05851466953754425\n","Iteration 3929: Training Loss = 53.7125244140625 Training Accuracy = 0.058683376759290695\n","Iteration 3930: Training Loss = 53.73480987548828 Training Accuracy = 0.05866794288158417\n","Iteration 3931: Training Loss = 53.77083206176758 Training Accuracy = 0.058822426944971085\n","Iteration 3932: Training Loss = 53.80561065673828 Training Accuracy = 0.05896870791912079\n","Iteration 3933: Training Loss = 53.87442398071289 Training Accuracy = 0.05875773727893829\n","Iteration 3934: Training Loss = 53.926048278808594 Training Accuracy = 0.05937863513827324\n","Iteration 3935: Training Loss = 54.071590423583984 Training Accuracy = 0.058505717664957047\n","Iteration 3936: Training Loss = 54.11881637573242 Training Accuracy = 0.05988071858882904\n","Iteration 3937: Training Loss = 54.318721771240234 Training Accuracy = 0.05815308168530464\n","Iteration 3938: Training Loss = 54.24211120605469 Training Accuracy = 0.060161542147397995\n","Iteration 3939: Training Loss = 54.3166618347168 Training Accuracy = 0.058026064187288284\n","Iteration 3940: Training Loss = 54.09739685058594 Training Accuracy = 0.0598207488656044\n","Iteration 3941: Training Loss = 54.01765441894531 Training Accuracy = 0.05809538811445236\n","Iteration 3942: Training Loss = 53.85647964477539 Training Accuracy = 0.05911600589752197\n","Iteration 3943: Training Loss = 53.78438186645508 Training Accuracy = 0.05817735940217972\n","Iteration 3944: Training Loss = 53.71966552734375 Training Accuracy = 0.05872214958071709\n","Iteration 3945: Training Loss = 53.687828063964844 Training Accuracy = 0.058266814798116684\n","Iteration 3946: Training Loss = 53.6675910949707 Training Accuracy = 0.05868447944521904\n","Iteration 3947: Training Loss = 53.657100677490234 Training Accuracy = 0.05853237584233284\n","Iteration 3948: Training Loss = 53.65093231201172 Training Accuracy = 0.05871468782424927\n","Iteration 3949: Training Loss = 53.647247314453125 Training Accuracy = 0.0588035061955452\n","Iteration 3950: Training Loss = 53.64533996582031 Training Accuracy = 0.058796536177396774\n","Iteration 3951: Training Loss = 53.64552688598633 Training Accuracy = 0.05904562026262283\n","Iteration 3952: Training Loss = 53.6495475769043 Training Accuracy = 0.05885739624500275\n","Iteration 3953: Training Loss = 53.65870666503906 Training Accuracy = 0.05935367941856384\n","Iteration 3954: Training Loss = 53.678977966308594 Training Accuracy = 0.05885308235883713\n","Iteration 3955: Training Loss = 53.71360397338867 Training Accuracy = 0.059749554842710495\n","Iteration 3956: Training Loss = 53.78862380981445 Training Accuracy = 0.05875074490904808\n","Iteration 3957: Training Loss = 53.87997817993164 Training Accuracy = 0.0601666234433651\n","Iteration 3958: Training Loss = 54.09164047241211 Training Accuracy = 0.05860795080661774\n","Iteration 3959: Training Loss = 54.199066162109375 Training Accuracy = 0.06052249297499657\n","Iteration 3960: Training Loss = 54.523582458496094 Training Accuracy = 0.058459438383579254\n","Iteration 3961: Training Loss = 54.36128234863281 Training Accuracy = 0.060566771775484085\n","Iteration 3962: Training Loss = 54.44761657714844 Training Accuracy = 0.05849054083228111\n","Iteration 3963: Training Loss = 54.12654495239258 Training Accuracy = 0.06001532822847366\n","Iteration 3964: Training Loss = 54.00930404663086 Training Accuracy = 0.05859959498047829\n","Iteration 3965: Training Loss = 53.81949234008789 Training Accuracy = 0.05919929966330528\n","Iteration 3966: Training Loss = 53.73429870605469 Training Accuracy = 0.05875625088810921\n","Iteration 3967: Training Loss = 53.66976547241211 Training Accuracy = 0.05863545835018158\n","Iteration 3968: Training Loss = 53.637271881103516 Training Accuracy = 0.05887499824166298\n","Iteration 3969: Training Loss = 53.61713409423828 Training Accuracy = 0.05856320261955261\n","Iteration 3970: Training Loss = 53.60737228393555 Training Accuracy = 0.05896666646003723\n","Iteration 3971: Training Loss = 53.6087760925293 Training Accuracy = 0.058791305869817734\n","Iteration 3972: Training Loss = 53.62105178833008 Training Accuracy = 0.05912782624363899\n","Iteration 3973: Training Loss = 53.65001678466797 Training Accuracy = 0.05891556292772293\n","Iteration 3974: Training Loss = 53.687557220458984 Training Accuracy = 0.05939079448580742\n","Iteration 3975: Training Loss = 53.76266098022461 Training Accuracy = 0.058892879635095596\n","Iteration 3976: Training Loss = 53.8270263671875 Training Accuracy = 0.05972183123230934\n","Iteration 3977: Training Loss = 53.989498138427734 Training Accuracy = 0.05872064083814621\n","Iteration 3978: Training Loss = 54.05036544799805 Training Accuracy = 0.06018250435590744\n","Iteration 3979: Training Loss = 54.26887893676758 Training Accuracy = 0.05846460536122322\n","Iteration 3980: Training Loss = 54.17604064941406 Training Accuracy = 0.060489244759082794\n","Iteration 3981: Training Loss = 54.235713958740234 Training Accuracy = 0.05838005989789963\n","Iteration 3982: Training Loss = 54.00590896606445 Training Accuracy = 0.060123831033706665\n","Iteration 3983: Training Loss = 53.92155075073242 Training Accuracy = 0.05848337709903717\n","Iteration 3984: Training Loss = 53.76582336425781 Training Accuracy = 0.05943097546696663\n","Iteration 3985: Training Loss = 53.69722366333008 Training Accuracy = 0.05854753404855728\n","Iteration 3986: Training Loss = 53.634239196777344 Training Accuracy = 0.05907682701945305\n","Iteration 3987: Training Loss = 53.60270690917969 Training Accuracy = 0.058631591498851776\n","Iteration 3988: Training Loss = 53.58287048339844 Training Accuracy = 0.059091899544000626\n","Iteration 3989: Training Loss = 53.5738410949707 Training Accuracy = 0.05878182500600815\n","Iteration 3990: Training Loss = 53.57064437866211 Training Accuracy = 0.05926847457885742\n","Iteration 3991: Training Loss = 53.57085418701172 Training Accuracy = 0.058954868465662\n","Iteration 3992: Training Loss = 53.57316589355469 Training Accuracy = 0.05944423750042915\n","Iteration 3993: Training Loss = 53.57758331298828 Training Accuracy = 0.05910873785614967\n","Iteration 3994: Training Loss = 53.58533477783203 Training Accuracy = 0.05961573123931885\n","Iteration 3995: Training Loss = 53.59557342529297 Training Accuracy = 0.05925905704498291\n","Iteration 3996: Training Loss = 53.610408782958984 Training Accuracy = 0.05973692610859871\n","Iteration 3997: Training Loss = 53.62441635131836 Training Accuracy = 0.059468429535627365\n","Iteration 3998: Training Loss = 53.64657211303711 Training Accuracy = 0.05978645756840706\n","Iteration 3999: Training Loss = 53.66592788696289 Training Accuracy = 0.0597183033823967\n","Iteration 4000: Training Loss = 53.713809967041016 Training Accuracy = 0.05975690856575966\n","Iteration 4001: Training Loss = 53.76362991333008 Training Accuracy = 0.059998683631420135\n","Iteration 4002: Training Loss = 53.9134407043457 Training Accuracy = 0.059522129595279694\n","Iteration 4003: Training Loss = 54.01826477050781 Training Accuracy = 0.06049302592873573\n","Iteration 4004: Training Loss = 54.36271667480469 Training Accuracy = 0.0589878186583519\n","Iteration 4005: Training Loss = 54.32246017456055 Training Accuracy = 0.061049170792102814\n","Iteration 4006: Training Loss = 54.54487228393555 Training Accuracy = 0.05849304795265198\n","Iteration 4007: Training Loss = 54.178958892822266 Training Accuracy = 0.060946982353925705\n","Iteration 4008: Training Loss = 54.009647369384766 Training Accuracy = 0.05857100337743759\n","Iteration 4009: Training Loss = 53.75199508666992 Training Accuracy = 0.059782590717077255\n","Iteration 4010: Training Loss = 53.64530944824219 Training Accuracy = 0.05879019945859909\n","Iteration 4011: Training Loss = 53.5801887512207 Training Accuracy = 0.05894285812973976\n","Iteration 4012: Training Loss = 53.555362701416016 Training Accuracy = 0.05916783586144447\n","Iteration 4013: Training Loss = 53.545494079589844 Training Accuracy = 0.058744240552186966\n","Iteration 4014: Training Loss = 53.54616165161133 Training Accuracy = 0.0594756156206131\n","Iteration 4015: Training Loss = 53.55998992919922 Training Accuracy = 0.058828867971897125\n","Iteration 4016: Training Loss = 53.589717864990234 Training Accuracy = 0.05964119732379913\n","Iteration 4017: Training Loss = 53.651283264160156 Training Accuracy = 0.05892616882920265\n","Iteration 4018: Training Loss = 53.708106994628906 Training Accuracy = 0.05993515998125076\n","Iteration 4019: Training Loss = 53.83626174926758 Training Accuracy = 0.058845408260822296\n","Iteration 4020: Training Loss = 53.88786697387695 Training Accuracy = 0.060286395251750946\n","Iteration 4021: Training Loss = 54.062416076660156 Training Accuracy = 0.05866305157542229\n","Iteration 4022: Training Loss = 54.019813537597656 Training Accuracy = 0.060498449951410294\n","Iteration 4023: Training Loss = 54.109432220458984 Training Accuracy = 0.05866902694106102\n","Iteration 4024: Training Loss = 53.938865661621094 Training Accuracy = 0.06031496450304985\n","Iteration 4025: Training Loss = 53.89582443237305 Training Accuracy = 0.0588105246424675\n","Iteration 4026: Training Loss = 53.74393844604492 Training Accuracy = 0.059802040457725525\n","Iteration 4027: Training Loss = 53.68531799316406 Training Accuracy = 0.058977190405130386\n","Iteration 4028: Training Loss = 53.607444763183594 Training Accuracy = 0.059445492923259735\n","Iteration 4029: Training Loss = 53.57087326049805 Training Accuracy = 0.05898386612534523\n","Iteration 4030: Training Loss = 53.5373420715332 Training Accuracy = 0.05938107892870903\n","Iteration 4031: Training Loss = 53.52049255371094 Training Accuracy = 0.059012945741415024\n","Iteration 4032: Training Loss = 53.51048278808594 Training Accuracy = 0.05955301970243454\n","Iteration 4033: Training Loss = 53.50788879394531 Training Accuracy = 0.05911165103316307\n","Iteration 4034: Training Loss = 53.50712203979492 Training Accuracy = 0.05975931137800217\n","Iteration 4035: Training Loss = 53.5085334777832 Training Accuracy = 0.05919825658202171\n","Iteration 4036: Training Loss = 53.5112190246582 Training Accuracy = 0.05997106805443764\n","Iteration 4037: Training Loss = 53.519412994384766 Training Accuracy = 0.05930338054895401\n","Iteration 4038: Training Loss = 53.533416748046875 Training Accuracy = 0.060187775641679764\n","Iteration 4039: Training Loss = 53.565223693847656 Training Accuracy = 0.05938754230737686\n","Iteration 4040: Training Loss = 53.605323791503906 Training Accuracy = 0.060468558222055435\n","Iteration 4041: Training Loss = 53.695743560791016 Training Accuracy = 0.05936770886182785\n","Iteration 4042: Training Loss = 53.77602767944336 Training Accuracy = 0.060824934393167496\n","Iteration 4043: Training Loss = 53.983863830566406 Training Accuracy = 0.05919608846306801\n","Iteration 4044: Training Loss = 54.0430908203125 Training Accuracy = 0.06109825521707535\n","Iteration 4045: Training Loss = 54.310733795166016 Training Accuracy = 0.05905525013804436\n","Iteration 4046: Training Loss = 54.15263366699219 Training Accuracy = 0.06101856008172035\n","Iteration 4047: Training Loss = 54.21233367919922 Training Accuracy = 0.059108100831508636\n","Iteration 4048: Training Loss = 53.92058181762695 Training Accuracy = 0.060427699238061905\n","Iteration 4049: Training Loss = 53.8090705871582 Training Accuracy = 0.05923869088292122\n","Iteration 4050: Training Loss = 53.64107894897461 Training Accuracy = 0.05970265716314316\n","Iteration 4051: Training Loss = 53.56521987915039 Training Accuracy = 0.05929020047187805\n","Iteration 4052: Training Loss = 53.50560760498047 Training Accuracy = 0.05929965898394585\n","Iteration 4053: Training Loss = 53.473941802978516 Training Accuracy = 0.05931587889790535\n","Iteration 4054: Training Loss = 53.45578384399414 Training Accuracy = 0.05931135267019272\n","Iteration 4055: Training Loss = 53.45004653930664 Training Accuracy = 0.059402529150247574\n","Iteration 4056: Training Loss = 53.45408248901367 Training Accuracy = 0.059574682265520096\n","Iteration 4057: Training Loss = 53.46588897705078 Training Accuracy = 0.05957040935754776\n","Iteration 4058: Training Loss = 53.48576354980469 Training Accuracy = 0.05972179025411606\n","Iteration 4059: Training Loss = 53.51031494140625 Training Accuracy = 0.05980826914310455\n","Iteration 4060: Training Loss = 53.55582809448242 Training Accuracy = 0.059730783104896545\n","Iteration 4061: Training Loss = 53.60356521606445 Training Accuracy = 0.06018165498971939\n","Iteration 4062: Training Loss = 53.71816635131836 Training Accuracy = 0.059579335153102875\n","Iteration 4063: Training Loss = 53.79422378540039 Training Accuracy = 0.06065380573272705\n","Iteration 4064: Training Loss = 54.00977325439453 Training Accuracy = 0.05929245427250862\n","Iteration 4065: Training Loss = 54.01564407348633 Training Accuracy = 0.06111722066998482\n","Iteration 4066: Training Loss = 54.197105407714844 Training Accuracy = 0.059055209159851074\n","Iteration 4067: Training Loss = 53.99371337890625 Training Accuracy = 0.061090923845767975\n","Iteration 4068: Training Loss = 53.947608947753906 Training Accuracy = 0.05900678038597107\n","Iteration 4069: Training Loss = 53.72293472290039 Training Accuracy = 0.06042861565947533\n","Iteration 4070: Training Loss = 53.62358474731445 Training Accuracy = 0.059037670493125916\n","Iteration 4071: Training Loss = 53.52104187011719 Training Accuracy = 0.05983307957649231\n","Iteration 4072: Training Loss = 53.47208786010742 Training Accuracy = 0.0591929629445076\n","Iteration 4073: Training Loss = 53.4400749206543 Training Accuracy = 0.05964861810207367\n","Iteration 4074: Training Loss = 53.42430114746094 Training Accuracy = 0.05947427824139595\n","Iteration 4075: Training Loss = 53.41623306274414 Training Accuracy = 0.05969158187508583\n","Iteration 4076: Training Loss = 53.41291427612305 Training Accuracy = 0.05975795164704323\n","Iteration 4077: Training Loss = 53.41294860839844 Training Accuracy = 0.05976443365216255\n","Iteration 4078: Training Loss = 53.41602325439453 Training Accuracy = 0.060022491961717606\n","Iteration 4079: Training Loss = 53.42409133911133 Training Accuracy = 0.05979417636990547\n","Iteration 4080: Training Loss = 53.43868637084961 Training Accuracy = 0.060316432267427444\n","Iteration 4081: Training Loss = 53.469085693359375 Training Accuracy = 0.05977878347039223\n","Iteration 4082: Training Loss = 53.513694763183594 Training Accuracy = 0.06068061292171478\n","Iteration 4083: Training Loss = 53.61153030395508 Training Accuracy = 0.05966498702764511\n","Iteration 4084: Training Loss = 53.71004104614258 Training Accuracy = 0.061133332550525665\n","Iteration 4085: Training Loss = 53.94309997558594 Training Accuracy = 0.05945569649338722\n","Iteration 4086: Training Loss = 54.00770950317383 Training Accuracy = 0.061473745852708817\n","Iteration 4087: Training Loss = 54.26890563964844 Training Accuracy = 0.059371110051870346\n","Iteration 4088: Training Loss = 54.05397033691406 Training Accuracy = 0.06135017052292824\n","Iteration 4089: Training Loss = 54.05351257324219 Training Accuracy = 0.05941286310553551\n","Iteration 4090: Training Loss = 53.7808837890625 Training Accuracy = 0.06066245585680008\n","Iteration 4091: Training Loss = 53.673248291015625 Training Accuracy = 0.059505805373191833\n","Iteration 4092: Training Loss = 53.53789138793945 Training Accuracy = 0.05998954176902771\n","Iteration 4093: Training Loss = 53.4769401550293 Training Accuracy = 0.05971553921699524\n","Iteration 4094: Training Loss = 53.43355178833008 Training Accuracy = 0.059572894126176834\n","Iteration 4095: Training Loss = 53.41143798828125 Training Accuracy = 0.05988156795501709\n","Iteration 4096: Training Loss = 53.39748764038086 Training Accuracy = 0.05950890854001045\n","Iteration 4097: Training Loss = 53.38970184326172 Training Accuracy = 0.06006636843085289\n","Iteration 4098: Training Loss = 53.38910675048828 Training Accuracy = 0.059706781059503555\n","Iteration 4099: Training Loss = 53.39591598510742 Training Accuracy = 0.06026120483875275\n","Iteration 4100: Training Loss = 53.413883209228516 Training Accuracy = 0.059839922934770584\n","Iteration 4101: Training Loss = 53.44087600708008 Training Accuracy = 0.06052780523896217\n","Iteration 4102: Training Loss = 53.49661636352539 Training Accuracy = 0.059839773923158646\n","Iteration 4103: Training Loss = 53.555267333984375 Training Accuracy = 0.06085880100727081\n","Iteration 4104: Training Loss = 53.69365692138672 Training Accuracy = 0.05968720093369484\n","Iteration 4105: Training Loss = 53.77488327026367 Training Accuracy = 0.06124989315867424\n","Iteration 4106: Training Loss = 54.00541687011719 Training Accuracy = 0.05949073284864426\n","Iteration 4107: Training Loss = 53.96934509277344 Training Accuracy = 0.061510927975177765\n","Iteration 4108: Training Loss = 54.10260009765625 Training Accuracy = 0.059477996081113815\n","Iteration 4109: Training Loss = 53.873348236083984 Training Accuracy = 0.06121094897389412\n","Iteration 4110: Training Loss = 53.81320571899414 Training Accuracy = 0.05957788974046707\n","Iteration 4111: Training Loss = 53.61830520629883 Training Accuracy = 0.06047842279076576\n","Iteration 4112: Training Loss = 53.53739547729492 Training Accuracy = 0.05968254804611206\n","Iteration 4113: Training Loss = 53.451148986816406 Training Accuracy = 0.05998001620173454\n","Iteration 4114: Training Loss = 53.409542083740234 Training Accuracy = 0.0598183237016201\n","Iteration 4115: Training Loss = 53.37910842895508 Training Accuracy = 0.05979345366358757\n","Iteration 4116: Training Loss = 53.362030029296875 Training Accuracy = 0.059976086020469666\n","Iteration 4117: Training Loss = 53.3515510559082 Training Accuracy = 0.0598750002682209\n","Iteration 4118: Training Loss = 53.34634017944336 Training Accuracy = 0.06018463149666786\n","Iteration 4119: Training Loss = 53.345001220703125 Training Accuracy = 0.06003112345933914\n","Iteration 4120: Training Loss = 53.346839904785156 Training Accuracy = 0.06040365621447563\n","Iteration 4121: Training Loss = 53.35269546508789 Training Accuracy = 0.060147449374198914\n","Iteration 4122: Training Loss = 53.36298370361328 Training Accuracy = 0.060654059052467346\n","Iteration 4123: Training Loss = 53.3853874206543 Training Accuracy = 0.06018862500786781\n","Iteration 4124: Training Loss = 53.41834259033203 Training Accuracy = 0.060997724533081055\n","Iteration 4125: Training Loss = 53.4931526184082 Training Accuracy = 0.060063328593969345\n","Iteration 4126: Training Loss = 53.57968521118164 Training Accuracy = 0.061450596898794174\n","Iteration 4127: Training Loss = 53.77648162841797 Training Accuracy = 0.059846896678209305\n","Iteration 4128: Training Loss = 53.87018585205078 Training Accuracy = 0.06185021251440048\n","Iteration 4129: Training Loss = 54.150001525878906 Training Accuracy = 0.05972769856452942\n","Iteration 4130: Training Loss = 54.01046371459961 Training Accuracy = 0.0618002749979496\n","Iteration 4131: Training Loss = 54.088626861572266 Training Accuracy = 0.059816859662532806\n","Iteration 4132: Training Loss = 53.79756164550781 Training Accuracy = 0.06119026243686676\n","Iteration 4133: Training Loss = 53.698707580566406 Training Accuracy = 0.0599384568631649\n","Iteration 4134: Training Loss = 53.525753021240234 Training Accuracy = 0.06040903553366661\n","Iteration 4135: Training Loss = 53.44961929321289 Training Accuracy = 0.060073789209127426\n","Iteration 4136: Training Loss = 53.385623931884766 Training Accuracy = 0.05990210548043251\n","Iteration 4137: Training Loss = 53.35157012939453 Training Accuracy = 0.06023764982819557\n","Iteration 4138: Training Loss = 53.327911376953125 Training Accuracy = 0.059821873903274536\n","Iteration 4139: Training Loss = 53.31465148925781 Training Accuracy = 0.06036281958222389\n","Iteration 4140: Training Loss = 53.31119155883789 Training Accuracy = 0.060038838535547256\n","Iteration 4141: Training Loss = 53.3168830871582 Training Accuracy = 0.06048405542969704\n","Iteration 4142: Training Loss = 53.334014892578125 Training Accuracy = 0.0602194108068943\n","Iteration 4143: Training Loss = 53.359840393066406 Training Accuracy = 0.06071243807673454\n","Iteration 4144: Training Loss = 53.41225814819336 Training Accuracy = 0.0602622888982296\n","Iteration 4145: Training Loss = 53.46688461303711 Training Accuracy = 0.06102880463004112\n","Iteration 4146: Training Loss = 53.5940055847168 Training Accuracy = 0.06016203388571739\n","Iteration 4147: Training Loss = 53.6697883605957 Training Accuracy = 0.06141522154211998\n","Iteration 4148: Training Loss = 53.89090347290039 Training Accuracy = 0.05997651070356369\n","Iteration 4149: Training Loss = 53.87273406982422 Training Accuracy = 0.061789944767951965\n","Iteration 4150: Training Loss = 54.02277755737305 Training Accuracy = 0.059810757637023926\n","Iteration 4151: Training Loss = 53.8125 Training Accuracy = 0.061718642711639404\n","Iteration 4152: Training Loss = 53.75737380981445 Training Accuracy = 0.05976564809679985\n","Iteration 4153: Training Loss = 53.5552864074707 Training Accuracy = 0.061059437692165375\n","Iteration 4154: Training Loss = 53.47003173828125 Training Accuracy = 0.05978439748287201\n","Iteration 4155: Training Loss = 53.37781524658203 Training Accuracy = 0.06051092594861984\n","Iteration 4156: Training Loss = 53.334598541259766 Training Accuracy = 0.05988507717847824\n","Iteration 4157: Training Loss = 53.30257034301758 Training Accuracy = 0.06038954108953476\n","Iteration 4158: Training Loss = 53.28628158569336 Training Accuracy = 0.06005833297967911\n","Iteration 4159: Training Loss = 53.276702880859375 Training Accuracy = 0.06051177904009819\n","Iteration 4160: Training Loss = 53.272769927978516 Training Accuracy = 0.06027325615286827\n","Iteration 4161: Training Loss = 53.27299118041992 Training Accuracy = 0.06069919094443321\n","Iteration 4162: Training Loss = 53.27637481689453 Training Accuracy = 0.06045199930667877\n","Iteration 4163: Training Loss = 53.282894134521484 Training Accuracy = 0.06087436154484749\n","Iteration 4164: Training Loss = 53.292720794677734 Training Accuracy = 0.0606231726706028\n","Iteration 4165: Training Loss = 53.310142517089844 Training Accuracy = 0.06102236360311508\n","Iteration 4166: Training Loss = 53.33296203613281 Training Accuracy = 0.060838840901851654\n","Iteration 4167: Training Loss = 53.37765884399414 Training Accuracy = 0.06106358394026756\n","Iteration 4168: Training Loss = 53.426692962646484 Training Accuracy = 0.061159078031778336\n","Iteration 4169: Training Loss = 53.53995132446289 Training Accuracy = 0.06091762334108353\n","Iteration 4170: Training Loss = 53.62423324584961 Training Accuracy = 0.061587732285261154\n","Iteration 4171: Training Loss = 53.868133544921875 Training Accuracy = 0.060589540749788284\n","Iteration 4172: Training Loss = 53.89263153076172 Training Accuracy = 0.06203484162688255\n","Iteration 4173: Training Loss = 54.160884857177734 Training Accuracy = 0.06011294573545456\n","Iteration 4174: Training Loss = 53.92307662963867 Training Accuracy = 0.06222761422395706\n","Iteration 4175: Training Loss = 53.89176940917969 Training Accuracy = 0.05982174724340439\n","Iteration 4176: Training Loss = 53.60519027709961 Training Accuracy = 0.06168175861239433\n","Iteration 4177: Training Loss = 53.45954132080078 Training Accuracy = 0.0598403699696064\n","Iteration 4178: Training Loss = 53.33144760131836 Training Accuracy = 0.06074241176247597\n","Iteration 4179: Training Loss = 53.27414321899414 Training Accuracy = 0.06005382537841797\n","Iteration 4180: Training Loss = 53.244747161865234 Training Accuracy = 0.060333482921123505\n","Iteration 4181: Training Loss = 53.235294342041016 Training Accuracy = 0.060522958636283875\n","Iteration 4182: Training Loss = 53.236900329589844 Training Accuracy = 0.06030799448490143\n","Iteration 4183: Training Loss = 53.24768829345703 Training Accuracy = 0.06090272217988968\n","Iteration 4184: Training Loss = 53.270809173583984 Training Accuracy = 0.060335393995046616\n","Iteration 4185: Training Loss = 53.307044982910156 Training Accuracy = 0.06122123822569847\n","Iteration 4186: Training Loss = 53.37592315673828 Training Accuracy = 0.06031602993607521\n","Iteration 4187: Training Loss = 53.44142532348633 Training Accuracy = 0.06160714104771614\n","Iteration 4188: Training Loss = 53.579219818115234 Training Accuracy = 0.06022004783153534\n","Iteration 4189: Training Loss = 53.62324905395508 Training Accuracy = 0.06190299615263939\n","Iteration 4190: Training Loss = 53.78443145751953 Training Accuracy = 0.060140881687402725\n","Iteration 4191: Training Loss = 53.716373443603516 Training Accuracy = 0.06193694844841957\n","Iteration 4192: Training Loss = 53.795928955078125 Training Accuracy = 0.060212522745132446\n","Iteration 4193: Training Loss = 53.6385498046875 Training Accuracy = 0.061667900532484055\n","Iteration 4194: Training Loss = 53.609989166259766 Training Accuracy = 0.06032799929380417\n","Iteration 4195: Training Loss = 53.46525955200195 Training Accuracy = 0.06113875284790993\n","Iteration 4196: Training Loss = 53.40985870361328 Training Accuracy = 0.060463838279247284\n","Iteration 4197: Training Loss = 53.334373474121094 Training Accuracy = 0.06069179251790047\n","Iteration 4198: Training Loss = 53.30160140991211 Training Accuracy = 0.060578785836696625\n","Iteration 4199: Training Loss = 53.265323638916016 Training Accuracy = 0.06058888137340546\n","Iteration 4200: Training Loss = 53.24425506591797 Training Accuracy = 0.06062325835227966\n","Iteration 4201: Training Loss = 53.22534942626953 Training Accuracy = 0.06074729934334755\n","Iteration 4202: Training Loss = 53.21623992919922 Training Accuracy = 0.060639627277851105\n","Iteration 4203: Training Loss = 53.212120056152344 Training Accuracy = 0.06100933253765106\n","Iteration 4204: Training Loss = 53.215267181396484 Training Accuracy = 0.06068025156855583\n","Iteration 4205: Training Loss = 53.222049713134766 Training Accuracy = 0.06131596490740776\n","Iteration 4206: Training Loss = 53.23625946044922 Training Accuracy = 0.06069885194301605\n","Iteration 4207: Training Loss = 53.25667190551758 Training Accuracy = 0.061641305685043335\n","Iteration 4208: Training Loss = 53.299991607666016 Training Accuracy = 0.06071513518691063\n","Iteration 4209: Training Loss = 53.3585090637207 Training Accuracy = 0.06196632608771324\n","Iteration 4210: Training Loss = 53.49114227294922 Training Accuracy = 0.06064600497484207\n","Iteration 4211: Training Loss = 53.587249755859375 Training Accuracy = 0.06227761507034302\n","Iteration 4212: Training Loss = 53.83760452270508 Training Accuracy = 0.06048526614904404\n","Iteration 4213: Training Loss = 53.80206298828125 Training Accuracy = 0.062408629804849625\n","Iteration 4214: Training Loss = 53.964599609375 Training Accuracy = 0.06040499731898308\n","Iteration 4215: Training Loss = 53.73627853393555 Training Accuracy = 0.06217191740870476\n","Iteration 4216: Training Loss = 53.698341369628906 Training Accuracy = 0.060413263738155365\n","Iteration 4217: Training Loss = 53.4845085144043 Training Accuracy = 0.06151396781206131\n","Iteration 4218: Training Loss = 53.39247512817383 Training Accuracy = 0.06052115187048912\n","Iteration 4219: Training Loss = 53.29146957397461 Training Accuracy = 0.06086689978837967\n","Iteration 4220: Training Loss = 53.248069763183594 Training Accuracy = 0.06070943921804428\n","Iteration 4221: Training Loss = 53.215484619140625 Training Accuracy = 0.0605471096932888\n","Iteration 4222: Training Loss = 53.19652557373047 Training Accuracy = 0.06086007505655289\n","Iteration 4223: Training Loss = 53.17918395996094 Training Accuracy = 0.06066199019551277\n","Iteration 4224: Training Loss = 53.16594696044922 Training Accuracy = 0.060956165194511414\n","Iteration 4225: Training Loss = 53.157875061035156 Training Accuracy = 0.060943346470594406\n","Iteration 4226: Training Loss = 53.15530014038086 Training Accuracy = 0.061051446944475174\n","Iteration 4227: Training Loss = 53.15686798095703 Training Accuracy = 0.0611475333571434\n","Iteration 4228: Training Loss = 53.16019821166992 Training Accuracy = 0.061171725392341614\n","Iteration 4229: Training Loss = 53.164058685302734 Training Accuracy = 0.06134466454386711\n","Iteration 4230: Training Loss = 53.16830062866211 Training Accuracy = 0.06130599603056908\n","Iteration 4231: Training Loss = 53.17890548706055 Training Accuracy = 0.061423107981681824\n","Iteration 4232: Training Loss = 53.19835662841797 Training Accuracy = 0.06159389764070511\n","Iteration 4233: Training Loss = 53.24739456176758 Training Accuracy = 0.061406463384628296\n","Iteration 4234: Training Loss = 53.317779541015625 Training Accuracy = 0.06199156120419502\n","Iteration 4235: Training Loss = 53.489315032958984 Training Accuracy = 0.061167728155851364\n","Iteration 4236: Training Loss = 53.634033203125 Training Accuracy = 0.0624767430126667\n","Iteration 4237: Training Loss = 54.018577575683594 Training Accuracy = 0.06076989695429802\n","Iteration 4238: Training Loss = 53.975215911865234 Training Accuracy = 0.06288881599903107\n","Iteration 4239: Training Loss = 54.195411682128906 Training Accuracy = 0.0604686439037323\n","Iteration 4240: Training Loss = 53.76199722290039 Training Accuracy = 0.06260047107934952\n","Iteration 4241: Training Loss = 53.576171875 Training Accuracy = 0.06040661036968231\n","Iteration 4242: Training Loss = 53.32494354248047 Training Accuracy = 0.061565134674310684\n","Iteration 4243: Training Loss = 53.2125244140625 Training Accuracy = 0.06041586026549339\n","Iteration 4244: Training Loss = 53.15088653564453 Training Accuracy = 0.06087476760149002\n","Iteration 4245: Training Loss = 53.131465911865234 Training Accuracy = 0.060666218400001526\n","Iteration 4246: Training Loss = 53.13503646850586 Training Accuracy = 0.060743559151887894\n","Iteration 4247: Training Loss = 53.155738830566406 Training Accuracy = 0.06103971228003502\n","Iteration 4248: Training Loss = 53.199378967285156 Training Accuracy = 0.060824573040008545\n","Iteration 4249: Training Loss = 53.254669189453125 Training Accuracy = 0.06140337884426117\n","Iteration 4250: Training Loss = 53.36430740356445 Training Accuracy = 0.06082652881741524\n","Iteration 4251: Training Loss = 53.43214416503906 Training Accuracy = 0.06175442039966583\n","Iteration 4252: Training Loss = 53.60296630859375 Training Accuracy = 0.06067861244082451\n","Iteration 4253: Training Loss = 53.577056884765625 Training Accuracy = 0.06207533925771713\n","Iteration 4254: Training Loss = 53.678749084472656 Training Accuracy = 0.060513563454151154\n","Iteration 4255: Training Loss = 53.53547668457031 Training Accuracy = 0.062135521322488785\n","Iteration 4256: Training Loss = 53.508209228515625 Training Accuracy = 0.06037687137722969\n","Iteration 4257: Training Loss = 53.36604690551758 Training Accuracy = 0.06190561130642891\n","Iteration 4258: Training Loss = 53.29843521118164 Training Accuracy = 0.060405123978853226\n","Iteration 4259: Training Loss = 53.22075653076172 Training Accuracy = 0.06151394546031952\n","Iteration 4260: Training Loss = 53.18609619140625 Training Accuracy = 0.060582611709833145\n","Iteration 4261: Training Loss = 53.15700912475586 Training Accuracy = 0.061262309551239014\n","Iteration 4262: Training Loss = 53.14542770385742 Training Accuracy = 0.060781292617321014\n","Iteration 4263: Training Loss = 53.132713317871094 Training Accuracy = 0.0612996369600296\n","Iteration 4264: Training Loss = 53.12702560424805 Training Accuracy = 0.06088860705494881\n","Iteration 4265: Training Loss = 53.12334060668945 Training Accuracy = 0.06150880083441734\n","Iteration 4266: Training Loss = 53.126705169677734 Training Accuracy = 0.060905952006578445\n","Iteration 4267: Training Loss = 53.13474655151367 Training Accuracy = 0.06181230768561363\n","Iteration 4268: Training Loss = 53.149505615234375 Training Accuracy = 0.06095484644174576\n","Iteration 4269: Training Loss = 53.170166015625 Training Accuracy = 0.06212850660085678\n","Iteration 4270: Training Loss = 53.20266342163086 Training Accuracy = 0.06100514531135559\n","Iteration 4271: Training Loss = 53.24311065673828 Training Accuracy = 0.0623740628361702\n","Iteration 4272: Training Loss = 53.336875915527344 Training Accuracy = 0.06104336678981781\n","Iteration 4273: Training Loss = 53.40957260131836 Training Accuracy = 0.06254526227712631\n","Iteration 4274: Training Loss = 53.615135192871094 Training Accuracy = 0.06101870909333229\n","Iteration 4275: Training Loss = 53.629783630371094 Training Accuracy = 0.06267821043729782\n","Iteration 4276: Training Loss = 53.84349060058594 Training Accuracy = 0.060939520597457886\n","Iteration 4277: Training Loss = 53.67966842651367 Training Accuracy = 0.06258779764175415\n","Iteration 4278: Training Loss = 53.70839309692383 Training Accuracy = 0.06085491180419922\n","Iteration 4279: Training Loss = 53.472965240478516 Training Accuracy = 0.06207563728094101\n","Iteration 4280: Training Loss = 53.38060760498047 Training Accuracy = 0.06090031936764717\n","Iteration 4281: Training Loss = 53.24090576171875 Training Accuracy = 0.06138864904642105\n","Iteration 4282: Training Loss = 53.18296432495117 Training Accuracy = 0.061050381511449814\n","Iteration 4283: Training Loss = 53.133907318115234 Training Accuracy = 0.060983866453170776\n","Iteration 4284: Training Loss = 53.108367919921875 Training Accuracy = 0.06119202822446823\n","Iteration 4285: Training Loss = 53.0853385925293 Training Accuracy = 0.06099868193268776\n","Iteration 4286: Training Loss = 53.06903076171875 Training Accuracy = 0.06127495691180229\n","Iteration 4287: Training Loss = 53.0591926574707 Training Accuracy = 0.06122380867600441\n","Iteration 4288: Training Loss = 53.05613327026367 Training Accuracy = 0.06138571351766586\n","Iteration 4289: Training Loss = 53.05772018432617 Training Accuracy = 0.06139334663748741\n","Iteration 4290: Training Loss = 53.060577392578125 Training Accuracy = 0.06159849092364311\n","Iteration 4291: Training Loss = 53.06391143798828 Training Accuracy = 0.0615265928208828\n","Iteration 4292: Training Loss = 53.06727981567383 Training Accuracy = 0.06182925030589104\n","Iteration 4293: Training Loss = 53.07763671875 Training Accuracy = 0.061546746641397476\n","Iteration 4294: Training Loss = 53.09759521484375 Training Accuracy = 0.06219623610377312\n","Iteration 4295: Training Loss = 53.14449691772461 Training Accuracy = 0.061409905552864075\n","Iteration 4296: Training Loss = 53.215572357177734 Training Accuracy = 0.0627003163099289\n","Iteration 4297: Training Loss = 53.36149978637695 Training Accuracy = 0.061174530535936356\n","Iteration 4298: Training Loss = 53.48964309692383 Training Accuracy = 0.0631152018904686\n","Iteration 4299: Training Loss = 53.79655456542969 Training Accuracy = 0.06102151423692703\n","Iteration 4300: Training Loss = 53.774864196777344 Training Accuracy = 0.06314283609390259\n","Iteration 4301: Training Loss = 54.007164001464844 Training Accuracy = 0.061103399842977524\n","Iteration 4302: Training Loss = 53.67342758178711 Training Accuracy = 0.06269413232803345\n","Iteration 4303: Training Loss = 53.58587646484375 Training Accuracy = 0.06115144491195679\n","Iteration 4304: Training Loss = 53.33015060424805 Training Accuracy = 0.06182100251317024\n","Iteration 4305: Training Loss = 53.216949462890625 Training Accuracy = 0.061208970844745636\n","Iteration 4306: Training Loss = 53.11911392211914 Training Accuracy = 0.06113005802035332\n","Iteration 4307: Training Loss = 53.074989318847656 Training Accuracy = 0.061386797577142715\n","Iteration 4308: Training Loss = 53.04904556274414 Training Accuracy = 0.060887712985277176\n","Iteration 4309: Training Loss = 53.037559509277344 Training Accuracy = 0.06155146658420563\n","Iteration 4310: Training Loss = 53.03684616088867 Training Accuracy = 0.061085715889930725\n","Iteration 4311: Training Loss = 53.05048370361328 Training Accuracy = 0.06167812645435333\n","Iteration 4312: Training Loss = 53.08358383178711 Training Accuracy = 0.06128086894750595\n","Iteration 4313: Training Loss = 53.128849029541016 Training Accuracy = 0.06188199296593666\n","Iteration 4314: Training Loss = 53.222957611083984 Training Accuracy = 0.06128454580903053\n","Iteration 4315: Training Loss = 53.29253387451172 Training Accuracy = 0.062276680022478104\n","Iteration 4316: Training Loss = 53.46421813964844 Training Accuracy = 0.06117170676589012\n","Iteration 4317: Training Loss = 53.485260009765625 Training Accuracy = 0.06263775378465652\n","Iteration 4318: Training Loss = 53.65339279174805 Training Accuracy = 0.06106358394026756\n","Iteration 4319: Training Loss = 53.528690338134766 Training Accuracy = 0.06281127035617828\n","Iteration 4320: Training Loss = 53.54534912109375 Training Accuracy = 0.06093597039580345\n","Iteration 4321: Training Loss = 53.356048583984375 Training Accuracy = 0.0625329539179802\n","Iteration 4322: Training Loss = 53.2778434753418 Training Accuracy = 0.06088260933756828\n","Iteration 4323: Training Loss = 53.16407775878906 Training Accuracy = 0.06199317425489426\n","Iteration 4324: Training Loss = 53.116493225097656 Training Accuracy = 0.060959458351135254\n","Iteration 4325: Training Loss = 53.07122039794922 Training Accuracy = 0.061603762209415436\n","Iteration 4326: Training Loss = 53.04996109008789 Training Accuracy = 0.061106760054826736\n","Iteration 4327: Training Loss = 53.02978515625 Training Accuracy = 0.06161632761359215\n","Iteration 4328: Training Loss = 53.018890380859375 Training Accuracy = 0.06118573620915413\n","Iteration 4329: Training Loss = 53.01314926147461 Training Accuracy = 0.061846937984228134\n","Iteration 4330: Training Loss = 53.012229919433594 Training Accuracy = 0.06128290668129921\n","Iteration 4331: Training Loss = 53.01464080810547 Training Accuracy = 0.06210344284772873\n","Iteration 4332: Training Loss = 53.0167350769043 Training Accuracy = 0.0614071860909462\n","Iteration 4333: Training Loss = 53.01991653442383 Training Accuracy = 0.062323831021785736\n","Iteration 4334: Training Loss = 53.023109436035156 Training Accuracy = 0.061583228409290314\n","Iteration 4335: Training Loss = 53.0333137512207 Training Accuracy = 0.06252364069223404\n","Iteration 4336: Training Loss = 53.05790710449219 Training Accuracy = 0.061701636761426926\n","Iteration 4337: Training Loss = 53.09626770019531 Training Accuracy = 0.06274077296257019\n","Iteration 4338: Training Loss = 53.18498611450195 Training Accuracy = 0.06166760250926018\n","Iteration 4339: Training Loss = 53.27139663696289 Training Accuracy = 0.06302859634160995\n","Iteration 4340: Training Loss = 53.49353790283203 Training Accuracy = 0.061481036245822906\n","Iteration 4341: Training Loss = 53.56810760498047 Training Accuracy = 0.06332459300756454\n","Iteration 4342: Training Loss = 53.8696403503418 Training Accuracy = 0.06128460913896561\n","Iteration 4343: Training Loss = 53.69900894165039 Training Accuracy = 0.06333983689546585\n","Iteration 4344: Training Loss = 53.75810241699219 Training Accuracy = 0.06130644306540489\n","Iteration 4345: Training Loss = 53.43998718261719 Training Accuracy = 0.0626961961388588\n","Iteration 4346: Training Loss = 53.312255859375 Training Accuracy = 0.06141383945941925\n","Iteration 4347: Training Loss = 53.143707275390625 Training Accuracy = 0.06173352524638176\n","Iteration 4348: Training Loss = 53.07564926147461 Training Accuracy = 0.061566922813653946\n","Iteration 4349: Training Loss = 53.02265930175781 Training Accuracy = 0.06124117597937584\n","Iteration 4350: Training Loss = 52.99175262451172 Training Accuracy = 0.061674658209085464\n","Iteration 4351: Training Loss = 52.96889877319336 Training Accuracy = 0.06135314702987671\n","Iteration 4352: Training Loss = 52.95974349975586 Training Accuracy = 0.06172982603311539\n","Iteration 4353: Training Loss = 52.964073181152344 Training Accuracy = 0.061637647449970245\n","Iteration 4354: Training Loss = 52.98041534423828 Training Accuracy = 0.06187191605567932\n","Iteration 4355: Training Loss = 53.011207580566406 Training Accuracy = 0.061789244413375854\n","Iteration 4356: Training Loss = 53.04640579223633 Training Accuracy = 0.062104079872369766\n","Iteration 4357: Training Loss = 53.116695404052734 Training Accuracy = 0.06181233003735542\n","Iteration 4358: Training Loss = 53.176876068115234 Training Accuracy = 0.06244449317455292\n","Iteration 4359: Training Loss = 53.330726623535156 Training Accuracy = 0.061715539544820786\n","Iteration 4360: Training Loss = 53.389522552490234 Training Accuracy = 0.06286285817623138\n","Iteration 4361: Training Loss = 53.60335922241211 Training Accuracy = 0.061480674892663956\n","Iteration 4362: Training Loss = 53.52035903930664 Training Accuracy = 0.06320402026176453\n","Iteration 4363: Training Loss = 53.60073471069336 Training Accuracy = 0.06127138435840607\n","Iteration 4364: Training Loss = 53.38911819458008 Training Accuracy = 0.06303089112043381\n","Iteration 4365: Training Loss = 53.32109069824219 Training Accuracy = 0.06113911420106888\n","Iteration 4366: Training Loss = 53.16410827636719 Training Accuracy = 0.06241492182016373\n","Iteration 4367: Training Loss = 53.09575653076172 Training Accuracy = 0.061084650456905365\n","Iteration 4368: Training Loss = 53.024940490722656 Training Accuracy = 0.06198469549417496\n","Iteration 4369: Training Loss = 52.98801803588867 Training Accuracy = 0.061210840940475464\n","Iteration 4370: Training Loss = 52.96185302734375 Training Accuracy = 0.061959099024534225\n","Iteration 4371: Training Loss = 52.946475982666016 Training Accuracy = 0.06140250712633133\n","Iteration 4372: Training Loss = 52.93864822387695 Training Accuracy = 0.062057651579380035\n","Iteration 4373: Training Loss = 52.933963775634766 Training Accuracy = 0.061682164669036865\n","Iteration 4374: Training Loss = 52.932395935058594 Training Accuracy = 0.062142156064510345\n","Iteration 4375: Training Loss = 52.93183517456055 Training Accuracy = 0.06194659695029259\n","Iteration 4376: Training Loss = 52.93439865112305 Training Accuracy = 0.062225572764873505\n","Iteration 4377: Training Loss = 52.940940856933594 Training Accuracy = 0.062247343361377716\n","Iteration 4378: Training Loss = 52.9578742980957 Training Accuracy = 0.06227714568376541\n","Iteration 4379: Training Loss = 52.984920501708984 Training Accuracy = 0.06256143748760223\n","Iteration 4380: Training Loss = 53.04801559448242 Training Accuracy = 0.062202997505664825\n","Iteration 4381: Training Loss = 53.12617111206055 Training Accuracy = 0.06297170370817184\n","Iteration 4382: Training Loss = 53.31680679321289 Training Accuracy = 0.061991624534130096\n","Iteration 4383: Training Loss = 53.43836212158203 Training Accuracy = 0.06344247609376907\n","Iteration 4384: Training Loss = 53.78593826293945 Training Accuracy = 0.06166069209575653\n","Iteration 4385: Training Loss = 53.67239761352539 Training Accuracy = 0.0637534037232399\n","Iteration 4386: Training Loss = 53.7855339050293 Training Accuracy = 0.06140195578336716\n","Iteration 4387: Training Loss = 53.4295768737793 Training Accuracy = 0.06334681063890457\n","Iteration 4388: Training Loss = 53.27444076538086 Training Accuracy = 0.061348892748355865\n","Iteration 4389: Training Loss = 53.074153900146484 Training Accuracy = 0.06242929399013519\n","Iteration 4390: Training Loss = 52.98261642456055 Training Accuracy = 0.06140142306685448\n","Iteration 4391: Training Loss = 52.92753219604492 Training Accuracy = 0.06183579936623573\n","Iteration 4392: Training Loss = 52.90590286254883 Training Accuracy = 0.0616987869143486\n","Iteration 4393: Training Loss = 52.89919662475586 Training Accuracy = 0.06167808175086975\n","Iteration 4394: Training Loss = 52.90182876586914 Training Accuracy = 0.06205267831683159\n","Iteration 4395: Training Loss = 52.91353988647461 Training Accuracy = 0.061769451946020126\n","Iteration 4396: Training Loss = 52.93598937988281 Training Accuracy = 0.06239451467990875\n","Iteration 4397: Training Loss = 52.9815559387207 Training Accuracy = 0.06183329224586487\n","Iteration 4398: Training Loss = 53.038204193115234 Training Accuracy = 0.06275880336761475\n","Iteration 4399: Training Loss = 53.15674591064453 Training Accuracy = 0.06175112724304199\n","Iteration 4400: Training Loss = 53.23014831542969 Training Accuracy = 0.06315121054649353\n","Iteration 4401: Training Loss = 53.41617965698242 Training Accuracy = 0.061555568128824234\n","Iteration 4402: Training Loss = 53.39244079589844 Training Accuracy = 0.06340008229017258\n","Iteration 4403: Training Loss = 53.50226593017578 Training Accuracy = 0.06146630644798279\n","Iteration 4404: Training Loss = 53.340824127197266 Training Accuracy = 0.06327787041664124\n","Iteration 4405: Training Loss = 53.3085823059082 Training Accuracy = 0.06154704466462135\n","Iteration 4406: Training Loss = 53.14948272705078 Training Accuracy = 0.06273192912340164\n","Iteration 4407: Training Loss = 53.090267181396484 Training Accuracy = 0.06170259416103363\n","Iteration 4408: Training Loss = 53.00990676879883 Training Accuracy = 0.06216566637158394\n","Iteration 4409: Training Loss = 52.983009338378906 Training Accuracy = 0.06192823126912117\n","Iteration 4410: Training Loss = 52.9533576965332 Training Accuracy = 0.06184423714876175\n","Iteration 4411: Training Loss = 52.942745208740234 Training Accuracy = 0.06213567033410072\n","Iteration 4412: Training Loss = 52.925315856933594 Training Accuracy = 0.06187714636325836\n","Iteration 4413: Training Loss = 52.916587829589844 Training Accuracy = 0.06225814297795296\n","Iteration 4414: Training Loss = 52.907527923583984 Training Accuracy = 0.06207706034183502\n","Iteration 4415: Training Loss = 52.90819549560547 Training Accuracy = 0.062317751348018646\n","Iteration 4416: Training Loss = 52.90878677368164 Training Accuracy = 0.062310099601745605\n","Iteration 4417: Training Loss = 52.91819763183594 Training Accuracy = 0.06236041709780693\n","Iteration 4418: Training Loss = 52.92557144165039 Training Accuracy = 0.06254659593105316\n","Iteration 4419: Training Loss = 52.95125961303711 Training Accuracy = 0.062445301562547684\n","Iteration 4420: Training Loss = 52.98046112060547 Training Accuracy = 0.06277104467153549\n","Iteration 4421: Training Loss = 53.06195831298828 Training Accuracy = 0.06246764585375786\n","Iteration 4422: Training Loss = 53.13187789916992 Training Accuracy = 0.06305010616779327\n","Iteration 4423: Training Loss = 53.32333755493164 Training Accuracy = 0.06229972466826439\n","Iteration 4424: Training Loss = 53.368839263916016 Training Accuracy = 0.06340134143829346\n","Iteration 4425: Training Loss = 53.60952377319336 Training Accuracy = 0.06190074235200882\n","Iteration 4426: Training Loss = 53.46802520751953 Training Accuracy = 0.06368275731801987\n","Iteration 4427: Training Loss = 53.52540969848633 Training Accuracy = 0.061508674174547195\n","Iteration 4428: Training Loss = 53.29444122314453 Training Accuracy = 0.06349453330039978\n","Iteration 4429: Training Loss = 53.18776321411133 Training Accuracy = 0.06137213110923767\n","Iteration 4430: Training Loss = 53.02911376953125 Training Accuracy = 0.06275170296430588\n","Iteration 4431: Training Loss = 52.95597839355469 Training Accuracy = 0.061527784913778305\n","Iteration 4432: Training Loss = 52.90218734741211 Training Accuracy = 0.06220580264925957\n","Iteration 4433: Training Loss = 52.879112243652344 Training Accuracy = 0.061830081045627594\n","Iteration 4434: Training Loss = 52.856449127197266 Training Accuracy = 0.062100447714328766\n","Iteration 4435: Training Loss = 52.84000015258789 Training Accuracy = 0.06203807517886162\n","Iteration 4436: Training Loss = 52.82721710205078 Training Accuracy = 0.06225782260298729\n","Iteration 4437: Training Loss = 52.821537017822266 Training Accuracy = 0.06221768632531166\n","Iteration 4438: Training Loss = 52.81989669799805 Training Accuracy = 0.06247999519109726\n","Iteration 4439: Training Loss = 52.82166290283203 Training Accuracy = 0.062400106340646744\n","Iteration 4440: Training Loss = 52.82189178466797 Training Accuracy = 0.06267855316400528\n","Iteration 4441: Training Loss = 52.820438385009766 Training Accuracy = 0.06254685670137405\n","Iteration 4442: Training Loss = 52.817840576171875 Training Accuracy = 0.06280561536550522\n","Iteration 4443: Training Loss = 52.81489181518555 Training Accuracy = 0.06275280565023422\n","Iteration 4444: Training Loss = 52.817413330078125 Training Accuracy = 0.06287042796611786\n","Iteration 4445: Training Loss = 52.82810974121094 Training Accuracy = 0.06302410364151001\n","Iteration 4446: Training Loss = 52.85404968261719 Training Accuracy = 0.06288541853427887\n","Iteration 4447: Training Loss = 52.89911651611328 Training Accuracy = 0.06330429762601852\n","Iteration 4448: Training Loss = 53.005470275878906 Training Accuracy = 0.06282999366521835\n","Iteration 4449: Training Loss = 53.13533020019531 Training Accuracy = 0.06358449906110764\n","Iteration 4450: Training Loss = 53.46256637573242 Training Accuracy = 0.06261902302503586\n","Iteration 4451: Training Loss = 53.5764045715332 Training Accuracy = 0.06406284123659134\n","Iteration 4452: Training Loss = 54.01877212524414 Training Accuracy = 0.062176890671253204\n","Iteration 4453: Training Loss = 53.62718200683594 Training Accuracy = 0.06429426372051239\n","Iteration 4454: Training Loss = 53.546016693115234 Training Accuracy = 0.061869941651821136\n","Iteration 4455: Training Loss = 53.185306549072266 Training Accuracy = 0.063679039478302\n","Iteration 4456: Training Loss = 53.01851272583008 Training Accuracy = 0.0616292729973793\n","Iteration 4457: Training Loss = 52.8885498046875 Training Accuracy = 0.06272146850824356\n","Iteration 4458: Training Loss = 52.82974624633789 Training Accuracy = 0.06169183552265167\n","Iteration 4459: Training Loss = 52.81422424316406 Training Accuracy = 0.06230301782488823\n","Iteration 4460: Training Loss = 52.81874465942383 Training Accuracy = 0.0621049739420414\n","Iteration 4461: Training Loss = 52.83343505859375 Training Accuracy = 0.06233159080147743\n","Iteration 4462: Training Loss = 52.84990692138672 Training Accuracy = 0.0624396912753582\n","Iteration 4463: Training Loss = 52.890281677246094 Training Accuracy = 0.06237378716468811\n","Iteration 4464: Training Loss = 52.9358024597168 Training Accuracy = 0.0627218559384346\n","Iteration 4465: Training Loss = 53.03837966918945 Training Accuracy = 0.062357399612665176\n","Iteration 4466: Training Loss = 53.076900482177734 Training Accuracy = 0.06307993084192276\n","Iteration 4467: Training Loss = 53.2091064453125 Training Accuracy = 0.06223312020301819\n","Iteration 4468: Training Loss = 53.17854690551758 Training Accuracy = 0.06343919783830643\n","Iteration 4469: Training Loss = 53.276397705078125 Training Accuracy = 0.061882078647613525\n","Iteration 4470: Training Loss = 53.19294357299805 Training Accuracy = 0.06374604254961014\n","Iteration 4471: Training Loss = 53.201419830322266 Training Accuracy = 0.061637990176677704\n","Iteration 4472: Training Loss = 53.06979751586914 Training Accuracy = 0.0635414719581604\n","Iteration 4473: Training Loss = 53.00472640991211 Training Accuracy = 0.06175701692700386\n","Iteration 4474: Training Loss = 52.91633987426758 Training Accuracy = 0.06291273236274719\n","Iteration 4475: Training Loss = 52.88732147216797 Training Accuracy = 0.062073469161987305\n","Iteration 4476: Training Loss = 52.845741271972656 Training Accuracy = 0.06253809481859207\n","Iteration 4477: Training Loss = 52.829261779785156 Training Accuracy = 0.062242135405540466\n","Iteration 4478: Training Loss = 52.804866790771484 Training Accuracy = 0.0626089945435524\n","Iteration 4479: Training Loss = 52.79950714111328 Training Accuracy = 0.06233499199151993\n","Iteration 4480: Training Loss = 52.7967529296875 Training Accuracy = 0.06282240897417068\n","Iteration 4481: Training Loss = 52.808223724365234 Training Accuracy = 0.062439095228910446\n","Iteration 4482: Training Loss = 52.819087982177734 Training Accuracy = 0.06302276998758316\n","Iteration 4483: Training Loss = 52.850982666015625 Training Accuracy = 0.06256569176912308\n","Iteration 4484: Training Loss = 52.87685012817383 Training Accuracy = 0.06324867904186249\n","Iteration 4485: Training Loss = 52.95493698120117 Training Accuracy = 0.06248905137181282\n","Iteration 4486: Training Loss = 53.015132904052734 Training Accuracy = 0.06364069133996964\n","Iteration 4487: Training Loss = 53.18610763549805 Training Accuracy = 0.062281250953674316\n","Iteration 4488: Training Loss = 53.21864700317383 Training Accuracy = 0.06400565803050995\n","Iteration 4489: Training Loss = 53.39965057373047 Training Accuracy = 0.06202176958322525\n","Iteration 4490: Training Loss = 53.261505126953125 Training Accuracy = 0.06404680758714676\n","Iteration 4491: Training Loss = 53.25486373901367 Training Accuracy = 0.06193849816918373\n","Iteration 4492: Training Loss = 53.06271743774414 Training Accuracy = 0.06358027458190918\n","Iteration 4493: Training Loss = 52.9852409362793 Training Accuracy = 0.062025297433137894\n","Iteration 4494: Training Loss = 52.877830505371094 Training Accuracy = 0.06298964470624924\n","Iteration 4495: Training Loss = 52.83371353149414 Training Accuracy = 0.06225361302495003\n","Iteration 4496: Training Loss = 52.79159927368164 Training Accuracy = 0.06266620010137558\n","Iteration 4497: Training Loss = 52.77727508544922 Training Accuracy = 0.06256228685379028\n","Iteration 4498: Training Loss = 52.763763427734375 Training Accuracy = 0.062497980892658234\n","Iteration 4499: Training Loss = 52.76089096069336 Training Accuracy = 0.06282665580511093\n","Iteration 4500: Training Loss = 52.7553825378418 Training Accuracy = 0.06262058019638062\n","Iteration 4501: Training Loss = 52.75746536254883 Training Accuracy = 0.06296584010124207\n","Iteration 4502: Training Loss = 52.7593994140625 Training Accuracy = 0.06280452758073807\n","Iteration 4503: Training Loss = 52.77042007446289 Training Accuracy = 0.0630657970905304\n","Iteration 4504: Training Loss = 52.780033111572266 Training Accuracy = 0.06300901621580124\n","Iteration 4505: Training Loss = 52.805259704589844 Training Accuracy = 0.06316158920526505\n","Iteration 4506: Training Loss = 52.82505798339844 Training Accuracy = 0.06317859143018723\n","Iteration 4507: Training Loss = 52.88350296020508 Training Accuracy = 0.06310699135065079\n","Iteration 4508: Training Loss = 52.93018341064453 Training Accuracy = 0.06346103549003601\n","Iteration 4509: Training Loss = 53.06774139404297 Training Accuracy = 0.0629737451672554\n","Iteration 4510: Training Loss = 53.12593078613281 Training Accuracy = 0.06378260999917984\n","Iteration 4511: Training Loss = 53.351627349853516 Training Accuracy = 0.06262147426605225\n","Iteration 4512: Training Loss = 53.283294677734375 Training Accuracy = 0.06416258215904236\n","Iteration 4513: Training Loss = 53.421226501464844 Training Accuracy = 0.062152571976184845\n","Iteration 4514: Training Loss = 53.2205696105957 Training Accuracy = 0.06423246115446091\n","Iteration 4515: Training Loss = 53.1517448425293 Training Accuracy = 0.06186549738049507\n","Iteration 4516: Training Loss = 52.95654296875 Training Accuracy = 0.06362268328666687\n","Iteration 4517: Training Loss = 52.8549919128418 Training Accuracy = 0.0620083324611187\n","Iteration 4518: Training Loss = 52.77035903930664 Training Accuracy = 0.06285244226455688\n","Iteration 4519: Training Loss = 52.73527145385742 Training Accuracy = 0.06233726441860199\n","Iteration 4520: Training Loss = 52.7075309753418 Training Accuracy = 0.06258393079042435\n","Iteration 4521: Training Loss = 52.69124984741211 Training Accuracy = 0.06267223507165909\n","Iteration 4522: Training Loss = 52.6783447265625 Training Accuracy = 0.06269557774066925\n","Iteration 4523: Training Loss = 52.67182922363281 Training Accuracy = 0.06291088461875916\n","Iteration 4524: Training Loss = 52.67017364501953 Training Accuracy = 0.06289077550172806\n","Iteration 4525: Training Loss = 52.67268371582031 Training Accuracy = 0.06314347684383392\n","Iteration 4526: Training Loss = 52.67741012573242 Training Accuracy = 0.06297185271978378\n","Iteration 4527: Training Loss = 52.68428421020508 Training Accuracy = 0.06344003230333328\n","Iteration 4528: Training Loss = 52.69479751586914 Training Accuracy = 0.06294862180948257\n","Iteration 4529: Training Loss = 52.712989807128906 Training Accuracy = 0.06377200037240982\n","Iteration 4530: Training Loss = 52.74551773071289 Training Accuracy = 0.06283798813819885\n","Iteration 4531: Training Loss = 52.792030334472656 Training Accuracy = 0.06412340700626373\n","Iteration 4532: Training Loss = 52.875858306884766 Training Accuracy = 0.06272393465042114\n","Iteration 4533: Training Loss = 52.96587371826172 Training Accuracy = 0.06442750990390778\n","Iteration 4534: Training Loss = 53.19222640991211 Training Accuracy = 0.06263669580221176\n","Iteration 4535: Training Loss = 53.27718734741211 Training Accuracy = 0.06456764787435532\n","Iteration 4536: Training Loss = 53.61052322387695 Training Accuracy = 0.06258518248796463\n","Iteration 4537: Training Loss = 53.39112854003906 Training Accuracy = 0.06448212265968323\n","Iteration 4538: Training Loss = 53.41557312011719 Training Accuracy = 0.06258410215377808\n","Iteration 4539: Training Loss = 53.07634353637695 Training Accuracy = 0.06386377662420273\n","Iteration 4540: Training Loss = 52.929954528808594 Training Accuracy = 0.06250876188278198\n","Iteration 4541: Training Loss = 52.771636962890625 Training Accuracy = 0.06307990849018097\n","Iteration 4542: Training Loss = 52.70290756225586 Training Accuracy = 0.06250089406967163\n","Iteration 4543: Training Loss = 52.66465759277344 Training Accuracy = 0.06273224949836731\n","Iteration 4544: Training Loss = 52.65095901489258 Training Accuracy = 0.06264802068471909\n","Iteration 4545: Training Loss = 52.6455078125 Training Accuracy = 0.06280586868524551\n","Iteration 4546: Training Loss = 52.64717102050781 Training Accuracy = 0.06280467659235\n","Iteration 4547: Training Loss = 52.658199310302734 Training Accuracy = 0.06311336904764175\n","Iteration 4548: Training Loss = 52.67698669433594 Training Accuracy = 0.06295197457075119\n","Iteration 4549: Training Loss = 52.71117401123047 Training Accuracy = 0.06328173726797104\n","Iteration 4550: Training Loss = 52.743099212646484 Training Accuracy = 0.06317786872386932\n","Iteration 4551: Training Loss = 52.804718017578125 Training Accuracy = 0.06326315551996231\n","Iteration 4552: Training Loss = 52.8474006652832 Training Accuracy = 0.06356732547283173\n","Iteration 4553: Training Loss = 52.974239349365234 Training Accuracy = 0.06300025433301926\n","Iteration 4554: Training Loss = 53.02647399902344 Training Accuracy = 0.06406460702419281\n","Iteration 4555: Training Loss = 53.2354736328125 Training Accuracy = 0.0625942200422287\n","Iteration 4556: Training Loss = 53.20023727416992 Training Accuracy = 0.06455756723880768\n","Iteration 4557: Training Loss = 53.314022064208984 Training Accuracy = 0.06221103295683861\n","Iteration 4558: Training Loss = 53.104217529296875 Training Accuracy = 0.06436049938201904\n","Iteration 4559: Training Loss = 53.01491928100586 Training Accuracy = 0.06222017481923103\n","Iteration 4560: Training Loss = 52.832820892333984 Training Accuracy = 0.06347863376140594\n","Iteration 4561: Training Loss = 52.753211975097656 Training Accuracy = 0.062399234622716904\n","Iteration 4562: Training Loss = 52.67821502685547 Training Accuracy = 0.0628930926322937\n","Iteration 4563: Training Loss = 52.64126968383789 Training Accuracy = 0.06261711567640305\n","Iteration 4564: Training Loss = 52.617225646972656 Training Accuracy = 0.06282778829336166\n","Iteration 4565: Training Loss = 52.6060676574707 Training Accuracy = 0.06288422644138336\n","Iteration 4566: Training Loss = 52.601993560791016 Training Accuracy = 0.06293788552284241\n","Iteration 4567: Training Loss = 52.602874755859375 Training Accuracy = 0.06322625279426575\n","Iteration 4568: Training Loss = 52.60734558105469 Training Accuracy = 0.06299791485071182\n","Iteration 4569: Training Loss = 52.61663818359375 Training Accuracy = 0.06358018517494202\n","Iteration 4570: Training Loss = 52.63260269165039 Training Accuracy = 0.06297614425420761\n","Iteration 4571: Training Loss = 52.658226013183594 Training Accuracy = 0.06396613270044327\n","Iteration 4572: Training Loss = 52.702903747558594 Training Accuracy = 0.06293120980262756\n","Iteration 4573: Training Loss = 52.76087188720703 Training Accuracy = 0.06429489701986313\n","Iteration 4574: Training Loss = 52.87751770019531 Training Accuracy = 0.06282319128513336\n","Iteration 4575: Training Loss = 52.97182083129883 Training Accuracy = 0.06454057991504669\n","Iteration 4576: Training Loss = 53.218299865722656 Training Accuracy = 0.06274279206991196\n","Iteration 4577: Training Loss = 53.218345642089844 Training Accuracy = 0.06458820402622223\n","Iteration 4578: Training Loss = 53.43086624145508 Training Accuracy = 0.06269358098506927\n","Iteration 4579: Training Loss = 53.17921829223633 Training Accuracy = 0.06437529623508453\n","Iteration 4580: Training Loss = 53.12510681152344 Training Accuracy = 0.0626913458108902\n","Iteration 4581: Training Loss = 52.87961196899414 Training Accuracy = 0.06372328102588654\n","Iteration 4582: Training Loss = 52.77127456665039 Training Accuracy = 0.06265833228826523\n","Iteration 4583: Training Loss = 52.665191650390625 Training Accuracy = 0.0631127581000328\n","Iteration 4584: Training Loss = 52.61832046508789 Training Accuracy = 0.06272068619728088\n","Iteration 4585: Training Loss = 52.59037780761719 Training Accuracy = 0.06291192770004272\n","Iteration 4586: Training Loss = 52.57737350463867 Training Accuracy = 0.06287525594234467\n","Iteration 4587: Training Loss = 52.56987380981445 Training Accuracy = 0.06304717063903809\n","Iteration 4588: Training Loss = 52.56726837158203 Training Accuracy = 0.06307625770568848\n","Iteration 4589: Training Loss = 52.56972122192383 Training Accuracy = 0.0632927268743515\n","Iteration 4590: Training Loss = 52.5777587890625 Training Accuracy = 0.06325074285268784\n","Iteration 4591: Training Loss = 52.595619201660156 Training Accuracy = 0.06344402581453323\n","Iteration 4592: Training Loss = 52.61859130859375 Training Accuracy = 0.06340010464191437\n","Iteration 4593: Training Loss = 52.65888977050781 Training Accuracy = 0.06355752795934677\n","Iteration 4594: Training Loss = 52.698265075683594 Training Accuracy = 0.06364810466766357\n","Iteration 4595: Training Loss = 52.79065704345703 Training Accuracy = 0.06347808241844177\n","Iteration 4596: Training Loss = 52.86045837402344 Training Accuracy = 0.06404107064008713\n","Iteration 4597: Training Loss = 53.06476593017578 Training Accuracy = 0.06316022574901581\n","Iteration 4598: Training Loss = 53.10292053222656 Training Accuracy = 0.06454889476299286\n","Iteration 4599: Training Loss = 53.34096908569336 Training Accuracy = 0.06264375150203705\n","Iteration 4600: Training Loss = 53.188201904296875 Training Accuracy = 0.0648016557097435\n","Iteration 4601: Training Loss = 53.20226287841797 Training Accuracy = 0.06235480308532715\n","Iteration 4602: Training Loss = 52.94562530517578 Training Accuracy = 0.06427934020757675\n","Iteration 4603: Training Loss = 52.8193244934082 Training Accuracy = 0.06239066645503044\n","Iteration 4604: Training Loss = 52.677120208740234 Training Accuracy = 0.06339617073535919\n","Iteration 4605: Training Loss = 52.61126708984375 Training Accuracy = 0.06250782310962677\n","Iteration 4606: Training Loss = 52.56548309326172 Training Accuracy = 0.06300057470798492\n","Iteration 4607: Training Loss = 52.54226303100586 Training Accuracy = 0.06280156970024109\n","Iteration 4608: Training Loss = 52.53035354614258 Training Accuracy = 0.06308569014072418\n","Iteration 4609: Training Loss = 52.52772903442383 Training Accuracy = 0.0631210058927536\n","Iteration 4610: Training Loss = 52.53205490112305 Training Accuracy = 0.06320880353450775\n","Iteration 4611: Training Loss = 52.54329299926758 Training Accuracy = 0.06341447681188583\n","Iteration 4612: Training Loss = 52.56489181518555 Training Accuracy = 0.06330637633800507\n","Iteration 4613: Training Loss = 52.592803955078125 Training Accuracy = 0.06370086967945099\n","Iteration 4614: Training Loss = 52.650657653808594 Training Accuracy = 0.0632939413189888\n","Iteration 4615: Training Loss = 52.7106819152832 Training Accuracy = 0.06406936794519424\n","Iteration 4616: Training Loss = 52.85753631591797 Training Accuracy = 0.06308463215827942\n","Iteration 4617: Training Loss = 52.94670104980469 Training Accuracy = 0.0645599290728569\n","Iteration 4618: Training Loss = 53.192047119140625 Training Accuracy = 0.06268777698278427\n","Iteration 4619: Training Loss = 53.14108657836914 Training Accuracy = 0.06486864387989044\n","Iteration 4620: Training Loss = 53.233829498291016 Training Accuracy = 0.06254364550113678\n","Iteration 4621: Training Loss = 52.985992431640625 Training Accuracy = 0.06444615125656128\n","Iteration 4622: Training Loss = 52.89143371582031 Training Accuracy = 0.06265195459127426\n","Iteration 4623: Training Loss = 52.71725845336914 Training Accuracy = 0.06365146487951279\n","Iteration 4624: Training Loss = 52.64270782470703 Training Accuracy = 0.06278599053621292\n","Iteration 4625: Training Loss = 52.57324981689453 Training Accuracy = 0.06315786391496658\n","Iteration 4626: Training Loss = 52.54017639160156 Training Accuracy = 0.06297242641448975\n","Iteration 4627: Training Loss = 52.51853561401367 Training Accuracy = 0.06303924322128296\n","Iteration 4628: Training Loss = 52.507991790771484 Training Accuracy = 0.06324787437915802\n","Iteration 4629: Training Loss = 52.50334930419922 Training Accuracy = 0.06310569494962692\n","Iteration 4630: Training Loss = 52.505027770996094 Training Accuracy = 0.0635717511177063\n","Iteration 4631: Training Loss = 52.51213836669922 Training Accuracy = 0.06312710791826248\n","Iteration 4632: Training Loss = 52.52662658691406 Training Accuracy = 0.06393042206764221\n","Iteration 4633: Training Loss = 52.54629135131836 Training Accuracy = 0.06312761455774307\n","Iteration 4634: Training Loss = 52.57150650024414 Training Accuracy = 0.06422219425439835\n","Iteration 4635: Training Loss = 52.607574462890625 Training Accuracy = 0.06310217082500458\n","Iteration 4636: Training Loss = 52.64530563354492 Training Accuracy = 0.06445121020078659\n","Iteration 4637: Training Loss = 52.728477478027344 Training Accuracy = 0.06308063119649887\n","Iteration 4638: Training Loss = 52.7961540222168 Training Accuracy = 0.06463178247213364\n","Iteration 4639: Training Loss = 52.989540100097656 Training Accuracy = 0.06300556659698486\n","Iteration 4640: Training Loss = 53.03559494018555 Training Accuracy = 0.06477297842502594\n","Iteration 4641: Training Loss = 53.280574798583984 Training Accuracy = 0.06294205039739609\n","Iteration 4642: Training Loss = 53.11640930175781 Training Accuracy = 0.06469324231147766\n","Iteration 4643: Training Loss = 53.13857650756836 Training Accuracy = 0.06289013475179672\n","Iteration 4644: Training Loss = 52.87022399902344 Training Accuracy = 0.06410793215036392\n","Iteration 4645: Training Loss = 52.76097106933594 Training Accuracy = 0.06285674124956131\n","Iteration 4646: Training Loss = 52.6207160949707 Training Accuracy = 0.06338456273078918\n","Iteration 4647: Training Loss = 52.56051254272461 Training Accuracy = 0.06291905045509338\n","Iteration 4648: Training Loss = 52.51274490356445 Training Accuracy = 0.06300237774848938\n","Iteration 4649: Training Loss = 52.4881591796875 Training Accuracy = 0.06303633004426956\n","Iteration 4650: Training Loss = 52.47092056274414 Training Accuracy = 0.06307300180196762\n","Iteration 4651: Training Loss = 52.46162033081055 Training Accuracy = 0.06325756758451462\n","Iteration 4652: Training Loss = 52.45876693725586 Training Accuracy = 0.06327009201049805\n","Iteration 4653: Training Loss = 52.46254348754883 Training Accuracy = 0.06353471428155899\n","Iteration 4654: Training Loss = 52.4746208190918 Training Accuracy = 0.06333442032337189\n","Iteration 4655: Training Loss = 52.49516296386719 Training Accuracy = 0.06385142356157303\n","Iteration 4656: Training Loss = 52.53489303588867 Training Accuracy = 0.06330648064613342\n","Iteration 4657: Training Loss = 52.585079193115234 Training Accuracy = 0.06425595283508301\n","Iteration 4658: Training Loss = 52.69454574584961 Training Accuracy = 0.06310588866472244\n","Iteration 4659: Training Loss = 52.7901496887207 Training Accuracy = 0.06469574570655823\n","Iteration 4660: Training Loss = 53.00224685668945 Training Accuracy = 0.06286103278398514\n","Iteration 4661: Training Loss = 53.02573013305664 Training Accuracy = 0.06500080972909927\n","Iteration 4662: Training Loss = 53.196571350097656 Training Accuracy = 0.0627768263220787\n","Iteration 4663: Training Loss = 53.00463104248047 Training Accuracy = 0.06467631459236145\n","Iteration 4664: Training Loss = 52.98713684082031 Training Accuracy = 0.06290081143379211\n","Iteration 4665: Training Loss = 52.77621841430664 Training Accuracy = 0.06389985233545303\n","Iteration 4666: Training Loss = 52.7015495300293 Training Accuracy = 0.06306011974811554\n","Iteration 4667: Training Loss = 52.596012115478516 Training Accuracy = 0.06316645443439484\n","Iteration 4668: Training Loss = 52.552852630615234 Training Accuracy = 0.06327012926340103\n","Iteration 4669: Training Loss = 52.5096435546875 Training Accuracy = 0.06287965923547745\n","Iteration 4670: Training Loss = 52.4831657409668 Training Accuracy = 0.06343620270490646\n","Iteration 4671: Training Loss = 52.45792770385742 Training Accuracy = 0.06300178915262222\n","Iteration 4672: Training Loss = 52.44226837158203 Training Accuracy = 0.06353428959846497\n","Iteration 4673: Training Loss = 52.43440246582031 Training Accuracy = 0.06321186572313309\n","Iteration 4674: Training Loss = 52.43397521972656 Training Accuracy = 0.06367823481559753\n","Iteration 4675: Training Loss = 52.4398193359375 Training Accuracy = 0.06338771432638168\n","Iteration 4676: Training Loss = 52.452213287353516 Training Accuracy = 0.06393267214298248\n","Iteration 4677: Training Loss = 52.479549407958984 Training Accuracy = 0.06341911107301712\n","Iteration 4678: Training Loss = 52.52060317993164 Training Accuracy = 0.06429038941860199\n","Iteration 4679: Training Loss = 52.610107421875 Training Accuracy = 0.06327202171087265\n","Iteration 4680: Training Loss = 52.706336975097656 Training Accuracy = 0.06474149972200394\n","Iteration 4681: Training Loss = 52.916259765625 Training Accuracy = 0.06302997469902039\n","Iteration 4682: Training Loss = 52.98306655883789 Training Accuracy = 0.0651032105088234\n","Iteration 4683: Training Loss = 53.21430587768555 Training Accuracy = 0.06292072683572769\n","Iteration 4684: Training Loss = 53.03234100341797 Training Accuracy = 0.06491073220968246\n","Iteration 4685: Training Loss = 53.03565979003906 Training Accuracy = 0.06300453096628189\n","Iteration 4686: Training Loss = 52.78196334838867 Training Accuracy = 0.0642070546746254\n","Iteration 4687: Training Loss = 52.68324279785156 Training Accuracy = 0.06303754448890686\n","Iteration 4688: Training Loss = 52.553428649902344 Training Accuracy = 0.06344529986381531\n","Iteration 4689: Training Loss = 52.49696350097656 Training Accuracy = 0.06318692862987518\n","Iteration 4690: Training Loss = 52.45290756225586 Training Accuracy = 0.06307066231966019\n","Iteration 4691: Training Loss = 52.43218994140625 Training Accuracy = 0.0634150505065918\n","Iteration 4692: Training Loss = 52.4179573059082 Training Accuracy = 0.06309007108211517\n","Iteration 4693: Training Loss = 52.41012954711914 Training Accuracy = 0.0636802688241005\n","Iteration 4694: Training Loss = 52.40693664550781 Training Accuracy = 0.06321486085653305\n","Iteration 4695: Training Loss = 52.41016387939453 Training Accuracy = 0.06394106894731522\n","Iteration 4696: Training Loss = 52.42058563232422 Training Accuracy = 0.06334096193313599\n","Iteration 4697: Training Loss = 52.44158172607422 Training Accuracy = 0.06418018788099289\n","Iteration 4698: Training Loss = 52.48274230957031 Training Accuracy = 0.06336862593889236\n","Iteration 4699: Training Loss = 52.53436279296875 Training Accuracy = 0.06449861824512482\n","Iteration 4700: Training Loss = 52.64326477050781 Training Accuracy = 0.06327138841152191\n","Iteration 4701: Training Loss = 52.727481842041016 Training Accuracy = 0.06483154743909836\n","Iteration 4702: Training Loss = 52.93605041503906 Training Accuracy = 0.06311625987291336\n","Iteration 4703: Training Loss = 52.94731140136719 Training Accuracy = 0.06507125496864319\n","Iteration 4704: Training Loss = 53.12595748901367 Training Accuracy = 0.06303767114877701\n","Iteration 4705: Training Loss = 52.93593978881836 Training Accuracy = 0.06486266851425171\n","Iteration 4706: Training Loss = 52.91078186035156 Training Accuracy = 0.06306830048561096\n","Iteration 4707: Training Loss = 52.692771911621094 Training Accuracy = 0.06419596076011658\n","Iteration 4708: Training Loss = 52.603904724121094 Training Accuracy = 0.06310688704252243\n","Iteration 4709: Training Loss = 52.49652099609375 Training Accuracy = 0.0635305717587471\n","Iteration 4710: Training Loss = 52.449893951416016 Training Accuracy = 0.06322814524173737\n","Iteration 4711: Training Loss = 52.41166687011719 Training Accuracy = 0.06326805055141449\n","Iteration 4712: Training Loss = 52.39164733886719 Training Accuracy = 0.06341896206140518\n","Iteration 4713: Training Loss = 52.37549591064453 Training Accuracy = 0.06335052847862244\n","Iteration 4714: Training Loss = 52.36528778076172 Training Accuracy = 0.06365624815225601\n","Iteration 4715: Training Loss = 52.359527587890625 Training Accuracy = 0.06349494308233261\n","Iteration 4716: Training Loss = 52.35908889770508 Training Accuracy = 0.06390059739351273\n","Iteration 4717: Training Loss = 52.363365173339844 Training Accuracy = 0.06357215344905853\n","Iteration 4718: Training Loss = 52.37305450439453 Training Accuracy = 0.06416213512420654\n","Iteration 4719: Training Loss = 52.38945388793945 Training Accuracy = 0.06360746175050735\n","Iteration 4720: Training Loss = 52.41321563720703 Training Accuracy = 0.0645257830619812\n","Iteration 4721: Training Loss = 52.45183181762695 Training Accuracy = 0.06353314220905304\n","Iteration 4722: Training Loss = 52.49620056152344 Training Accuracy = 0.06486628204584122\n","Iteration 4723: Training Loss = 52.58369445800781 Training Accuracy = 0.06340291351079941\n","Iteration 4724: Training Loss = 52.664306640625 Training Accuracy = 0.06512336432933807\n","Iteration 4725: Training Loss = 52.8698844909668 Training Accuracy = 0.0633067786693573\n","Iteration 4726: Training Loss = 52.928096771240234 Training Accuracy = 0.06524457782506943\n","Iteration 4727: Training Loss = 53.2001838684082 Training Accuracy = 0.06330497562885284\n","Iteration 4728: Training Loss = 53.01615905761719 Training Accuracy = 0.06503547728061676\n","Iteration 4729: Training Loss = 53.041908264160156 Training Accuracy = 0.0633024200797081\n","Iteration 4730: Training Loss = 52.7463264465332 Training Accuracy = 0.06437614560127258\n","Iteration 4731: Training Loss = 52.6236572265625 Training Accuracy = 0.06331080198287964\n","Iteration 4732: Training Loss = 52.4744987487793 Training Accuracy = 0.06359664350748062\n","Iteration 4733: Training Loss = 52.4080810546875 Training Accuracy = 0.06337663531303406\n","Iteration 4734: Training Loss = 52.362483978271484 Training Accuracy = 0.06323958188295364\n","Iteration 4735: Training Loss = 52.339900970458984 Training Accuracy = 0.06355524808168411\n","Iteration 4736: Training Loss = 52.327667236328125 Training Accuracy = 0.06331384181976318\n","Iteration 4737: Training Loss = 52.326393127441406 Training Accuracy = 0.06378286331892014\n","Iteration 4738: Training Loss = 52.33559036254883 Training Accuracy = 0.0635068416595459\n","Iteration 4739: Training Loss = 52.35658645629883 Training Accuracy = 0.06407519429922104\n","Iteration 4740: Training Loss = 52.3986930847168 Training Accuracy = 0.06350915879011154\n","Iteration 4741: Training Loss = 52.452091217041016 Training Accuracy = 0.06446018069982529\n","Iteration 4742: Training Loss = 52.56036376953125 Training Accuracy = 0.06339683383703232\n","Iteration 4743: Training Loss = 52.636024475097656 Training Accuracy = 0.0648910254240036\n","Iteration 4744: Training Loss = 52.8179931640625 Training Accuracy = 0.06320461630821228\n","Iteration 4745: Training Loss = 52.82210922241211 Training Accuracy = 0.06512976437807083\n","Iteration 4746: Training Loss = 52.9566764831543 Training Accuracy = 0.06312078982591629\n","Iteration 4747: Training Loss = 52.795684814453125 Training Accuracy = 0.06495733559131622\n","Iteration 4748: Training Loss = 52.768985748291016 Training Accuracy = 0.06317245215177536\n","Iteration 4749: Training Loss = 52.593910217285156 Training Accuracy = 0.06438862532377243\n","Iteration 4750: Training Loss = 52.52531433105469 Training Accuracy = 0.06327418982982635\n","Iteration 4751: Training Loss = 52.43336486816406 Training Accuracy = 0.06382980197668076\n","Iteration 4752: Training Loss = 52.39676284790039 Training Accuracy = 0.06345618516206741\n","Iteration 4753: Training Loss = 52.36000442504883 Training Accuracy = 0.06353186815977097\n","Iteration 4754: Training Loss = 52.343074798583984 Training Accuracy = 0.0636533573269844\n","Iteration 4755: Training Loss = 52.3266487121582 Training Accuracy = 0.06355465203523636\n","Iteration 4756: Training Loss = 52.31804275512695 Training Accuracy = 0.06387608498334885\n","Iteration 4757: Training Loss = 52.31208419799805 Training Accuracy = 0.06369655579328537\n","Iteration 4758: Training Loss = 52.31142044067383 Training Accuracy = 0.06408184766769409\n","Iteration 4759: Training Loss = 52.313453674316406 Training Accuracy = 0.06381970643997192\n","Iteration 4760: Training Loss = 52.31930160522461 Training Accuracy = 0.06429294496774673\n","Iteration 4761: Training Loss = 52.32365798950195 Training Accuracy = 0.06396190822124481\n","Iteration 4762: Training Loss = 52.32957458496094 Training Accuracy = 0.06448046118021011\n","Iteration 4763: Training Loss = 52.33183670043945 Training Accuracy = 0.0640769973397255\n","Iteration 4764: Training Loss = 52.33848190307617 Training Accuracy = 0.06458571553230286\n","Iteration 4765: Training Loss = 52.3447151184082 Training Accuracy = 0.06422117352485657\n","Iteration 4766: Training Loss = 52.36397171020508 Training Accuracy = 0.06464657932519913\n","Iteration 4767: Training Loss = 52.38844299316406 Training Accuracy = 0.0644386038184166\n","Iteration 4768: Training Loss = 52.45642852783203 Training Accuracy = 0.0645160898566246\n","Iteration 4769: Training Loss = 52.532981872558594 Training Accuracy = 0.06478988379240036\n","Iteration 4770: Training Loss = 52.74787902832031 Training Accuracy = 0.06412752717733383\n","Iteration 4771: Training Loss = 52.85968780517578 Training Accuracy = 0.06539381295442581\n","Iteration 4772: Training Loss = 53.242000579833984 Training Accuracy = 0.06348369270563126\n","Iteration 4773: Training Loss = 53.090152740478516 Training Accuracy = 0.06587393581867218\n","Iteration 4774: Training Loss = 53.163753509521484 Training Accuracy = 0.0630994662642479\n","Iteration 4775: Training Loss = 52.78545379638672 Training Accuracy = 0.06537215411663055\n","Iteration 4776: Training Loss = 52.592674255371094 Training Accuracy = 0.06315051019191742\n","Iteration 4777: Training Loss = 52.40169906616211 Training Accuracy = 0.06410318613052368\n","Iteration 4778: Training Loss = 52.32320022583008 Training Accuracy = 0.06333231180906296\n","Iteration 4779: Training Loss = 52.282615661621094 Training Accuracy = 0.06344933807849884\n","Iteration 4780: Training Loss = 52.27012252807617 Training Accuracy = 0.06373749673366547\n","Iteration 4781: Training Loss = 52.26910400390625 Training Accuracy = 0.06342648714780807\n","Iteration 4782: Training Loss = 52.280120849609375 Training Accuracy = 0.06415002048015594\n","Iteration 4783: Training Loss = 52.30885314941406 Training Accuracy = 0.06348739564418793\n","Iteration 4784: Training Loss = 52.35661697387695 Training Accuracy = 0.0645187720656395\n","Iteration 4785: Training Loss = 52.44579315185547 Training Accuracy = 0.06341532617807388\n","Iteration 4786: Training Loss = 52.51335906982422 Training Accuracy = 0.06486883759498596\n","Iteration 4787: Training Loss = 52.65594482421875 Training Accuracy = 0.06336098909378052\n","Iteration 4788: Training Loss = 52.65789031982422 Training Accuracy = 0.06506139785051346\n","Iteration 4789: Training Loss = 52.77322769165039 Training Accuracy = 0.06338250637054443\n","Iteration 4790: Training Loss = 52.68067932128906 Training Accuracy = 0.06501217931509018\n","Iteration 4791: Training Loss = 52.706295013427734 Training Accuracy = 0.06336554139852524\n","Iteration 4792: Training Loss = 52.56597137451172 Training Accuracy = 0.06471271067857742\n","Iteration 4793: Training Loss = 52.519290924072266 Training Accuracy = 0.06344205141067505\n","Iteration 4794: Training Loss = 52.41604995727539 Training Accuracy = 0.06432021409273148\n","Iteration 4795: Training Loss = 52.378780364990234 Training Accuracy = 0.06358864903450012\n","Iteration 4796: Training Loss = 52.3312873840332 Training Accuracy = 0.0640413910150528\n","Iteration 4797: Training Loss = 52.315372467041016 Training Accuracy = 0.0636749342083931\n","Iteration 4798: Training Loss = 52.29048156738281 Training Accuracy = 0.06408137828111649\n","Iteration 4799: Training Loss = 52.28099060058594 Training Accuracy = 0.06368257105350494\n","Iteration 4800: Training Loss = 52.27152633666992 Training Accuracy = 0.06435342133045197\n","Iteration 4801: Training Loss = 52.27779769897461 Training Accuracy = 0.06365325301885605\n","Iteration 4802: Training Loss = 52.29088592529297 Training Accuracy = 0.06468641757965088\n","Iteration 4803: Training Loss = 52.31696701049805 Training Accuracy = 0.06362920999526978\n","Iteration 4804: Training Loss = 52.343021392822266 Training Accuracy = 0.06497634202241898\n","Iteration 4805: Training Loss = 52.385189056396484 Training Accuracy = 0.06368705630302429\n","Iteration 4806: Training Loss = 52.42050552368164 Training Accuracy = 0.06516298651695251\n","Iteration 4807: Training Loss = 52.51947021484375 Training Accuracy = 0.06374461948871613\n","Iteration 4808: Training Loss = 52.56759262084961 Training Accuracy = 0.06527108699083328\n","Iteration 4809: Training Loss = 52.73572540283203 Training Accuracy = 0.06378252804279327\n","Iteration 4810: Training Loss = 52.69879150390625 Training Accuracy = 0.0653005987405777\n","Iteration 4811: Training Loss = 52.813777923583984 Training Accuracy = 0.0637371838092804\n","Iteration 4812: Training Loss = 52.66352081298828 Training Accuracy = 0.06508871167898178\n","Iteration 4813: Training Loss = 52.65625762939453 Training Accuracy = 0.06365709751844406\n","Iteration 4814: Training Loss = 52.500152587890625 Training Accuracy = 0.06475149095058441\n","Iteration 4815: Training Loss = 52.43965148925781 Training Accuracy = 0.06366024911403656\n","Iteration 4816: Training Loss = 52.34538269042969 Training Accuracy = 0.0643944963812828\n","Iteration 4817: Training Loss = 52.30785369873047 Training Accuracy = 0.06372640281915665\n","Iteration 4818: Training Loss = 52.26976013183594 Training Accuracy = 0.06417251378297806\n","Iteration 4819: Training Loss = 52.25602340698242 Training Accuracy = 0.06378983706235886\n","Iteration 4820: Training Loss = 52.239253997802734 Training Accuracy = 0.064299575984478\n","Iteration 4821: Training Loss = 52.23419952392578 Training Accuracy = 0.06384077668190002\n","Iteration 4822: Training Loss = 52.230709075927734 Training Accuracy = 0.0646003857254982\n","Iteration 4823: Training Loss = 52.2391471862793 Training Accuracy = 0.06383352726697922\n","Iteration 4824: Training Loss = 52.253089904785156 Training Accuracy = 0.06493671238422394\n","Iteration 4825: Training Loss = 52.27064895629883 Training Accuracy = 0.06383562833070755\n","Iteration 4826: Training Loss = 52.28670883178711 Training Accuracy = 0.06514980643987656\n","Iteration 4827: Training Loss = 52.313133239746094 Training Accuracy = 0.06387965381145477\n","Iteration 4828: Training Loss = 52.34202575683594 Training Accuracy = 0.06524863839149475\n","Iteration 4829: Training Loss = 52.426185607910156 Training Accuracy = 0.06397327780723572\n","Iteration 4830: Training Loss = 52.484683990478516 Training Accuracy = 0.06538144499063492\n","Iteration 4831: Training Loss = 52.660438537597656 Training Accuracy = 0.06396400928497314\n","Iteration 4832: Training Loss = 52.66282653808594 Training Accuracy = 0.06549283862113953\n","Iteration 4833: Training Loss = 52.831520080566406 Training Accuracy = 0.06388507783412933\n","Iteration 4834: Training Loss = 52.67961502075195 Training Accuracy = 0.06542094051837921\n","Iteration 4835: Training Loss = 52.687400817871094 Training Accuracy = 0.0638316348195076\n","Iteration 4836: Training Loss = 52.485469818115234 Training Accuracy = 0.06505850702524185\n","Iteration 4837: Training Loss = 52.40337371826172 Training Accuracy = 0.06383966654539108\n","Iteration 4838: Training Loss = 52.28367233276367 Training Accuracy = 0.06459436565637589\n","Iteration 4839: Training Loss = 52.22966384887695 Training Accuracy = 0.06391771137714386\n","Iteration 4840: Training Loss = 52.18279266357422 Training Accuracy = 0.06430860608816147\n","Iteration 4841: Training Loss = 52.16192626953125 Training Accuracy = 0.06401900202035904\n","Iteration 4842: Training Loss = 52.14535140991211 Training Accuracy = 0.06438250094652176\n","Iteration 4843: Training Loss = 52.13648986816406 Training Accuracy = 0.06411171704530716\n","Iteration 4844: Training Loss = 52.13100814819336 Training Accuracy = 0.0646432414650917\n","Iteration 4845: Training Loss = 52.1314697265625 Training Accuracy = 0.06416681408882141\n","Iteration 4846: Training Loss = 52.13772964477539 Training Accuracy = 0.06492838263511658\n","Iteration 4847: Training Loss = 52.146324157714844 Training Accuracy = 0.06420112401247025\n","Iteration 4848: Training Loss = 52.15401077270508 Training Accuracy = 0.06517846882343292\n","Iteration 4849: Training Loss = 52.15251159667969 Training Accuracy = 0.06430478394031525\n","Iteration 4850: Training Loss = 52.14788055419922 Training Accuracy = 0.0653061643242836\n","Iteration 4851: Training Loss = 52.14078903198242 Training Accuracy = 0.06448286771774292\n","Iteration 4852: Training Loss = 52.13861846923828 Training Accuracy = 0.06535312533378601\n","Iteration 4853: Training Loss = 52.144378662109375 Training Accuracy = 0.06463514268398285\n","Iteration 4854: Training Loss = 52.156776428222656 Training Accuracy = 0.06541062891483307\n","Iteration 4855: Training Loss = 52.18967819213867 Training Accuracy = 0.0646284893155098\n","Iteration 4856: Training Loss = 52.23695755004883 Training Accuracy = 0.06566985696554184\n","Iteration 4857: Training Loss = 52.356807708740234 Training Accuracy = 0.06442912667989731\n","Iteration 4858: Training Loss = 52.486122131347656 Training Accuracy = 0.06607678532600403\n","Iteration 4859: Training Loss = 52.813262939453125 Training Accuracy = 0.06409189850091934\n","Iteration 4860: Training Loss = 52.890533447265625 Training Accuracy = 0.06642327457666397\n","Iteration 4861: Training Loss = 53.240203857421875 Training Accuracy = 0.06395144760608673\n","Iteration 4862: Training Loss = 52.85811233520508 Training Accuracy = 0.06610618531703949\n","Iteration 4863: Training Loss = 52.74370574951172 Training Accuracy = 0.06409589946269989\n","Iteration 4864: Training Loss = 52.40489196777344 Training Accuracy = 0.06500633805990219\n","Iteration 4865: Training Loss = 52.26242446899414 Training Accuracy = 0.06417419016361237\n","Iteration 4866: Training Loss = 52.15978240966797 Training Accuracy = 0.06408120691776276\n","Iteration 4867: Training Loss = 52.12017059326172 Training Accuracy = 0.0643855631351471\n","Iteration 4868: Training Loss = 52.10404968261719 Training Accuracy = 0.06378050893545151\n","Iteration 4869: Training Loss = 52.104854583740234 Training Accuracy = 0.06464160233736038\n","Iteration 4870: Training Loss = 52.123748779296875 Training Accuracy = 0.06395731121301651\n","Iteration 4871: Training Loss = 52.164363861083984 Training Accuracy = 0.0648854598402977\n","Iteration 4872: Training Loss = 52.24727249145508 Training Accuracy = 0.0640910267829895\n","Iteration 4873: Training Loss = 52.31711959838867 Training Accuracy = 0.06519893556833267\n","Iteration 4874: Training Loss = 52.4648323059082 Training Accuracy = 0.06401105225086212\n","Iteration 4875: Training Loss = 52.47224426269531 Training Accuracy = 0.06547185033559799\n","Iteration 4876: Training Loss = 52.59087371826172 Training Accuracy = 0.06396590173244476\n","Iteration 4877: Training Loss = 52.49208450317383 Training Accuracy = 0.06546035408973694\n","Iteration 4878: Training Loss = 52.50619125366211 Training Accuracy = 0.06390184909105301\n","Iteration 4879: Training Loss = 52.365474700927734 Training Accuracy = 0.0651925578713417\n","Iteration 4880: Training Loss = 52.31147003173828 Training Accuracy = 0.06393133848905563\n","Iteration 4881: Training Loss = 52.212894439697266 Training Accuracy = 0.06490765511989594\n","Iteration 4882: Training Loss = 52.17410659790039 Training Accuracy = 0.06403765082359314\n","Iteration 4883: Training Loss = 52.13372802734375 Training Accuracy = 0.06468873471021652\n","Iteration 4884: Training Loss = 52.11952209472656 Training Accuracy = 0.06411430984735489\n","Iteration 4885: Training Loss = 52.0992317199707 Training Accuracy = 0.0647115632891655\n","Iteration 4886: Training Loss = 52.09033203125 Training Accuracy = 0.06417861580848694\n","Iteration 4887: Training Loss = 52.081817626953125 Training Accuracy = 0.0648820549249649\n","Iteration 4888: Training Loss = 52.08432388305664 Training Accuracy = 0.06418546289205551\n","Iteration 4889: Training Loss = 52.09290313720703 Training Accuracy = 0.06515354663133621\n","Iteration 4890: Training Loss = 52.1063117980957 Training Accuracy = 0.06419283896684647\n","Iteration 4891: Training Loss = 52.119224548339844 Training Accuracy = 0.06543968617916107\n","Iteration 4892: Training Loss = 52.13959503173828 Training Accuracy = 0.06426709145307541\n","Iteration 4893: Training Loss = 52.16550064086914 Training Accuracy = 0.06560824811458588\n","Iteration 4894: Training Loss = 52.23406982421875 Training Accuracy = 0.0643608421087265\n","Iteration 4895: Training Loss = 52.29945755004883 Training Accuracy = 0.06575197726488113\n","Iteration 4896: Training Loss = 52.47504425048828 Training Accuracy = 0.06434816867113113\n","Iteration 4897: Training Loss = 52.5240592956543 Training Accuracy = 0.06591371446847916\n","Iteration 4898: Training Loss = 52.745975494384766 Training Accuracy = 0.06419157981872559\n","Iteration 4899: Training Loss = 52.62166976928711 Training Accuracy = 0.06595201790332794\n","Iteration 4900: Training Loss = 52.67827606201172 Training Accuracy = 0.06409113854169846\n","Iteration 4901: Training Loss = 52.44874572753906 Training Accuracy = 0.06553009897470474\n","Iteration 4902: Training Loss = 52.3623161315918 Training Accuracy = 0.06408439576625824\n","Iteration 4903: Training Loss = 52.20743179321289 Training Accuracy = 0.06488800793886185\n","Iteration 4904: Training Loss = 52.140811920166016 Training Accuracy = 0.06421808898448944\n","Iteration 4905: Training Loss = 52.08342361450195 Training Accuracy = 0.0644468292593956\n","Iteration 4906: Training Loss = 52.058563232421875 Training Accuracy = 0.06443877518177032\n","Iteration 4907: Training Loss = 52.03823471069336 Training Accuracy = 0.0643572062253952\n","Iteration 4908: Training Loss = 52.02577209472656 Training Accuracy = 0.06466662138700485\n","Iteration 4909: Training Loss = 52.01536178588867 Training Accuracy = 0.06449691951274872\n","Iteration 4910: Training Loss = 52.0108757019043 Training Accuracy = 0.06493494659662247\n","Iteration 4911: Training Loss = 52.012264251708984 Training Accuracy = 0.06458418071269989\n","Iteration 4912: Training Loss = 52.01976013183594 Training Accuracy = 0.06521022319793701\n","Iteration 4913: Training Loss = 52.031532287597656 Training Accuracy = 0.06461845338344574\n","Iteration 4914: Training Loss = 52.0455207824707 Training Accuracy = 0.06547510623931885\n","Iteration 4915: Training Loss = 52.06454086303711 Training Accuracy = 0.06463517993688583\n","Iteration 4916: Training Loss = 52.08381652832031 Training Accuracy = 0.06573237478733063\n","Iteration 4917: Training Loss = 52.11955261230469 Training Accuracy = 0.0645531639456749\n","Iteration 4918: Training Loss = 52.159950256347656 Training Accuracy = 0.0660092681646347\n","Iteration 4919: Training Loss = 52.254756927490234 Training Accuracy = 0.06447955220937729\n","Iteration 4920: Training Loss = 52.34373474121094 Training Accuracy = 0.066271111369133\n","Iteration 4921: Training Loss = 52.58568572998047 Training Accuracy = 0.06442601978778839\n","Iteration 4922: Training Loss = 52.635982513427734 Training Accuracy = 0.06634591519832611\n","Iteration 4923: Training Loss = 52.91360855102539 Training Accuracy = 0.06443092972040176\n","Iteration 4924: Training Loss = 52.653568267822266 Training Accuracy = 0.06607077270746231\n","Iteration 4925: Training Loss = 52.61136245727539 Training Accuracy = 0.06442604213953018\n","Iteration 4926: Training Loss = 52.323486328125 Training Accuracy = 0.06532032042741776\n","Iteration 4927: Training Loss = 52.19773483276367 Training Accuracy = 0.06446433067321777\n","Iteration 4928: Training Loss = 52.079200744628906 Training Accuracy = 0.06455125659704208\n","Iteration 4929: Training Loss = 52.02486801147461 Training Accuracy = 0.06464022397994995\n","Iteration 4930: Training Loss = 51.993186950683594 Training Accuracy = 0.06428101658821106\n","Iteration 4931: Training Loss = 51.97883987426758 Training Accuracy = 0.06485684216022491\n","Iteration 4932: Training Loss = 51.97718048095703 Training Accuracy = 0.06440930813550949\n","Iteration 4933: Training Loss = 51.98971176147461 Training Accuracy = 0.06506237387657166\n","Iteration 4934: Training Loss = 52.02014923095703 Training Accuracy = 0.06459540873765945\n","Iteration 4935: Training Loss = 52.06174087524414 Training Accuracy = 0.06537480652332306\n","Iteration 4936: Training Loss = 52.143829345703125 Training Accuracy = 0.06459241360425949\n","Iteration 4937: Training Loss = 52.209320068359375 Training Accuracy = 0.06576181948184967\n","Iteration 4938: Training Loss = 52.36324691772461 Training Accuracy = 0.06443080306053162\n","Iteration 4939: Training Loss = 52.395877838134766 Training Accuracy = 0.06604506820440292\n","Iteration 4940: Training Loss = 52.55724334716797 Training Accuracy = 0.06425675749778748\n","Iteration 4941: Training Loss = 52.46177673339844 Training Accuracy = 0.06611868739128113\n","Iteration 4942: Training Loss = 52.48981857299805 Training Accuracy = 0.06420110911130905\n","Iteration 4943: Training Loss = 52.30842971801758 Training Accuracy = 0.06578909605741501\n","Iteration 4944: Training Loss = 52.237606048583984 Training Accuracy = 0.06426654011011124\n","Iteration 4945: Training Loss = 52.11566162109375 Training Accuracy = 0.0652855634689331\n","Iteration 4946: Training Loss = 52.063201904296875 Training Accuracy = 0.06437361985445023\n","Iteration 4947: Training Loss = 52.011322021484375 Training Accuracy = 0.06495098024606705\n","Iteration 4948: Training Loss = 51.986572265625 Training Accuracy = 0.0645541250705719\n","Iteration 4949: Training Loss = 51.96397399902344 Training Accuracy = 0.06488907337188721\n","Iteration 4950: Training Loss = 51.95072555541992 Training Accuracy = 0.06478811800479889\n","Iteration 4951: Training Loss = 51.94038772583008 Training Accuracy = 0.0650501474738121\n","Iteration 4952: Training Loss = 51.934898376464844 Training Accuracy = 0.06498852372169495\n","Iteration 4953: Training Loss = 51.932376861572266 Training Accuracy = 0.06524829566478729\n","Iteration 4954: Training Loss = 51.931739807128906 Training Accuracy = 0.06517969816923141\n","Iteration 4955: Training Loss = 51.93135452270508 Training Accuracy = 0.06541889905929565\n","Iteration 4956: Training Loss = 51.931251525878906 Training Accuracy = 0.06534797698259354\n","Iteration 4957: Training Loss = 51.93166732788086 Training Accuracy = 0.06556175649166107\n","Iteration 4958: Training Loss = 51.93504333496094 Training Accuracy = 0.06550348550081253\n","Iteration 4959: Training Loss = 51.943084716796875 Training Accuracy = 0.06570089608430862\n","Iteration 4960: Training Loss = 51.96494674682617 Training Accuracy = 0.06560786813497543\n","Iteration 4961: Training Loss = 52.002159118652344 Training Accuracy = 0.06586422026157379\n","Iteration 4962: Training Loss = 52.08952331542969 Training Accuracy = 0.06564266234636307\n","Iteration 4963: Training Loss = 52.198150634765625 Training Accuracy = 0.06607948243618011\n","Iteration 4964: Training Loss = 52.45876693725586 Training Accuracy = 0.06554491817951202\n","Iteration 4965: Training Loss = 52.57821273803711 Training Accuracy = 0.06639967858791351\n","Iteration 4966: Training Loss = 52.95691680908203 Training Accuracy = 0.06519876420497894\n","Iteration 4967: Training Loss = 52.69744110107422 Training Accuracy = 0.066631019115448\n","Iteration 4968: Training Loss = 52.7010498046875 Training Accuracy = 0.06473781913518906\n","Iteration 4969: Training Loss = 52.355716705322266 Training Accuracy = 0.06634368747472763\n","Iteration 4970: Training Loss = 52.226646423339844 Training Accuracy = 0.06430159509181976\n","Iteration 4971: Training Loss = 52.102943420410156 Training Accuracy = 0.06578592956066132\n","Iteration 4972: Training Loss = 52.04581069946289 Training Accuracy = 0.06415580213069916\n","Iteration 4973: Training Loss = 52.003517150878906 Training Accuracy = 0.06532791256904602\n","Iteration 4974: Training Loss = 51.97208023071289 Training Accuracy = 0.06440973281860352\n","Iteration 4975: Training Loss = 51.94776916503906 Training Accuracy = 0.0652153268456459\n","Iteration 4976: Training Loss = 51.928794860839844 Training Accuracy = 0.06487508863210678\n","Iteration 4977: Training Loss = 51.91793441772461 Training Accuracy = 0.0652497261762619\n","Iteration 4978: Training Loss = 51.91710662841797 Training Accuracy = 0.06531069427728653\n","Iteration 4979: Training Loss = 51.93617630004883 Training Accuracy = 0.06513042002916336\n","Iteration 4980: Training Loss = 51.968013763427734 Training Accuracy = 0.06567242741584778\n","Iteration 4981: Training Loss = 52.031585693359375 Training Accuracy = 0.0650433897972107\n","Iteration 4982: Training Loss = 52.08640670776367 Training Accuracy = 0.06606564670801163\n","Iteration 4983: Training Loss = 52.21907043457031 Training Accuracy = 0.0648672804236412\n","Iteration 4984: Training Loss = 52.287109375 Training Accuracy = 0.06651109457015991\n","Iteration 4985: Training Loss = 52.47867202758789 Training Accuracy = 0.06455846130847931\n","Iteration 4986: Training Loss = 52.45069885253906 Training Accuracy = 0.06678375601768494\n","Iteration 4987: Training Loss = 52.542423248291016 Training Accuracy = 0.06442544609308243\n","Iteration 4988: Training Loss = 52.3565673828125 Training Accuracy = 0.06638084352016449\n","Iteration 4989: Training Loss = 52.310455322265625 Training Accuracy = 0.06463741511106491\n","Iteration 4990: Training Loss = 52.145347595214844 Training Accuracy = 0.06558966636657715\n","Iteration 4991: Training Loss = 52.086219787597656 Training Accuracy = 0.064904123544693\n","Iteration 4992: Training Loss = 52.00385665893555 Training Accuracy = 0.06507933884859085\n","Iteration 4993: Training Loss = 51.967742919921875 Training Accuracy = 0.06508269906044006\n","Iteration 4994: Training Loss = 51.930397033691406 Training Accuracy = 0.0649786964058876\n","Iteration 4995: Training Loss = 51.9090690612793 Training Accuracy = 0.06528452038764954\n","Iteration 4996: Training Loss = 51.89268493652344 Training Accuracy = 0.06510565429925919\n","Iteration 4997: Training Loss = 51.88362503051758 Training Accuracy = 0.06551998108625412\n","Iteration 4998: Training Loss = 51.879600524902344 Training Accuracy = 0.06522568315267563\n","Iteration 4999: Training Loss = 51.879390716552734 Training Accuracy = 0.06575140357017517\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x288 with 30 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Execution Time:  294.21341311599997\n"],"name":"stdout"}]}]}