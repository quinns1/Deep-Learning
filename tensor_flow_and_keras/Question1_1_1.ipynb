{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Question1_1_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MijiOx0RLMn6","outputId":"ab8dc9f2-2966-4a75-eda2-b5d4ca2f408b"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Name: Shane Quinn\n","Student Number: R00144107\n","Email: shane.quinn1@mycit.ie\n","Course: MSc Artificial Intelligence\n","Module: Deep Learning\n","Date: 03/04/2021\n","\"\"\"\n","\n","import tensorflow as tf\n","from tensorflow.python.client import device_lib\n","from keras.utils import np_utils\n","import matplotlib.pyplot as plt\n","import functools\n","import time\n","\n","\n","def forward_pass(X, w1, w2, b1, b2):\n","    \"\"\"\n","    Push feature data through neural network. Returns 10 class probabilities for all feature instances\n","\n","    Parameters\n","    ----------\n","    X : tf.Variable\n","        Pre-processed input data.\n","    w1 : tf.Variable\n","        Layer 1 learnable weights.\n","    w2 : tf.Variable\n","        Layer 2 learnable weights.\n","    b1 : tf.Variable\n","        Layer 1 bias.\n","    b2 : tf.Variable\n","        Layer 2 bias.\n","\n","    Returns\n","    -------\n","    H : tf.Variable\n","        Softmax layer output predicted probability of each class.\n","\n","    \"\"\"\n","\n","    #Layer 1 - 200 Relu Neurons\n","    A = tf.matmul(w1, tf.transpose(X)) + b1                    #A1 = x1.X + b1\n","    H = tf.keras.activations.relu(A)                           #H1 = act(A1)\n","    #Layer 2 - Softmax Layer\n","    A = tf.matmul(w2, H)+b2                                    #A2 = w2.H1 + b2\n","    #Softmax = (e^A2)/sum(A2)\n","    H = tf.exp(A) / tf.reduce_sum(tf.exp(A), axis=0)           #H2 = act(A2)\n","    \n","    return H\n","    \n","\n","def cross_entropy(pred_y, y):\n","    \"\"\"\n","    Take in softmax probabilities (output of forward_pass) and true class labels and calculate cross entropy loss\n","\n","    Parameters\n","    ----------\n","    pred_y : tf.Variable\n","        Predictions (Output of softmax layer in forward_pass()).\n","    y : tf.Variable\n","        One-hot encoded true class labels.\n","\n","    Returns\n","    -------\n","    cross_ent : Cross entropy loss\n","        tf.Variable.\n","\n","    \"\"\"\n","    \n","    #Cross entropy loss per class = -sum((True class encoded values)*log(predicted probabilities))     \n","    a = -tf.reduce_sum(y * tf.math.log(pred_y), axis=0)\n","    #Cross entropy loss = mean of all losses calculated above.\n","    cross_ent = tf.reduce_mean(a, axis=0)\n","    \n","    return cross_ent\n","\n","\n","\n","def calculate_accuracy(pred_y, y, datatype=tf.float32):\n","    \"\"\"\n","    Calculate the model accuracy given predicted probabilities and true class labels\n","\n","    Parameters\n","    ----------\n","    pred_y : tf.Variable\n","        Predicted class probabilities, output of forward pass/softmax layer.\n","    y : tf.Variable\n","        True class values.\n","    datatype : tf.float32/tf.float64, optional\n","        One of the above tf datatypes. The default is tf.float32.\n","\n","    Returns\n","    -------\n","    accuracy : float32\n","        Model Accuracy.\n","\n","    \"\"\"\n","    \n","    # Convert predicted probabilities to 0 or 1\n","    pred_y = tf.round(pred_y)\n","    # Boolean True (1) if prediction is correct, cast to tf.Variable\n","    predictions = tf.cast(tf.equal(pred_y, y), datatype)\n","    #Mean value of correct predictions\n","    accuracy = tf.reduce_mean(predictions)\n","\n","    return accuracy\n","\n","\n","def exec_time(func):\n","    \"\"\"\n","    Generic Execution time recorder, pass in function. Records execution time using decorators\n","\n","    Parameters\n","    ----------\n","    func : FUNCTION\n","        Function we're recording and printing execution time of.\n","    \"\"\"\n","    \n","    @functools.wraps(func)\n","    def record_exec_time(*args, **kwargs):\n","        start_time = time.perf_counter()\n","        mn = func(*args, **kwargs)\n","        execution_time = time.perf_counter() - start_time\n","        print(\"Execution Time: \", execution_time)\n","        return mn\n","\n","    return record_exec_time\n","\n","@exec_time \n","def main():\n","      \n","    #Retrieve feature data/class labels\n","    X, y, X_val, y_val = pre_process() \n","\n","    #Initialise Learning rate and iterations.\n","    learning_rate = 0.05\n","    iterations = 1000\n","    datatype = tf.float64\n","    \n","    #Initialise lists for saving accuracies/loss\n","    te_acc = []\n","    tr_acc = []\n","    te_loss = []\n","    tr_loss = []\n","    \n","    # Create tf variables from data\n","    X = tf.cast(X, datatype)\n","    y = tf.cast(y, datatype)\n","    X_val = tf.cast(X_val, datatype)\n","    y_val = tf.cast(y_val, datatype)\n","    \n","    #Initialise Adam Optimizer\n","    adam = tf.keras.optimizers.Adam()\n","    \n","    #Initialise weights and bias\n","    zeros = tf.zeros_initializer()\n","    layer1_weights = tf.Variable(tf.random.normal([200,784], stddev=0.05, dtype=datatype))\n","    layer2_weights = tf.Variable(tf.random.normal([10, 200], stddev=0.05, dtype=datatype))\n","    layer1_bias = tf.Variable(0, dtype=datatype)\n","    layer2_bias = tf.Variable(0, dtype=datatype)\n","    \n","    #Repeat gradient descent loop 'iterations' times\n","    for i in range(iterations): \n","        \n","        with tf.GradientTape() as tape:\n","            #Create instance of gradient tape to record forward pass and calculate gradients for learnable weights and biases \n","            pred_y = forward_pass(X, layer1_weights, layer2_weights, layer1_bias, layer2_bias)\n","            loss = cross_entropy(pred_y, y)\n","        \n","        tr_loss.append(loss)\n","        gradients = tape.gradient(loss, [layer1_weights, layer2_weights, layer1_bias, layer2_bias])     #Calculate gradients \n","        accuracy = calculate_accuracy(pred_y, y, datatype)                                              #Calculate accuracy of model\n","        tr_acc.append(accuracy)\n","        print(\"Iteration {}: Training Loss = {} Training Accuracy = {}\".format(i, loss.numpy(), accuracy.numpy()))\n","        \n","        #Apply gradients using adaptive movement estimation, see accompanied report for more details\n","        adam.apply_gradients(zip(gradients, [layer1_weights, layer2_weights, layer1_bias, layer2_bias]))\n","        \n","        #Test model on validation data\n","        test_pred_y = forward_pass(X_val, layer1_weights, layer2_weights, layer1_bias, layer2_bias)             \n","        test_loss = cross_entropy(test_pred_y, y_val)\n","        te_loss.append(test_loss)\n","        te_acc.append(calculate_accuracy(test_pred_y, y_val, datatype))\n","    \n","    plt.title(\"Question1_1_1\")\n","    plt.plot(te_loss, label=\"Validation Loss\")\n","    plt.plot(tr_loss, label=\"Train Loss\")\n","    plt.plot(te_acc, label=\"Validation Accuracy\")\n","    plt.plot(tr_acc, label=\"Train Accuracy\")\n","    plt.ylim((0,1))    \n","    plt.legend()\n","    plt.show()\n","    \n","\n","\n","def pre_process():\n","    \"\"\"\n","    Supplied Code for pre-processing Fashion MNIST dataset. Returns target class values and training data for training and validation\n","\n","    Returns\n","    -------\n","    tr_x : NUMPY N-D ARRAY\n","        X Training Data.\n","    tr_y : NUMPY N-D ARRAY\n","        y target class values 1 hot encoded (training data).\n","    te_x : NUMPY N-D ARRAY\n","        X Test Data.\n","    te_y : NUMPY N-D ARRAY\n","        y test target class values 1 hot encoded (test data).\n","\n","    \"\"\"\n","    \n","    fashion_mnist = tf.keras.datasets.fashion_mnist \n","    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n","    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n","    te_x = te_x.reshape(te_x.shape[0], 784)\n","    tr_x = tr_x / 255.0\n","    te_x = te_x / 255.0\n","    tr_y = np_utils.to_categorical(tr_y,10)\n","    tr_y = tr_y.T\n","    te_y = np_utils.to_categorical(te_y,10)\n","    te_y = te_y.T\n","\n","    return tr_x, tr_y, te_x, te_y\n","\n","\n","\n","if __name__ == '__main__':\n","    print(\"Local Devices: \\n\", device_lib.list_local_devices())\n","    main()\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Local Devices: \n"," [name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 16563406014209350229\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 14674281152\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 4312557326448041465\n","physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n","]\n","Iteration 0: Training Loss = 2.3793712373918843 Training Accuracy = 0.9\n","Iteration 1: Training Loss = 2.1621116195462626 Training Accuracy = 0.9\n","Iteration 2: Training Loss = 1.9879881180231755 Training Accuracy = 0.9\n","Iteration 3: Training Loss = 1.8375567253424863 Training Accuracy = 0.9000033333333334\n","Iteration 4: Training Loss = 1.699264798850866 Training Accuracy = 0.9005933333333334\n","Iteration 5: Training Loss = 1.5707960580403888 Training Accuracy = 0.9029833333333334\n","Iteration 6: Training Loss = 1.4539069589961533 Training Accuracy = 0.9056\n","Iteration 7: Training Loss = 1.3499762986393962 Training Accuracy = 0.9083683333333333\n","Iteration 8: Training Loss = 1.259358542244401 Training Accuracy = 0.914455\n","Iteration 9: Training Loss = 1.1809303924707997 Training Accuracy = 0.9220966666666667\n","Iteration 10: Training Loss = 1.1125285297735583 Training Accuracy = 0.9281433333333333\n","Iteration 11: Training Loss = 1.0529426337585435 Training Accuracy = 0.93244\n","Iteration 12: Training Loss = 1.0013601360180422 Training Accuracy = 0.9354916666666667\n","Iteration 13: Training Loss = 0.9568391296024803 Training Accuracy = 0.9378316666666666\n","Iteration 14: Training Loss = 0.9185443932977353 Training Accuracy = 0.939735\n","Iteration 15: Training Loss = 0.8858580935785593 Training Accuracy = 0.94147\n","Iteration 16: Training Loss = 0.8579244748139064 Training Accuracy = 0.94276\n","Iteration 17: Training Loss = 0.8335905602525673 Training Accuracy = 0.9437433333333334\n","Iteration 18: Training Loss = 0.8119988778009894 Training Accuracy = 0.944565\n","Iteration 19: Training Loss = 0.7925628522655805 Training Accuracy = 0.945555\n","Iteration 20: Training Loss = 0.7749956049931017 Training Accuracy = 0.9466883333333334\n","Iteration 21: Training Loss = 0.759210574265712 Training Accuracy = 0.9477883333333333\n","Iteration 22: Training Loss = 0.7448625408384149 Training Accuracy = 0.9485583333333333\n","Iteration 23: Training Loss = 0.7316018830516365 Training Accuracy = 0.94926\n","Iteration 24: Training Loss = 0.7191303129082846 Training Accuracy = 0.9501083333333333\n","Iteration 25: Training Loss = 0.7073906212219268 Training Accuracy = 0.9508916666666667\n","Iteration 26: Training Loss = 0.6964017045820476 Training Accuracy = 0.9516616666666666\n","Iteration 27: Training Loss = 0.6859781957981574 Training Accuracy = 0.9524416666666666\n","Iteration 28: Training Loss = 0.6761102438606402 Training Accuracy = 0.95336\n","Iteration 29: Training Loss = 0.666742285994304 Training Accuracy = 0.95423\n","Iteration 30: Training Loss = 0.6578279877809182 Training Accuracy = 0.95505\n","Iteration 31: Training Loss = 0.649217490111741 Training Accuracy = 0.9556683333333333\n","Iteration 32: Training Loss = 0.6409817265881371 Training Accuracy = 0.9561866666666666\n","Iteration 33: Training Loss = 0.6331859503713589 Training Accuracy = 0.956725\n","Iteration 34: Training Loss = 0.6257791936972011 Training Accuracy = 0.9572766666666667\n","Iteration 35: Training Loss = 0.6186523147330734 Training Accuracy = 0.95784\n","Iteration 36: Training Loss = 0.6118315402931546 Training Accuracy = 0.9584533333333334\n","Iteration 37: Training Loss = 0.6053140454664437 Training Accuracy = 0.95904\n","Iteration 38: Training Loss = 0.5990653275454995 Training Accuracy = 0.959625\n","Iteration 39: Training Loss = 0.5930720911024009 Training Accuracy = 0.9600716666666667\n","Iteration 40: Training Loss = 0.5873536710462156 Training Accuracy = 0.9604416666666666\n","Iteration 41: Training Loss = 0.5818501702496199 Training Accuracy = 0.96087\n","Iteration 42: Training Loss = 0.5765636273035153 Training Accuracy = 0.9612366666666666\n","Iteration 43: Training Loss = 0.5714942114382134 Training Accuracy = 0.96159\n","Iteration 44: Training Loss = 0.566630065941189 Training Accuracy = 0.9619316666666666\n","Iteration 45: Training Loss = 0.5619603269517783 Training Accuracy = 0.9622716666666666\n","Iteration 46: Training Loss = 0.5574911849469316 Training Accuracy = 0.9625333333333334\n","Iteration 47: Training Loss = 0.55318330677651 Training Accuracy = 0.9627866666666667\n","Iteration 48: Training Loss = 0.5490341258329228 Training Accuracy = 0.9630566666666667\n","Iteration 49: Training Loss = 0.5450449927194885 Training Accuracy = 0.963365\n","Iteration 50: Training Loss = 0.5411833480689509 Training Accuracy = 0.96364\n","Iteration 51: Training Loss = 0.5374480969875653 Training Accuracy = 0.9639533333333333\n","Iteration 52: Training Loss = 0.5338422447685349 Training Accuracy = 0.96423\n","Iteration 53: Training Loss = 0.5303480841227668 Training Accuracy = 0.9644516666666667\n","Iteration 54: Training Loss = 0.5269616606356213 Training Accuracy = 0.964705\n","Iteration 55: Training Loss = 0.5236714910221485 Training Accuracy = 0.9649183333333333\n","Iteration 56: Training Loss = 0.5204774264601612 Training Accuracy = 0.9651566666666667\n","Iteration 57: Training Loss = 0.5173857913823675 Training Accuracy = 0.9653433333333333\n","Iteration 58: Training Loss = 0.5143786621025598 Training Accuracy = 0.9655433333333333\n","Iteration 59: Training Loss = 0.5114576028616097 Training Accuracy = 0.9657333333333333\n","Iteration 60: Training Loss = 0.5086291667028244 Training Accuracy = 0.9658916666666667\n","Iteration 61: Training Loss = 0.5058803280494204 Training Accuracy = 0.9661\n","Iteration 62: Training Loss = 0.5032035260743102 Training Accuracy = 0.9662283333333334\n","Iteration 63: Training Loss = 0.5005950111959431 Training Accuracy = 0.9664166666666667\n","Iteration 64: Training Loss = 0.49805001716672465 Training Accuracy = 0.96662\n","Iteration 65: Training Loss = 0.49556909801388677 Training Accuracy = 0.9668183333333333\n","Iteration 66: Training Loss = 0.4931437292061988 Training Accuracy = 0.9669483333333333\n","Iteration 67: Training Loss = 0.49077194585632783 Training Accuracy = 0.9671433333333334\n","Iteration 68: Training Loss = 0.4884577088361915 Training Accuracy = 0.9673016666666666\n","Iteration 69: Training Loss = 0.48620147439648065 Training Accuracy = 0.96747\n","Iteration 70: Training Loss = 0.483998329444039 Training Accuracy = 0.9676083333333333\n","Iteration 71: Training Loss = 0.48184427588184037 Training Accuracy = 0.967745\n","Iteration 72: Training Loss = 0.4797401279764533 Training Accuracy = 0.9679283333333333\n","Iteration 73: Training Loss = 0.4776836959638746 Training Accuracy = 0.9680733333333333\n","Iteration 74: Training Loss = 0.4756727329869165 Training Accuracy = 0.9681883333333333\n","Iteration 75: Training Loss = 0.4737063747887218 Training Accuracy = 0.9682883333333333\n","Iteration 76: Training Loss = 0.4717797994943604 Training Accuracy = 0.9684733333333333\n","Iteration 77: Training Loss = 0.46989126087351213 Training Accuracy = 0.9686233333333333\n","Iteration 78: Training Loss = 0.4680383994943853 Training Accuracy = 0.9687816666666667\n","Iteration 79: Training Loss = 0.466218930570357 Training Accuracy = 0.9688866666666667\n","Iteration 80: Training Loss = 0.46443524505136974 Training Accuracy = 0.9689983333333333\n","Iteration 81: Training Loss = 0.46268605836839294 Training Accuracy = 0.9690916666666667\n","Iteration 82: Training Loss = 0.4609701631746056 Training Accuracy = 0.9691916666666667\n","Iteration 83: Training Loss = 0.4592885673283 Training Accuracy = 0.96931\n","Iteration 84: Training Loss = 0.4576447874532132 Training Accuracy = 0.96942\n","Iteration 85: Training Loss = 0.4560585366989713 Training Accuracy = 0.96952\n","Iteration 86: Training Loss = 0.45458736565918983 Training Accuracy = 0.9695966666666667\n","Iteration 87: Training Loss = 0.45336665626549333 Training Accuracy = 0.969705\n","Iteration 88: Training Loss = 0.4524513113651106 Training Accuracy = 0.9697116666666666\n","Iteration 89: Training Loss = 0.4511826445795657 Training Accuracy = 0.969775\n","Iteration 90: Training Loss = 0.449023497104844 Training Accuracy = 0.9699633333333333\n","Iteration 91: Training Loss = 0.44701260061836473 Training Accuracy = 0.9700716666666667\n","Iteration 92: Training Loss = 0.44614429700910735 Training Accuracy = 0.9701716666666667\n","Iteration 93: Training Loss = 0.44513338152969206 Training Accuracy = 0.9701583333333333\n","Iteration 94: Training Loss = 0.4431656534315852 Training Accuracy = 0.9703583333333333\n","Iteration 95: Training Loss = 0.44167624876472505 Training Accuracy = 0.9704433333333333\n","Iteration 96: Training Loss = 0.440859384993021 Training Accuracy = 0.9704733333333333\n","Iteration 97: Training Loss = 0.43945016125343966 Training Accuracy = 0.970605\n","Iteration 98: Training Loss = 0.43779155661844454 Training Accuracy = 0.9707366666666667\n","Iteration 99: Training Loss = 0.4367597265897383 Training Accuracy = 0.9707633333333333\n","Iteration 100: Training Loss = 0.4357173960638956 Training Accuracy = 0.970815\n","Iteration 101: Training Loss = 0.43421155716097903 Training Accuracy = 0.9709516666666667\n","Iteration 102: Training Loss = 0.4329471009526681 Training Accuracy = 0.9710183333333333\n","Iteration 103: Training Loss = 0.4320035360731076 Training Accuracy = 0.9710666666666666\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3lIwL6NNMsyo"},"source":[""],"execution_count":null,"outputs":[]}]}