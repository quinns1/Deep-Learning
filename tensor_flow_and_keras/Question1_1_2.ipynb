{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question1_1_2.ipynb","provenance":[],"collapsed_sections":[],"history_visible":true,"authorship_tag":"ABX9TyM8NZlu2ztMwt2UNqyt3b3a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dDxZH8L1m9aF","outputId":"5ec1b67d-7674-43ae-ff82-755a33be91d5"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Name: Shane Quinn\n","Student Number: R00144107\n","Email: shane.quinn1@mycit.ie\n","Course: MSc Artificial Intelligence\n","Module: Deep Learning\n","Date: 03/04/2021\n","\"\"\"\n","\n","import tensorflow as tf\n","from tensorflow.python.client import device_lib\n","from keras.utils import np_utils\n","import matplotlib.pyplot as plt\n","import functools\n","import time\n","\n","\n","def forward_pass(X, w1, w2, w3, b1, b2, b3):\n","    \"\"\"\n","    Push feature data through neural network. Returns 10 class probabilities for all feature instances\n","\n","    Parameters\n","    ----------\n","    X : tf.Variable\n","        Pre-processed input data.\n","    w1 : tf.Variable\n","        Layer 1 learnable weights.\n","    w2 : tf.Variable\n","        Layer 2 learnable weights.\n","    w3 : tf.Variable\n","        Layer 3 learnable weights.\n","    b1 : tf.Variable\n","        Layer 1 bias.\n","    b2 : tf.Variable\n","        Layer 2 bias.\n","    b3 : tf.Variable\n","        Layer 3 bias.\n","\n","    Returns\n","    -------\n","    H : tf.Variable\n","        Softmax layer output predicted probability of each class.\n","\n","    \"\"\"\n","\n","    #Layer 1- 300 Relu Neruons\n","    A = tf.matmul(w1, tf.transpose(X)) + b1                   #A1 = w1.X + b1\n","    H = tf.keras.activations.relu(A)                          #H1 = act(A1)   \n","    #Layer 2 - 100 Relu Neurons\n","    A = tf.matmul(w2, H) + b2                                 #A2 = w2.H1 + b2\n","    H = tf.keras.activations.relu(A)                          #H2 = act(A2)\n","    #Layer 3 - Softmax = (e^A2)/sum(A2)\n","    A = tf.matmul(w3, H)+b3                                   #A3 = w3.H2 + b3\n","    H = tf.exp(A) / tf.reduce_sum(tf.exp(A), axis=0)          #H3 = act(A3)\n","    \n","    return H\n","    \n","\n","def cross_entropy(pred_y, y):\n","    \"\"\"\n","    Take in softmax probabilities (output of forward_pass) and true class labels and calculate cross entropy loss\n","\n","    Parameters\n","    ----------\n","    pred_y : tf.Variable\n","        Predictions (Output of softmax layer in forward_pass()).\n","    y : tf.Variable\n","        One-hot encoded true class labels.\n","\n","    Returns\n","    -------\n","    cross_ent : Cross entropy loss\n","        tf.Variable.\n","\n","    \"\"\"\n","\n","    #Cross entropy loss per class = -sum((True class encoded values)*log(predicted probabilities))     \n","    a = -tf.reduce_sum(y * tf.math.log(pred_y), axis=0)\n","    #Cross entropy loss = mean of all losses calculated above.\n","    cross_ent = tf.reduce_mean(a, axis=0)\n","\n","    return cross_ent\n","\n","\n","\n","def calculate_accuracy(pred_y, y, datatype=tf.float32):\n","    \"\"\"\n","    Calculate the model accuracy given predicted probabilities and true class labels\n","\n","    Parameters\n","    ----------\n","    pred_y : tf.Variable\n","        Predicted class probabilities, output of forward pass/softmax layer.\n","    y : tf.Variable\n","        True class values.\n","    datatype : tf.float32/tf.float64, optional\n","        One of the above tf datatypes. The default is tf.float32.\n","\n","    Returns\n","    -------\n","    accuracy : float32\n","        Model Accuracy.\n","\n","    \"\"\"\n","\n","    # Convert predicted probabilities to 0 or 1\n","    pred_y = tf.round(pred_y)\n","    # Boolean True (1) if prediction is correct\n","    predictions = tf.cast(tf.equal(pred_y, y), tf.float32)    \n","    #Mean value of correct predictions\n","    accuracy = tf.reduce_mean(predictions)\n","    \n","    return accuracy\n","\n","\n","def exec_time(func):\n","    \"\"\"\n","    Generic Execution time recorder, pass in function. Records execution time using decorators\n","\n","    Parameters\n","    ----------\n","    func : FUNCTION\n","        Function .\n","\n","\n","    \"\"\"\n","    \n","    @functools.wraps(func)\n","    def record_exec_time(*args, **kwargs):\n","        start_time = time.perf_counter()\n","        mn = func(*args, **kwargs)\n","        execution_time = time.perf_counter() - start_time\n","        print(\"Execution Time: \", execution_time)\n","        return mn\n","\n","    return record_exec_time\n","\n","@exec_time \n","def main():\n","      \n","    X, y, X_val, y_val = pre_process() \n","\n","    #Initialise Learning rate and iterations.\n","    learning_rate = 0.05\n","    iterations = 1000\n","    datatype=tf.float64\n","    \n","    #Initialise lists for saving accuracies/loss\n","    te_acc = []\n","    tr_acc = []\n","    te_loss = []\n","    tr_loss = []\n","    \n","    # Create tf variables from data\n","    X = tf.cast(X, datatype)\n","    y = tf.cast(y, datatype)\n","    X_val = tf.cast(X_val, datatype)\n","    y_val = tf.cast(y_val, datatype)\n","    \n","    #Initialise Adam Optimizer\n","    adam = tf.keras.optimizers.Adam()\n","    \n","    #Initialise weights and bias\n","    zeros = tf.zeros_initializer()\n","    layer1_weights = tf.Variable(tf.random.normal([300,784], stddev=0.05, dtype=datatype))\n","    layer2_weights = tf.Variable(tf.random.normal([100, 300], stddev=0.05, dtype=datatype))\n","    layer3_weights = tf.Variable(tf.random.normal([10, 100], stddev=0.05, dtype=datatype))\n","    layer1_bias = tf.Variable(0, dtype=datatype)\n","    layer2_bias = tf.Variable(0, dtype=datatype)\n","    layer3_bias = tf.Variable(0, dtype=datatype)\n","\n","    #Repeat gradient descent loop 'iterations' times\n","    for i in range(iterations):\n","        \n","        with tf.GradientTape() as tape:\n","            #Create instance of gradient tape to record forward pass and calculate gradients for learnable weights and biases\n","            pred_y = forward_pass(X, layer1_weights, layer2_weights, layer3_weights, layer1_bias, layer2_bias, layer3_bias)\n","            loss = cross_entropy(pred_y, y)\n","            \n","        tr_loss.append(loss)\n","        #Calculate Gradients using gradient tape\n","        gradients = tape.gradient(loss, [layer1_weights, layer2_weights, layer3_weights, layer1_bias, layer2_bias, layer3_bias])\n","        accuracy = calculate_accuracy(pred_y, y, datatype)\n","        tr_acc.append(accuracy)\n","        print(\"Iteration {}: Training Loss = {} Training Accuracy = {}\".format(i, loss.numpy(), accuracy.numpy()))\n","        \n","        #Apply gradients using adaptive movement estimation, see accompanied report for more details\n","        adam.apply_gradients(zip(gradients, [layer1_weights, layer2_weights, layer3_weights, layer1_bias, layer2_bias, layer3_bias]))\n","        \n","        #Test model on validation data\n","        test_pred_y = forward_pass(X_val, layer1_weights, layer2_weights, layer3_weights, layer1_bias, layer2_bias, layer3_bias)\n","        test_loss = cross_entropy(test_pred_y, y_val)\n","        te_loss.append(test_loss)\n","        te_acc.append(calculate_accuracy(test_pred_y, y_val, datatype))    \n","    \n","    plt.title(\"Question 1_1_2\")\n","    plt.plot(te_loss, label=\"Validation Loss\")\n","    plt.plot(tr_loss, label=\"Train Loss\")\n","    plt.plot(te_acc, label=\"Validation Accuracy\")\n","    plt.plot(tr_acc, label=\"Train Accuracy\")\n","    plt.ylim((0,1))\n","    plt.legend()\n","    plt.show()\n","    \n","\n","def pre_process():\n","    \"\"\"\n","    Supplied Code for pre-processing Fashion MNIST dataset. Returns target class values and training data for training and validation\n","\n","    Returns\n","    -------\n","    tr_x : NUMPY N-D ARRAY\n","        X Training Data.\n","    tr_y : NUMPY N-D ARRAY\n","        y target class values 1 hot encoded (training data).\n","    te_x : NUMPY N-D ARRAY\n","        X Test Data.\n","    te_y : NUMPY N-D ARRAY\n","        y test target class values 1 hot encoded (test data).\n","\n","    \"\"\"\n","    \n","    fashion_mnist = tf.keras.datasets.fashion_mnist \n","    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n","    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n","    te_x = te_x.reshape(te_x.shape[0], 784)\n","    tr_x = tr_x / 255.0\n","    te_x = te_x / 255.0\n","    tr_y = np_utils.to_categorical(tr_y,10)\n","    tr_y = tr_y.T\n","    te_y = np_utils.to_categorical(te_y,10)\n","    te_y = te_y.T\n","\n","    return tr_x, tr_y, te_x, te_y\n","\n","\n","if __name__ == '__main__':\n","    print(\"Local Devices: \\n\", device_lib.list_local_devices())\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Local Devices: \n"," [name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 5464378971897497662\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 14674281152\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 11423643944509714229\n","physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n","]\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","Iteration 0: Training Loss = 2.296594276585486 Training Accuracy = 0.8999999761581421\n","Iteration 1: Training Loss = 2.177232726076542 Training Accuracy = 0.8999999761581421\n","Iteration 2: Training Loss = 2.0688736296523773 Training Accuracy = 0.8999999761581421\n","Iteration 3: Training Loss = 1.9592316238168248 Training Accuracy = 0.8999999761581421\n","Iteration 4: Training Loss = 1.8433028011851629 Training Accuracy = 0.8999999761581421\n","Iteration 5: Training Loss = 1.722990512452114 Training Accuracy = 0.9000033140182495\n","Iteration 6: Training Loss = 1.6014319452886936 Training Accuracy = 0.9008949995040894\n","Iteration 7: Training Loss = 1.4827584919045365 Training Accuracy = 0.9037100076675415\n","Iteration 8: Training Loss = 1.3707003124657724 Training Accuracy = 0.9071583151817322\n","Iteration 9: Training Loss = 1.2673593713463036 Training Accuracy = 0.9138583540916443\n","Iteration 10: Training Loss = 1.1734203338791882 Training Accuracy = 0.9213933348655701\n","Iteration 11: Training Loss = 1.089738167188209 Training Accuracy = 0.92732834815979\n","Iteration 12: Training Loss = 1.017264747293926 Training Accuracy = 0.9322500228881836\n","Iteration 13: Training Loss = 0.9557450890321982 Training Accuracy = 0.9367250204086304\n","Iteration 14: Training Loss = 0.9037933381033074 Training Accuracy = 0.9401050209999084\n","Iteration 15: Training Loss = 0.8602985138158686 Training Accuracy = 0.9421250224113464\n","Iteration 16: Training Loss = 0.8238782203412442 Training Accuracy = 0.9438566565513611\n","Iteration 17: Training Loss = 0.793007763254319 Training Accuracy = 0.945306658744812\n","Iteration 18: Training Loss = 0.7671476943999824 Training Accuracy = 0.9465583562850952\n","Iteration 19: Training Loss = 0.7449426196021887 Training Accuracy = 0.9477616548538208\n","Iteration 20: Training Loss = 0.7259878278309806 Training Accuracy = 0.948906660079956\n","Iteration 21: Training Loss = 0.7097326387762264 Training Accuracy = 0.9502350091934204\n","Iteration 22: Training Loss = 0.6952344290999583 Training Accuracy = 0.9510433077812195\n","Iteration 23: Training Loss = 0.6808558185608159 Training Accuracy = 0.9523683190345764\n","Iteration 24: Training Loss = 0.6672149953669895 Training Accuracy = 0.9530583620071411\n","Iteration 25: Training Loss = 0.6553952415860976 Training Accuracy = 0.9538416862487793\n","Iteration 26: Training Loss = 0.6447276386243193 Training Accuracy = 0.9551900029182434\n","Iteration 27: Training Loss = 0.6343739091080501 Training Accuracy = 0.9557099938392639\n","Iteration 28: Training Loss = 0.6230939105265343 Training Accuracy = 0.9570716619491577\n","Iteration 29: Training Loss = 0.612105996222609 Training Accuracy = 0.957705020904541\n","Iteration 30: Training Loss = 0.6024956077568756 Training Accuracy = 0.9585433602333069\n","Iteration 31: Training Loss = 0.5939249636403413 Training Accuracy = 0.9594483375549316\n","Iteration 32: Training Loss = 0.5855906044461888 Training Accuracy = 0.9597399830818176\n","Iteration 33: Training Loss = 0.5768727962069012 Training Accuracy = 0.9605466723442078\n","Iteration 34: Training Loss = 0.5688627367131486 Training Accuracy = 0.9609416723251343\n","Iteration 35: Training Loss = 0.5618293699816607 Training Accuracy = 0.9615716934204102\n","Iteration 36: Training Loss = 0.5556404258078338 Training Accuracy = 0.9618766903877258\n","Iteration 37: Training Loss = 0.5480970500608794 Training Accuracy = 0.9624266624450684\n","Iteration 38: Training Loss = 0.5409576714137978 Training Accuracy = 0.962921679019928\n","Iteration 39: Training Loss = 0.5354777126806354 Training Accuracy = 0.9633366465568542\n","Iteration 40: Training Loss = 0.5310644188749057 Training Accuracy = 0.9636800289154053\n","Iteration 41: Training Loss = 0.5262359990832499 Training Accuracy = 0.9638100266456604\n","Iteration 42: Training Loss = 0.520124888964513 Training Accuracy = 0.9644916653633118\n","Iteration 43: Training Loss = 0.5149610256472337 Training Accuracy = 0.9648699760437012\n","Iteration 44: Training Loss = 0.5113356515784065 Training Accuracy = 0.9649500250816345\n","Iteration 45: Training Loss = 0.5074494135261972 Training Accuracy = 0.9652916789054871\n","Iteration 46: Training Loss = 0.5029899426816433 Training Accuracy = 0.9655299782752991\n","Iteration 47: Training Loss = 0.4981733693832101 Training Accuracy = 0.9660733342170715\n","Iteration 48: Training Loss = 0.4944814494032019 Training Accuracy = 0.9663516879081726\n","Iteration 49: Training Loss = 0.49142694878716686 Training Accuracy = 0.9662866592407227\n","Iteration 50: Training Loss = 0.4880658336378743 Training Accuracy = 0.9667149782180786\n","Iteration 51: Training Loss = 0.4842675477872032 Training Accuracy = 0.9667566418647766\n","Iteration 52: Training Loss = 0.48100904703586495 Training Accuracy = 0.9670883417129517\n","Iteration 53: Training Loss = 0.47962920134081644 Training Accuracy = 0.9672350287437439\n","Iteration 54: Training Loss = 0.47993429494900924 Training Accuracy = 0.966801643371582\n","Iteration 55: Training Loss = 0.47621088324403776 Training Accuracy = 0.9674233198165894\n","Iteration 56: Training Loss = 0.469790118266505 Training Accuracy = 0.9677349925041199\n","Iteration 57: Training Loss = 0.46553174206638115 Training Accuracy = 0.9680416584014893\n","Iteration 58: Training Loss = 0.46506719818990333 Training Accuracy = 0.9682666659355164\n","Iteration 59: Training Loss = 0.4640786694412305 Training Accuracy = 0.967989981174469\n","Iteration 60: Training Loss = 0.45918029222391094 Training Accuracy = 0.9686683416366577\n","Iteration 61: Training Loss = 0.4563470757960821 Training Accuracy = 0.968916654586792\n","Iteration 62: Training Loss = 0.4559894942937009 Training Accuracy = 0.9687366485595703\n","Iteration 63: Training Loss = 0.4526191073471667 Training Accuracy = 0.9692599773406982\n","Iteration 64: Training Loss = 0.4493272178794247 Training Accuracy = 0.9694116711616516\n","Iteration 65: Training Loss = 0.4479658994725941 Training Accuracy = 0.9694383144378662\n","Iteration 66: Training Loss = 0.44586693731582816 Training Accuracy = 0.9697149991989136\n","Iteration 67: Training Loss = 0.4429959695217671 Training Accuracy = 0.969843327999115\n","Iteration 68: Training Loss = 0.44082884095529556 Training Accuracy = 0.9700016379356384\n","Iteration 69: Training Loss = 0.43962307682706264 Training Accuracy = 0.9700949788093567\n","Iteration 70: Training Loss = 0.4376658111135426 Training Accuracy = 0.9702016711235046\n","Iteration 71: Training Loss = 0.43472765580362877 Training Accuracy = 0.9704899787902832\n","Iteration 72: Training Loss = 0.43296802665212697 Training Accuracy = 0.9705749750137329\n","Iteration 73: Training Loss = 0.43189060250144595 Training Accuracy = 0.9706400036811829\n","Iteration 74: Training Loss = 0.42983409611239576 Training Accuracy = 0.9708150029182434\n","Iteration 75: Training Loss = 0.42731534831564083 Training Accuracy = 0.9709600210189819\n","Iteration 76: Training Loss = 0.4254079727233366 Training Accuracy = 0.9711133241653442\n","Iteration 77: Training Loss = 0.42419909831904157 Training Accuracy = 0.971198320388794\n","Iteration 78: Training Loss = 0.4228119623476598 Training Accuracy = 0.9712949991226196\n","Iteration 79: Training Loss = 0.42066922570783755 Training Accuracy = 0.971423327922821\n","Iteration 80: Training Loss = 0.4184935640962024 Training Accuracy = 0.9715783596038818\n","Iteration 81: Training Loss = 0.41682375081041256 Training Accuracy = 0.9716833233833313\n","Iteration 82: Training Loss = 0.4155537989402253 Training Accuracy = 0.971756637096405\n","Iteration 83: Training Loss = 0.4142037641402721 Training Accuracy = 0.9718700051307678\n","Iteration 84: Training Loss = 0.41245602360249795 Training Accuracy = 0.9719449877738953\n","Iteration 85: Training Loss = 0.41055328413942566 Training Accuracy = 0.9721316695213318\n","Iteration 86: Training Loss = 0.4087251427450165 Training Accuracy = 0.9722633361816406\n","Iteration 87: Training Loss = 0.40715183901205865 Training Accuracy = 0.9723283052444458\n","Iteration 88: Training Loss = 0.40579179669327425 Training Accuracy = 0.9724433422088623\n","Iteration 89: Training Loss = 0.4044988829209361 Training Accuracy = 0.9724066853523254\n","Iteration 90: Training Loss = 0.4032757206837482 Training Accuracy = 0.9725683331489563\n","Iteration 91: Training Loss = 0.40190048286894803 Training Accuracy = 0.9725633263587952\n","Iteration 92: Training Loss = 0.4006642765897689 Training Accuracy = 0.9727283120155334\n","Iteration 93: Training Loss = 0.3991834820308735 Training Accuracy = 0.9727200269699097\n","Iteration 94: Training Loss = 0.39800971434407134 Training Accuracy = 0.972944974899292\n","Iteration 95: Training Loss = 0.3968959008232678 Training Accuracy = 0.9729366898536682\n","Iteration 96: Training Loss = 0.3966280192004576 Training Accuracy = 0.9730933308601379\n","Iteration 97: Training Loss = 0.39775284758159196 Training Accuracy = 0.9728233218193054\n","Iteration 98: Training Loss = 0.3988224708512093 Training Accuracy = 0.9728366732597351\n","Iteration 99: Training Loss = 0.39869427279356884 Training Accuracy = 0.9727316498756409\n","Iteration 100: Training Loss = 0.3918155156040356 Training Accuracy = 0.9733733534812927\n","Iteration 101: Training Loss = 0.38768421082383386 Training Accuracy = 0.973580002784729\n","Iteration 102: Training Loss = 0.3894981772000302 Training Accuracy = 0.9733916521072388\n","Iteration 103: Training Loss = 0.3903149877259093 Training Accuracy = 0.9734933376312256\n","Iteration 104: Training Loss = 0.38658492070731154 Training Accuracy = 0.9736066460609436\n","Iteration 105: Training Loss = 0.38302632596823644 Training Accuracy = 0.9739266633987427\n","Iteration 106: Training Loss = 0.38419178519366043 Training Accuracy = 0.9738133549690247\n","Iteration 107: Training Loss = 0.3844372448638835 Training Accuracy = 0.973704993724823\n","Iteration 108: Training Loss = 0.38041432636969097 Training Accuracy = 0.9740483164787292\n","Iteration 109: Training Loss = 0.37850534801520813 Training Accuracy = 0.974245011806488\n","Iteration 110: Training Loss = 0.37960112634336884 Training Accuracy = 0.9740099906921387\n","Iteration 111: Training Loss = 0.3780295440410608 Training Accuracy = 0.9741916656494141\n","Iteration 112: Training Loss = 0.3751355867094954 Training Accuracy = 0.9743883609771729\n","Iteration 113: Training Loss = 0.37439941862380577 Training Accuracy = 0.9744150042533875\n","Iteration 114: Training Loss = 0.37456985177895114 Training Accuracy = 0.9744566679000854\n","Iteration 115: Training Loss = 0.3728985542019589 Training Accuracy = 0.9745050072669983\n","Iteration 116: Training Loss = 0.3707031442336592 Training Accuracy = 0.9746916890144348\n","Iteration 117: Training Loss = 0.37029236230340296 Training Accuracy = 0.9747133255004883\n","Iteration 118: Training Loss = 0.37016441268599665 Training Accuracy = 0.974703311920166\n","Iteration 119: Training Loss = 0.36869104758007254 Training Accuracy = 0.9747099876403809\n","Iteration 120: Training Loss = 0.3679832209923155 Training Accuracy = 0.9748866558074951\n","Iteration 121: Training Loss = 0.36937113805684957 Training Accuracy = 0.9746633172035217\n","Iteration 122: Training Loss = 0.37410305598939114 Training Accuracy = 0.974601686000824\n","Iteration 123: Training Loss = 0.37164832556175714 Training Accuracy = 0.9744666814804077\n","Iteration 124: Training Loss = 0.3694503296391048 Training Accuracy = 0.9748533368110657\n","Iteration 125: Training Loss = 0.36299654467718123 Training Accuracy = 0.9750783443450928\n","Iteration 126: Training Loss = 0.3631198031567832 Training Accuracy = 0.9751499891281128\n","Iteration 127: Training Loss = 0.3669298575212177 Training Accuracy = 0.9750049710273743\n","Iteration 128: Training Loss = 0.3625431997156305 Training Accuracy = 0.9751583337783813\n","Iteration 129: Training Loss = 0.35886706331177376 Training Accuracy = 0.9754716753959656\n","Iteration 130: Training Loss = 0.35950802443406965 Training Accuracy = 0.9755516648292542\n","Iteration 131: Training Loss = 0.3596464009314161 Training Accuracy = 0.9753149747848511\n","Iteration 132: Training Loss = 0.3575337392050652 Training Accuracy = 0.9755566716194153\n","Iteration 133: Training Loss = 0.35503137079784636 Training Accuracy = 0.9757066369056702\n","Iteration 134: Training Loss = 0.35562317108506736 Training Accuracy = 0.975611686706543\n","Iteration 135: Training Loss = 0.3562959498493497 Training Accuracy = 0.9756550192832947\n","Iteration 136: Training Loss = 0.3532080527092874 Training Accuracy = 0.975820004940033\n","Iteration 137: Training Loss = 0.35169247202109827 Training Accuracy = 0.9759733080863953\n","Iteration 138: Training Loss = 0.3524580869934192 Training Accuracy = 0.9759116768836975\n","Iteration 139: Training Loss = 0.35143351806592266 Training Accuracy = 0.975903332233429\n","Iteration 140: Training Loss = 0.3493536468525366 Training Accuracy = 0.9760666489601135\n","Iteration 141: Training Loss = 0.3483159452586418 Training Accuracy = 0.9761800169944763\n","Iteration 142: Training Loss = 0.34848007277731435 Training Accuracy = 0.9761099815368652\n","Iteration 143: Training Loss = 0.34795203914932654 Training Accuracy = 0.9762399792671204\n","Iteration 144: Training Loss = 0.3460204572357368 Training Accuracy = 0.9763050079345703\n","Iteration 145: Training Loss = 0.3450596986560322 Training Accuracy = 0.9763733148574829\n","Iteration 146: Training Loss = 0.34503642635529413 Training Accuracy = 0.9764066934585571\n","Iteration 147: Training Loss = 0.34423189820426814 Training Accuracy = 0.9764383435249329\n","Iteration 148: Training Loss = 0.3428804638736094 Training Accuracy = 0.9765433073043823\n","Iteration 149: Training Loss = 0.3418229204710465 Training Accuracy = 0.9766116738319397\n","Iteration 150: Training Loss = 0.34142086928942894 Training Accuracy = 0.976621687412262\n","Iteration 151: Training Loss = 0.34096472615410794 Training Accuracy = 0.9766700267791748\n","Iteration 152: Training Loss = 0.3398617646608391 Training Accuracy = 0.9767383337020874\n","Iteration 153: Training Loss = 0.3387595765645878 Training Accuracy = 0.9768649935722351\n","Iteration 154: Training Loss = 0.3380447840625329 Training Accuracy = 0.9768316745758057\n","Iteration 155: Training Loss = 0.33754453098159126 Training Accuracy = 0.9768800139427185\n","Iteration 156: Training Loss = 0.33687811406767243 Training Accuracy = 0.9768800139427185\n","Iteration 157: Training Loss = 0.33588293518028034 Training Accuracy = 0.9770116806030273\n","Iteration 158: Training Loss = 0.33491211716150465 Training Accuracy = 0.9770749807357788\n","Iteration 159: Training Loss = 0.33415834013021206 Training Accuracy = 0.9770983457565308\n","Iteration 160: Training Loss = 0.3335811796879147 Training Accuracy = 0.9771366715431213\n","Iteration 161: Training Loss = 0.33299082699762805 Training Accuracy = 0.9771649837493896\n","Iteration 162: Training Loss = 0.33224064147923593 Training Accuracy = 0.9772083163261414\n","Iteration 163: Training Loss = 0.331383602561597 Training Accuracy = 0.9772533178329468\n","Iteration 164: Training Loss = 0.3305213643578081 Training Accuracy = 0.977358341217041\n","Iteration 165: Training Loss = 0.32975504363995095 Training Accuracy = 0.9773816466331482\n","Iteration 166: Training Loss = 0.3291002548805735 Training Accuracy = 0.9774016737937927\n","Iteration 167: Training Loss = 0.32853598261043854 Training Accuracy = 0.9774366617202759\n","Iteration 168: Training Loss = 0.32805964495772655 Training Accuracy = 0.9775016903877258\n","Iteration 169: Training Loss = 0.32767874509569667 Training Accuracy = 0.977453351020813\n","Iteration 170: Training Loss = 0.32755994223742596 Training Accuracy = 0.9775699973106384\n","Iteration 171: Training Loss = 0.3277844936418069 Training Accuracy = 0.977429986000061\n","Iteration 172: Training Loss = 0.32844220509798905 Training Accuracy = 0.9775649905204773\n","Iteration 173: Training Loss = 0.3297835090804743 Training Accuracy = 0.9773216843605042\n","Iteration 174: Training Loss = 0.32976768537491796 Training Accuracy = 0.9774266481399536\n","Iteration 175: Training Loss = 0.32896644266503694 Training Accuracy = 0.9773716926574707\n","Iteration 176: Training Loss = 0.32499224973954566 Training Accuracy = 0.9777100086212158\n","Iteration 177: Training Loss = 0.32195886175512256 Training Accuracy = 0.9778933525085449\n","Iteration 178: Training Loss = 0.321504091047735 Training Accuracy = 0.9778416752815247\n","Iteration 179: Training Loss = 0.32301976956226397 Training Accuracy = 0.9779099822044373\n","Iteration 180: Training Loss = 0.32352554541951106 Training Accuracy = 0.9777583479881287\n","Iteration 181: Training Loss = 0.32163092820620814 Training Accuracy = 0.9779933094978333\n","Iteration 182: Training Loss = 0.318792148701638 Training Accuracy = 0.9779783487319946\n","Iteration 183: Training Loss = 0.3174273320839933 Training Accuracy = 0.9781716465950012\n","Iteration 184: Training Loss = 0.317803971046427 Training Accuracy = 0.9781583547592163\n","Iteration 185: Training Loss = 0.3183824784860784 Training Accuracy = 0.9781233072280884\n","Iteration 186: Training Loss = 0.3174118073560741 Training Accuracy = 0.9782350063323975\n","Iteration 187: Training Loss = 0.3156466904312161 Training Accuracy = 0.9781983494758606\n","Iteration 188: Training Loss = 0.31430417132014504 Training Accuracy = 0.9783599972724915\n","Iteration 189: Training Loss = 0.314097742973009 Training Accuracy = 0.9784166812896729\n","Iteration 190: Training Loss = 0.31444750976281644 Training Accuracy = 0.9783816933631897\n","Iteration 191: Training Loss = 0.314254797318903 Training Accuracy = 0.978380024433136\n","Iteration 192: Training Loss = 0.3134343279151407 Training Accuracy = 0.9783816933631897\n","Iteration 193: Training Loss = 0.3124849667655367 Training Accuracy = 0.9784683585166931\n","Iteration 194: Training Loss = 0.31246480462716936 Training Accuracy = 0.9783933162689209\n","Iteration 195: Training Loss = 0.31290983954917717 Training Accuracy = 0.9784799814224243\n","Iteration 196: Training Loss = 0.31472908403021854 Training Accuracy = 0.9784016609191895\n","Iteration 197: Training Loss = 0.3133894747365004 Training Accuracy = 0.9784366488456726\n","Iteration 198: Training Loss = 0.31258529000994845 Training Accuracy = 0.9784483313560486\n","Iteration 199: Training Loss = 0.3093533601185661 Training Accuracy = 0.978618323802948\n","Iteration 200: Training Loss = 0.30758056823642344 Training Accuracy = 0.9787033200263977\n","Iteration 201: Training Loss = 0.3070351893415803 Training Accuracy = 0.9787716865539551\n","Iteration 202: Training Loss = 0.30710705167063496 Training Accuracy = 0.9787466526031494\n","Iteration 203: Training Loss = 0.3071208709297012 Training Accuracy = 0.9788650274276733\n","Iteration 204: Training Loss = 0.3062517958943734 Training Accuracy = 0.9788650274276733\n","Iteration 205: Training Loss = 0.3057071402349255 Training Accuracy = 0.9788450002670288\n","Iteration 206: Training Loss = 0.304899908069071 Training Accuracy = 0.9789283275604248\n","Iteration 207: Training Loss = 0.30415689486681086 Training Accuracy = 0.9789666533470154\n","Iteration 208: Training Loss = 0.3031890516076814 Training Accuracy = 0.9790133237838745\n","Iteration 209: Training Loss = 0.30222613812597626 Training Accuracy = 0.9790683388710022\n","Iteration 210: Training Loss = 0.30142697994178164 Training Accuracy = 0.9791499972343445\n","Iteration 211: Training Loss = 0.30095867308197866 Training Accuracy = 0.9791883230209351\n","Iteration 212: Training Loss = 0.300836892320251 Training Accuracy = 0.9791749715805054\n","Iteration 213: Training Loss = 0.30067985412500675 Training Accuracy = 0.9792083501815796\n","Iteration 214: Training Loss = 0.30033468927612 Training Accuracy = 0.9791916608810425\n","Iteration 215: Training Loss = 0.2995919079007061 Training Accuracy = 0.9792033433914185\n","Iteration 216: Training Loss = 0.2985846806571767 Training Accuracy = 0.9792600274085999\n","Iteration 217: Training Loss = 0.2974797792312558 Training Accuracy = 0.979324996471405\n","Iteration 218: Training Loss = 0.2965203354641773 Training Accuracy = 0.9794583320617676\n","Iteration 219: Training Loss = 0.2958194690485775 Training Accuracy = 0.9795100092887878\n","Iteration 220: Training Loss = 0.29536623532446127 Training Accuracy = 0.979473352432251\n","Iteration 221: Training Loss = 0.295069031684641 Training Accuracy = 0.9795266389846802\n","Iteration 222: Training Loss = 0.2948377854698177 Training Accuracy = 0.979503333568573\n","Iteration 223: Training Loss = 0.29460486539639896 Training Accuracy = 0.9795399904251099\n","Iteration 224: Training Loss = 0.29429173338935477 Training Accuracy = 0.9795166850090027\n","Iteration 225: Training Loss = 0.2939046109872503 Training Accuracy = 0.979568362236023\n","Iteration 226: Training Loss = 0.29339427362250203 Training Accuracy = 0.9796183109283447\n","Iteration 227: Training Loss = 0.2928937487917669 Training Accuracy = 0.9796566963195801\n","Iteration 228: Training Loss = 0.2923775705087564 Training Accuracy = 0.979711651802063\n","Iteration 229: Training Loss = 0.2922342961949357 Training Accuracy = 0.979651689529419\n","Iteration 230: Training Loss = 0.2921429258504497 Training Accuracy = 0.9797483086585999\n","Iteration 231: Training Loss = 0.29333531854985095 Training Accuracy = 0.9795849919319153\n","Iteration 232: Training Loss = 0.2939089138291518 Training Accuracy = 0.9794483184814453\n","Iteration 233: Training Loss = 0.296866603768946 Training Accuracy = 0.9793833494186401\n","Iteration 234: Training Loss = 0.2960453308443138 Training Accuracy = 0.9793033599853516\n","Iteration 235: Training Loss = 0.2951073776176424 Training Accuracy = 0.9794650077819824\n","Iteration 236: Training Loss = 0.2915327862673657 Training Accuracy = 0.979723334312439\n","Iteration 237: Training Loss = 0.2881811565475351 Training Accuracy = 0.9799066781997681\n","Iteration 238: Training Loss = 0.28754003397317873 Training Accuracy = 0.9798816442489624\n","Iteration 239: Training Loss = 0.2884653078869563 Training Accuracy = 0.9799233078956604\n","Iteration 240: Training Loss = 0.2900033369063381 Training Accuracy = 0.979753315448761\n","Iteration 241: Training Loss = 0.28763275854274944 Training Accuracy = 0.9799666404724121\n","Iteration 242: Training Loss = 0.28516750842533906 Training Accuracy = 0.9801016449928284\n","Iteration 243: Training Loss = 0.28383028787922515 Training Accuracy = 0.9802016615867615\n","Iteration 244: Training Loss = 0.2842006635598329 Training Accuracy = 0.9802316427230835\n","Iteration 245: Training Loss = 0.28547192585252257 Training Accuracy = 0.9799816608428955\n","Iteration 246: Training Loss = 0.284587716911324 Training Accuracy = 0.9801516532897949\n","Iteration 247: Training Loss = 0.2832217393262494 Training Accuracy = 0.9801666736602783\n","Iteration 248: Training Loss = 0.28118138662626857 Training Accuracy = 0.9803183078765869\n","Iteration 249: Training Loss = 0.2804520729802223 Training Accuracy = 0.980376660823822\n","Iteration 250: Training Loss = 0.2809128484826866 Training Accuracy = 0.9803199768066406\n","Iteration 251: Training Loss = 0.2811889998546382 Training Accuracy = 0.9803433418273926\n","Iteration 252: Training Loss = 0.2810402287387384 Training Accuracy = 0.9802799820899963\n","Iteration 253: Training Loss = 0.27950191598441226 Training Accuracy = 0.9804916381835938\n","Iteration 254: Training Loss = 0.278206296006114 Training Accuracy = 0.9804499745368958\n","Iteration 255: Training Loss = 0.27751678102604604 Training Accuracy = 0.9806050062179565\n","Iteration 256: Training Loss = 0.27744238848113867 Training Accuracy = 0.9805733561515808\n","Iteration 257: Training Loss = 0.2775688911711093 Training Accuracy = 0.9805533289909363\n","Iteration 258: Training Loss = 0.2772022509246082 Training Accuracy = 0.9806216955184937\n","Iteration 259: Training Loss = 0.2766464416531298 Training Accuracy = 0.9806333184242249\n","Iteration 260: Training Loss = 0.27567090922465215 Training Accuracy = 0.9807050228118896\n","Iteration 261: Training Loss = 0.27482986058820386 Training Accuracy = 0.9807583093643188\n","Iteration 262: Training Loss = 0.27421223270634254 Training Accuracy = 0.9807900190353394\n","Iteration 263: Training Loss = 0.2738377488032805 Training Accuracy = 0.9807833433151245\n","Iteration 264: Training Loss = 0.2736191603438274 Training Accuracy = 0.9808800220489502\n","Iteration 265: Training Loss = 0.2733761629049183 Training Accuracy = 0.9808350205421448\n","Iteration 266: Training Loss = 0.2731351870544972 Training Accuracy = 0.98089998960495\n","Iteration 267: Training Loss = 0.2726732675881167 Training Accuracy = 0.9809116721153259\n","Iteration 268: Training Loss = 0.2721996834382828 Training Accuracy = 0.9809749722480774\n","Iteration 269: Training Loss = 0.2715761250340661 Training Accuracy = 0.9809433221817017\n","Iteration 270: Training Loss = 0.2709769738705452 Training Accuracy = 0.9810649752616882\n","Iteration 271: Training Loss = 0.27039761832173514 Training Accuracy = 0.9810000061988831\n","Iteration 272: Training Loss = 0.2699549768452253 Training Accuracy = 0.9811133146286011\n","Iteration 273: Training Loss = 0.269828763656282 Training Accuracy = 0.9810299873352051\n","Iteration 274: Training Loss = 0.27037162581390317 Training Accuracy = 0.9810900092124939\n","Iteration 275: Training Loss = 0.27271873519427203 Training Accuracy = 0.9808549880981445\n","Iteration 276: Training Loss = 0.278129369818183 Training Accuracy = 0.9804083108901978\n","Iteration 277: Training Loss = 0.2895467583447702 Training Accuracy = 0.9795016646385193\n","Iteration 278: Training Loss = 0.2941463047312535 Training Accuracy = 0.9789283275604248\n","Iteration 279: Training Loss = 0.28534833510575286 Training Accuracy = 0.9798283576965332\n","Iteration 280: Training Loss = 0.26788577515361095 Training Accuracy = 0.9812216758728027\n","Iteration 281: Training Loss = 0.2728688956714439 Training Accuracy = 0.980804979801178\n","Iteration 282: Training Loss = 0.28378139004967673 Training Accuracy = 0.9799516797065735\n","Iteration 283: Training Loss = 0.27073035457083805 Training Accuracy = 0.9810383319854736\n","Iteration 284: Training Loss = 0.26675628285666697 Training Accuracy = 0.9813466668128967\n","Iteration 285: Training Loss = 0.27609355245205097 Training Accuracy = 0.980525016784668\n","Iteration 286: Training Loss = 0.2696314053505239 Training Accuracy = 0.9810433387756348\n","Iteration 287: Training Loss = 0.26457729517263034 Training Accuracy = 0.981511652469635\n","Iteration 288: Training Loss = 0.2704081548231665 Training Accuracy = 0.9809733629226685\n","Iteration 289: Training Loss = 0.2675755013834086 Training Accuracy = 0.9812899827957153\n","Iteration 290: Training Loss = 0.26294814970955854 Training Accuracy = 0.9816083312034607\n","Iteration 291: Training Loss = 0.26648037586471174 Training Accuracy = 0.9812566637992859\n","Iteration 292: Training Loss = 0.26585281308311565 Training Accuracy = 0.9814000129699707\n","Iteration 293: Training Loss = 0.26186381035299044 Training Accuracy = 0.9816550016403198\n","Iteration 294: Training Loss = 0.26361677438745884 Training Accuracy = 0.9815149903297424\n","Iteration 295: Training Loss = 0.2637951849432025 Training Accuracy = 0.9814916849136353\n","Iteration 296: Training Loss = 0.26082137943970823 Training Accuracy = 0.981678307056427\n","Iteration 297: Training Loss = 0.26108245865674967 Training Accuracy = 0.981701672077179\n","Iteration 298: Training Loss = 0.2620619282709059 Training Accuracy = 0.9816466569900513\n","Iteration 299: Training Loss = 0.26001438169124513 Training Accuracy = 0.981719970703125\n","Iteration 300: Training Loss = 0.25940535006819954 Training Accuracy = 0.9817916750907898\n","Iteration 301: Training Loss = 0.2603397285413429 Training Accuracy = 0.9818483591079712\n","Iteration 302: Training Loss = 0.2592139091667663 Training Accuracy = 0.981814980506897\n","Iteration 303: Training Loss = 0.2579909000054744 Training Accuracy = 0.9819283485412598\n","Iteration 304: Training Loss = 0.2586377815175052 Training Accuracy = 0.9818900227546692\n","Iteration 305: Training Loss = 0.2582749253201018 Training Accuracy = 0.9818549752235413\n","Iteration 306: Training Loss = 0.25693985335368297 Training Accuracy = 0.9820416569709778\n","Iteration 307: Training Loss = 0.25700330729021154 Training Accuracy = 0.9820533394813538\n","Iteration 308: Training Loss = 0.25713838593425964 Training Accuracy = 0.9819849729537964\n","Iteration 309: Training Loss = 0.25606611220133946 Training Accuracy = 0.9821500182151794\n","Iteration 310: Training Loss = 0.25557338858605017 Training Accuracy = 0.9821400046348572\n","Iteration 311: Training Loss = 0.25583472558573656 Training Accuracy = 0.9819916486740112\n","Iteration 312: Training Loss = 0.2553339269151412 Training Accuracy = 0.9822016954421997\n","Iteration 313: Training Loss = 0.25467688442989883 Training Accuracy = 0.9822649955749512\n","Iteration 314: Training Loss = 0.25483637358651773 Training Accuracy = 0.9821283221244812\n","Iteration 315: Training Loss = 0.25513045105389015 Training Accuracy = 0.9821933507919312\n","Iteration 316: Training Loss = 0.25543762165484313 Training Accuracy = 0.9820150136947632\n","Iteration 317: Training Loss = 0.2559624582748872 Training Accuracy = 0.9820449948310852\n","Iteration 318: Training Loss = 0.2585708406432758 Training Accuracy = 0.9817816615104675\n","Iteration 319: Training Loss = 0.25985731623560776 Training Accuracy = 0.9816449880599976\n","Iteration 320: Training Loss = 0.2630760077590223 Training Accuracy = 0.9814566373825073\n","Iteration 321: Training Loss = 0.25902923640627323 Training Accuracy = 0.9816616773605347\n","Iteration 322: Training Loss = 0.25678722677942106 Training Accuracy = 0.9818716645240784\n","Iteration 323: Training Loss = 0.25203121658168687 Training Accuracy = 0.9823566675186157\n","Iteration 324: Training Loss = 0.2506377307284009 Training Accuracy = 0.98246830701828\n","Iteration 325: Training Loss = 0.25234142094935885 Training Accuracy = 0.9822483062744141\n","Iteration 326: Training Loss = 0.25388049707416455 Training Accuracy = 0.9821816682815552\n","Iteration 327: Training Loss = 0.25505391032385777 Training Accuracy = 0.9820266962051392\n","Iteration 328: Training Loss = 0.2522649319585157 Training Accuracy = 0.9823533296585083\n","Iteration 329: Training Loss = 0.24999943167028507 Training Accuracy = 0.9825366735458374\n","Iteration 330: Training Loss = 0.2486535771134997 Training Accuracy = 0.9825966954231262\n","Iteration 331: Training Loss = 0.24881642240370758 Training Accuracy = 0.9826116561889648\n","Iteration 332: Training Loss = 0.25003004285844216 Training Accuracy = 0.9824566841125488\n","Iteration 333: Training Loss = 0.2503229855103077 Training Accuracy = 0.9824399948120117\n","Iteration 334: Training Loss = 0.24971061523918994 Training Accuracy = 0.9824900031089783\n","Iteration 335: Training Loss = 0.2479179751268996 Training Accuracy = 0.9826433062553406\n","Iteration 336: Training Loss = 0.24674536188274124 Training Accuracy = 0.9827033281326294\n","Iteration 337: Training Loss = 0.24638559580130853 Training Accuracy = 0.982795000076294\n","Iteration 338: Training Loss = 0.24660480010623134 Training Accuracy = 0.9827216863632202\n","Iteration 339: Training Loss = 0.24702626089242852 Training Accuracy = 0.982711672782898\n","Iteration 340: Training Loss = 0.2468416970886455 Training Accuracy = 0.9827250242233276\n","Iteration 341: Training Loss = 0.24644832470223807 Training Accuracy = 0.9826833605766296\n","Iteration 342: Training Loss = 0.2453470705296273 Training Accuracy = 0.9828166961669922\n","Iteration 343: Training Loss = 0.24448079421755622 Training Accuracy = 0.9828733205795288\n","Iteration 344: Training Loss = 0.24396966245345678 Training Accuracy = 0.9829149842262268\n","Iteration 345: Training Loss = 0.24379566313587356 Training Accuracy = 0.9829766750335693\n","Iteration 346: Training Loss = 0.24377469091812437 Training Accuracy = 0.982853353023529\n","Iteration 347: Training Loss = 0.24370237447495355 Training Accuracy = 0.9829283356666565\n","Iteration 348: Training Loss = 0.2436476889614787 Training Accuracy = 0.9829350113868713\n","Iteration 349: Training Loss = 0.2433094194966437 Training Accuracy = 0.9829549789428711\n","Iteration 350: Training Loss = 0.2428951634524086 Training Accuracy = 0.9829800128936768\n","Iteration 351: Training Loss = 0.24228587754270867 Training Accuracy = 0.9830183386802673\n","Iteration 352: Training Loss = 0.2417371092159937 Training Accuracy = 0.9830683469772339\n","Iteration 353: Training Loss = 0.24122395285873868 Training Accuracy = 0.9831133484840393\n","Iteration 354: Training Loss = 0.24079939604300124 Training Accuracy = 0.9831783175468445\n","Iteration 355: Training Loss = 0.24042764703250044 Training Accuracy = 0.9832050204277039\n","Iteration 356: Training Loss = 0.24011207932728468 Training Accuracy = 0.9832333326339722\n","Iteration 357: Training Loss = 0.23985778272949435 Training Accuracy = 0.9832666516304016\n","Iteration 358: Training Loss = 0.23964786746159292 Training Accuracy = 0.983251690864563\n","Iteration 359: Training Loss = 0.23950296619701716 Training Accuracy = 0.9832333326339722\n","Iteration 360: Training Loss = 0.23936727987328765 Training Accuracy = 0.9832649827003479\n","Iteration 361: Training Loss = 0.23940058538213338 Training Accuracy = 0.983186662197113\n","Iteration 362: Training Loss = 0.239463504714256 Training Accuracy = 0.983251690864563\n","Iteration 363: Training Loss = 0.2399861394629792 Training Accuracy = 0.9830866456031799\n","Iteration 364: Training Loss = 0.24044786414313615 Training Accuracy = 0.9831100106239319\n","Iteration 365: Training Loss = 0.2418730383901573 Training Accuracy = 0.9828433394432068\n","Iteration 366: Training Loss = 0.24233736621393656 Training Accuracy = 0.9828966856002808\n","Iteration 367: Training Loss = 0.24417695543941675 Training Accuracy = 0.9826733469963074\n","Iteration 368: Training Loss = 0.24268043384282562 Training Accuracy = 0.9828150272369385\n","Iteration 369: Training Loss = 0.24191224255436847 Training Accuracy = 0.9827899932861328\n","Iteration 370: Training Loss = 0.23861832663272198 Training Accuracy = 0.9832016825675964\n","Iteration 371: Training Loss = 0.23633363081740857 Training Accuracy = 0.9833716750144958\n","Iteration 372: Training Loss = 0.23515284761280753 Training Accuracy = 0.9835983514785767\n","Iteration 373: Training Loss = 0.23532635288725842 Training Accuracy = 0.9834966659545898\n","Iteration 374: Training Loss = 0.2364044203302429 Training Accuracy = 0.9833183288574219\n","Iteration 375: Training Loss = 0.23715619189666362 Training Accuracy = 0.9833216667175293\n","Iteration 376: Training Loss = 0.23802539992645885 Training Accuracy = 0.9830800294876099\n","Iteration 377: Training Loss = 0.23701425302373888 Training Accuracy = 0.983299970626831\n","Iteration 378: Training Loss = 0.23605492822517785 Training Accuracy = 0.9832950234413147\n","Iteration 379: Training Loss = 0.23433543766393528 Training Accuracy = 0.9835450053215027\n","Iteration 380: Training Loss = 0.23311893586580082 Training Accuracy = 0.9837183356285095\n","Iteration 381: Training Loss = 0.2325190388414423 Training Accuracy = 0.9837200045585632\n","Iteration 382: Training Loss = 0.23250558447141084 Training Accuracy = 0.9837200045585632\n","Iteration 383: Training Loss = 0.23282110001301493 Training Accuracy = 0.9836300015449524\n","Iteration 384: Training Loss = 0.23302946359408017 Training Accuracy = 0.9836300015449524\n","Iteration 385: Training Loss = 0.23311142112125713 Training Accuracy = 0.9834916591644287\n","Iteration 386: Training Loss = 0.23269352733871368 Training Accuracy = 0.9836716651916504\n","Iteration 387: Training Loss = 0.23218984240635904 Training Accuracy = 0.9836133122444153\n","Iteration 388: Training Loss = 0.2314716610260229 Training Accuracy = 0.9837499856948853\n","Iteration 389: Training Loss = 0.23089643043371003 Training Accuracy = 0.9838733077049255\n","Iteration 390: Training Loss = 0.23042598797258168 Training Accuracy = 0.9838100075721741\n","Iteration 391: Training Loss = 0.2300804944371471 Training Accuracy = 0.9839816689491272\n","Iteration 392: Training Loss = 0.22984845963504466 Training Accuracy = 0.983905017375946\n","Iteration 393: Training Loss = 0.2295042011405327 Training Accuracy = 0.9839866757392883\n","Iteration 394: Training Loss = 0.22908734062747996 Training Accuracy = 0.9839850068092346\n","Iteration 395: Training Loss = 0.22855094300057233 Training Accuracy = 0.9840566515922546\n","Iteration 396: Training Loss = 0.2280744846920239 Training Accuracy = 0.9840866923332214\n","Iteration 397: Training Loss = 0.2277839096512085 Training Accuracy = 0.9840283393859863\n","Iteration 398: Training Loss = 0.22767947439084862 Training Accuracy = 0.9842000007629395\n","Iteration 399: Training Loss = 0.22771096186840017 Training Accuracy = 0.9840250015258789\n","Iteration 400: Training Loss = 0.22774050149857059 Training Accuracy = 0.9841383099555969\n","Iteration 401: Training Loss = 0.22781660626430503 Training Accuracy = 0.9839483499526978\n","Iteration 402: Training Loss = 0.22791249012515916 Training Accuracy = 0.984041690826416\n","Iteration 403: Training Loss = 0.2281761802317306 Training Accuracy = 0.9839316606521606\n","Iteration 404: Training Loss = 0.22915969426841137 Training Accuracy = 0.9838516712188721\n","Iteration 405: Training Loss = 0.2302668147691963 Training Accuracy = 0.9838200211524963\n","Iteration 406: Training Loss = 0.23303540719695764 Training Accuracy = 0.9834700226783752\n","Iteration 407: Training Loss = 0.2335295697569086 Training Accuracy = 0.9834933280944824\n","Iteration 408: Training Loss = 0.23522770530071568 Training Accuracy = 0.9832383394241333\n","Iteration 409: Training Loss = 0.23107681614597636 Training Accuracy = 0.9836999773979187\n","Iteration 410: Training Loss = 0.22782324287479908 Training Accuracy = 0.9838433265686035\n","Iteration 411: Training Loss = 0.224424633909274 Training Accuracy = 0.9842566847801208\n","Iteration 412: Training Loss = 0.2236321518018842 Training Accuracy = 0.9843583106994629\n","Iteration 413: Training Loss = 0.22482980740027544 Training Accuracy = 0.984208345413208\n","Iteration 414: Training Loss = 0.22651463141375694 Training Accuracy = 0.9840366840362549\n","Iteration 415: Training Loss = 0.22773741224586552 Training Accuracy = 0.9838283061981201\n","Iteration 416: Training Loss = 0.22681434458328656 Training Accuracy = 0.984041690826416\n","Iteration 417: Training Loss = 0.22544785075071286 Training Accuracy = 0.984071671962738\n","Iteration 418: Training Loss = 0.22348836482108217 Training Accuracy = 0.9842983484268188\n","Iteration 419: Training Loss = 0.2221039500084069 Training Accuracy = 0.9845200181007385\n","Iteration 420: Training Loss = 0.22158108059107223 Training Accuracy = 0.9845033288002014\n","Iteration 421: Training Loss = 0.2217097628437566 Training Accuracy = 0.9844866394996643\n","Iteration 422: Training Loss = 0.22228533235321143 Training Accuracy = 0.9843583106994629\n","Iteration 423: Training Loss = 0.2224473421588534 Training Accuracy = 0.9843683242797852\n","Iteration 424: Training Loss = 0.2225840247439499 Training Accuracy = 0.9843283295631409\n","Iteration 425: Training Loss = 0.2216981554627467 Training Accuracy = 0.9844549894332886\n","Iteration 426: Training Loss = 0.22070622512249088 Training Accuracy = 0.9845366477966309\n","Iteration 427: Training Loss = 0.21960736854575633 Training Accuracy = 0.9846066832542419\n","Iteration 428: Training Loss = 0.21903033460984347 Training Accuracy = 0.984678328037262\n","Iteration 429: Training Loss = 0.21904377598082658 Training Accuracy = 0.9847433567047119\n","Iteration 430: Training Loss = 0.21949192212973592 Training Accuracy = 0.9845516681671143\n","Iteration 431: Training Loss = 0.21995884166657592 Training Accuracy = 0.9846100211143494\n","Iteration 432: Training Loss = 0.22072259759498733 Training Accuracy = 0.9843916893005371\n","Iteration 433: Training Loss = 0.2211607640524769 Training Accuracy = 0.9845383167266846\n","Iteration 434: Training Loss = 0.22247331174573962 Training Accuracy = 0.9842366576194763\n","Iteration 435: Training Loss = 0.22308085091191487 Training Accuracy = 0.9843416810035706\n","Iteration 436: Training Loss = 0.22438176498652632 Training Accuracy = 0.9840883612632751\n","Iteration 437: Training Loss = 0.22419240122958514 Training Accuracy = 0.9842816591262817\n","Iteration 438: Training Loss = 0.22354749323911802 Training Accuracy = 0.9841516613960266\n","Iteration 439: Training Loss = 0.22175861667346708 Training Accuracy = 0.984458327293396\n","Iteration 440: Training Loss = 0.22193499970817157 Training Accuracy = 0.9842333197593689\n","Iteration 441: Training Loss = 0.2207580857615182 Training Accuracy = 0.9844783544540405\n","Iteration 442: Training Loss = 0.22052279968845231 Training Accuracy = 0.984416663646698\n","Iteration 443: Training Loss = 0.21900523028528768 Training Accuracy = 0.98451167345047\n","Iteration 444: Training Loss = 0.21673901923157438 Training Accuracy = 0.9848916530609131\n","Iteration 445: Training Loss = 0.2157417131394412 Training Accuracy = 0.9847683310508728\n","Iteration 446: Training Loss = 0.21608259091350634 Training Accuracy = 0.9848849773406982\n","Iteration 447: Training Loss = 0.2171810744087294 Training Accuracy = 0.9846266508102417\n","Iteration 448: Training Loss = 0.2171326900506662 Training Accuracy = 0.9847450256347656\n","Iteration 449: Training Loss = 0.21672161188423034 Training Accuracy = 0.9846400022506714\n","Iteration 450: Training Loss = 0.21532929421611308 Training Accuracy = 0.9848650097846985\n","Iteration 451: Training Loss = 0.2140351700041581 Training Accuracy = 0.9850966930389404\n","Iteration 452: Training Loss = 0.213118303030144 Training Accuracy = 0.9850550293922424\n","Iteration 453: Training Loss = 0.2126916786181338 Training Accuracy = 0.9851816892623901\n","Iteration 454: Training Loss = 0.21293115526498077 Training Accuracy = 0.9850466847419739\n","Iteration 455: Training Loss = 0.21339774896388797 Training Accuracy = 0.9850999712944031\n","Iteration 456: Training Loss = 0.21393251347237044 Training Accuracy = 0.9849483370780945\n","Iteration 457: Training Loss = 0.21346535580574783 Training Accuracy = 0.9851250052452087\n","Iteration 458: Training Loss = 0.2126897560766687 Training Accuracy = 0.985111653804779\n","Iteration 459: Training Loss = 0.21152752035429725 Training Accuracy = 0.9852083325386047\n","Iteration 460: Training Loss = 0.21072665410674926 Training Accuracy = 0.985289990901947\n","Iteration 461: Training Loss = 0.21027962879598266 Training Accuracy = 0.985331654548645\n","Iteration 462: Training Loss = 0.21008584649006837 Training Accuracy = 0.985319972038269\n","Iteration 463: Training Loss = 0.20997396946214597 Training Accuracy = 0.985385000705719\n","Iteration 464: Training Loss = 0.2099404916255361 Training Accuracy = 0.9853150248527527\n","Iteration 465: Training Loss = 0.20995111193949995 Training Accuracy = 0.9853516817092896\n","Iteration 466: Training Loss = 0.2099473120058233 Training Accuracy = 0.9853399991989136\n","Iteration 467: Training Loss = 0.20990220501402382 Training Accuracy = 0.9853116869926453\n","Iteration 468: Training Loss = 0.20965848846996896 Training Accuracy = 0.985343337059021\n","Iteration 469: Training Loss = 0.20941524799061054 Training Accuracy = 0.9853083491325378\n","Iteration 470: Training Loss = 0.20902309942057049 Training Accuracy = 0.9853833317756653\n","Iteration 471: Training Loss = 0.20890653283846244 Training Accuracy = 0.9853183627128601\n","Iteration 472: Training Loss = 0.20869706853377976 Training Accuracy = 0.9854616522789001\n","Iteration 473: Training Loss = 0.20900739969297896 Training Accuracy = 0.9853166937828064\n","Iteration 474: Training Loss = 0.20923861563711343 Training Accuracy = 0.9854416847229004\n","Iteration 475: Training Loss = 0.21036232980723824 Training Accuracy = 0.9850900173187256\n","Iteration 476: Training Loss = 0.21160940385279917 Training Accuracy = 0.985230028629303\n","Iteration 477: Training Loss = 0.21410105935347246 Training Accuracy = 0.984761655330658\n","Iteration 478: Training Loss = 0.21604761180022106 Training Accuracy = 0.9848466515541077\n","Iteration 479: Training Loss = 0.21725472590588463 Training Accuracy = 0.9844833612442017\n","Iteration 480: Training Loss = 0.21470990677943658 Training Accuracy = 0.9849316477775574\n","Iteration 481: Training Loss = 0.20982536410765065 Training Accuracy = 0.9851199984550476\n","Iteration 482: Training Loss = 0.20554560228100652 Training Accuracy = 0.985718309879303\n","Iteration 483: Training Loss = 0.20528064826007147 Training Accuracy = 0.9857083559036255\n","Iteration 484: Training Loss = 0.20799792443098067 Training Accuracy = 0.9852550029754639\n","Iteration 485: Training Loss = 0.20980868268028663 Training Accuracy = 0.9853566884994507\n","Iteration 486: Training Loss = 0.20888114688247805 Training Accuracy = 0.9852016568183899\n","Iteration 487: Training Loss = 0.20571963280262673 Training Accuracy = 0.985729992389679\n","Iteration 488: Training Loss = 0.2037160639244115 Training Accuracy = 0.9857533574104309\n","Iteration 489: Training Loss = 0.20411502390233113 Training Accuracy = 0.9856816530227661\n","Iteration 490: Training Loss = 0.20562677516001118 Training Accuracy = 0.985718309879303\n","Iteration 491: Training Loss = 0.20630895220922213 Training Accuracy = 0.9854316711425781\n","Iteration 492: Training Loss = 0.20504025392674188 Training Accuracy = 0.985729992389679\n","Iteration 493: Training Loss = 0.20328885165266602 Training Accuracy = 0.9857733249664307\n","Iteration 494: Training Loss = 0.20241233611929407 Training Accuracy = 0.9858733415603638\n","Iteration 495: Training Loss = 0.20283798227506275 Training Accuracy = 0.9858866930007935\n","Iteration 496: Training Loss = 0.20360128040960904 Training Accuracy = 0.9856083393096924\n","Iteration 497: Training Loss = 0.20349610179390035 Training Accuracy = 0.9858666658401489\n","Iteration 498: Training Loss = 0.20290563986992133 Training Accuracy = 0.9856516718864441\n","Iteration 499: Training Loss = 0.20214720014667803 Training Accuracy = 0.9858883619308472\n","Iteration 500: Training Loss = 0.20250615219163784 Training Accuracy = 0.9858266711235046\n","Iteration 501: Training Loss = 0.2036732058798875 Training Accuracy = 0.9857116937637329\n","Iteration 502: Training Loss = 0.20501444047878598 Training Accuracy = 0.9856833219528198\n","Iteration 503: Training Loss = 0.20724136767447254 Training Accuracy = 0.9854033589363098\n","Iteration 504: Training Loss = 0.2085402667009309 Training Accuracy = 0.9852550029754639\n","Iteration 505: Training Loss = 0.21011310412705925 Training Accuracy = 0.9851199984550476\n","Iteration 506: Training Loss = 0.21065782344674583 Training Accuracy = 0.9849600195884705\n","Iteration 507: Training Loss = 0.2072724007799552 Training Accuracy = 0.9852650165557861\n","Iteration 508: Training Loss = 0.2039022848710998 Training Accuracy = 0.9855350255966187\n","Iteration 509: Training Loss = 0.20011930447109394 Training Accuracy = 0.9860600233078003\n","Iteration 510: Training Loss = 0.19929724703064117 Training Accuracy = 0.9860550165176392\n","Iteration 511: Training Loss = 0.20060878402953009 Training Accuracy = 0.9859349727630615\n","Iteration 512: Training Loss = 0.2022716813493176 Training Accuracy = 0.9858083128929138\n","Iteration 513: Training Loss = 0.20326942205548879 Training Accuracy = 0.9856316447257996\n","Iteration 514: Training Loss = 0.20198120035086287 Training Accuracy = 0.9858566522598267\n","Iteration 515: Training Loss = 0.20149032877963666 Training Accuracy = 0.9858099818229675\n","Iteration 516: Training Loss = 0.1996340105525356 Training Accuracy = 0.9860583543777466\n","Iteration 517: Training Loss = 0.19801055274271853 Training Accuracy = 0.9862366914749146\n","Iteration 518: Training Loss = 0.19685585047621793 Training Accuracy = 0.9863366484642029\n","Iteration 519: Training Loss = 0.19692557963459748 Training Accuracy = 0.9862233400344849\n","Iteration 520: Training Loss = 0.1980659090879908 Training Accuracy = 0.9861950278282166\n","Iteration 521: Training Loss = 0.19870567465618702 Training Accuracy = 0.9861183166503906\n","Iteration 522: Training Loss = 0.19866450368598987 Training Accuracy = 0.9860600233078003\n","Iteration 523: Training Loss = 0.1973657219515867 Training Accuracy = 0.9862316846847534\n","Iteration 524: Training Loss = 0.1964905540589144 Training Accuracy = 0.9862433075904846\n","Iteration 525: Training Loss = 0.19589999858902998 Training Accuracy = 0.9863466620445251\n","Iteration 526: Training Loss = 0.19547377435456928 Training Accuracy = 0.9864233136177063\n","Iteration 527: Training Loss = 0.19500785156009673 Training Accuracy = 0.9864183068275452\n","Iteration 528: Training Loss = 0.19471469961187426 Training Accuracy = 0.986478328704834\n","Iteration 529: Training Loss = 0.19476345405351497 Training Accuracy = 0.9863883256912231\n","Iteration 530: Training Loss = 0.19490045254132635 Training Accuracy = 0.9864749908447266\n","Iteration 531: Training Loss = 0.1949482459414435 Training Accuracy = 0.9863516688346863\n","Iteration 532: Training Loss = 0.19468919692877232 Training Accuracy = 0.9864466786384583\n","Iteration 533: Training Loss = 0.19447275764707725 Training Accuracy = 0.9864400029182434\n","Iteration 534: Training Loss = 0.1943575250400877 Training Accuracy = 0.9864299893379211\n","Iteration 535: Training Loss = 0.1943039816970441 Training Accuracy = 0.9864766597747803\n","Iteration 536: Training Loss = 0.19424716869563752 Training Accuracy = 0.9864566922187805\n","Iteration 537: Training Loss = 0.19389999410389347 Training Accuracy = 0.9865183234214783\n","Iteration 538: Training Loss = 0.19363411879215017 Training Accuracy = 0.9865099787712097\n","Iteration 539: Training Loss = 0.19328172454640608 Training Accuracy = 0.986531674861908\n","Iteration 540: Training Loss = 0.19308825640916413 Training Accuracy = 0.9865400195121765\n","Iteration 541: Training Loss = 0.19279717944434996 Training Accuracy = 0.9865450263023376\n","Iteration 542: Training Loss = 0.19251552412164746 Training Accuracy = 0.9865850210189819\n","Iteration 543: Training Loss = 0.192134608278566 Training Accuracy = 0.986591637134552\n","Iteration 544: Training Loss = 0.19184646422307391 Training Accuracy = 0.9866483211517334\n","Iteration 545: Training Loss = 0.1916844611671143 Training Accuracy = 0.9866533279418945\n","Iteration 546: Training Loss = 0.19164751320909906 Training Accuracy = 0.9867050051689148\n","Iteration 547: Training Loss = 0.1918376890491318 Training Accuracy = 0.9866383075714111\n","Iteration 548: Training Loss = 0.19201997883527316 Training Accuracy = 0.9866600036621094\n","Iteration 549: Training Loss = 0.19275188157734385 Training Accuracy = 0.9865700006484985\n","Iteration 550: Training Loss = 0.19325924144162018 Training Accuracy = 0.9865483045578003\n","Iteration 551: Training Loss = 0.1951898563506829 Training Accuracy = 0.9861533045768738\n","Iteration 552: Training Loss = 0.19567943505500704 Training Accuracy = 0.986240029335022\n","Iteration 553: Training Loss = 0.19821453178319892 Training Accuracy = 0.9859516620635986\n","Iteration 554: Training Loss = 0.1964344994251111 Training Accuracy = 0.9861616492271423\n","Iteration 555: Training Loss = 0.19620022786326327 Training Accuracy = 0.9860799908638\n","Iteration 556: Training Loss = 0.1923101754427597 Training Accuracy = 0.9866066575050354\n","Iteration 557: Training Loss = 0.189866059160133 Training Accuracy = 0.9866750240325928\n","Iteration 558: Training Loss = 0.18823167598347113 Training Accuracy = 0.987018346786499\n","Iteration 559: Training Loss = 0.1881267924327855 Training Accuracy = 0.986976683139801\n","Iteration 560: Training Loss = 0.18910775290531784 Training Accuracy = 0.986811637878418\n","Iteration 561: Training Loss = 0.19042632195226536 Training Accuracy = 0.9867733120918274\n","Iteration 562: Training Loss = 0.1920599373293075 Training Accuracy = 0.9865033626556396\n","Iteration 563: Training Loss = 0.1923996627542853 Training Accuracy = 0.9865649938583374\n","Iteration 564: Training Loss = 0.1927218856675615 Training Accuracy = 0.986365020275116\n","Iteration 565: Training Loss = 0.1918379273646122 Training Accuracy = 0.9866099953651428\n","Iteration 566: Training Loss = 0.19050553814531798 Training Accuracy = 0.9867616891860962\n","Iteration 567: Training Loss = 0.1901503512770648 Training Accuracy = 0.986686646938324\n","Iteration 568: Training Loss = 0.19015460156330488 Training Accuracy = 0.9868249893188477\n","Iteration 569: Training Loss = 0.19180280425154633 Training Accuracy = 0.9864733219146729\n","Iteration 570: Training Loss = 0.19377621449463361 Training Accuracy = 0.9864450097084045\n","Iteration 571: Training Loss = 0.19606714167805703 Training Accuracy = 0.986061692237854\n","Iteration 572: Training Loss = 0.1943100888055295 Training Accuracy = 0.9864616394042969\n","Iteration 573: Training Loss = 0.19169653679322884 Training Accuracy = 0.986466646194458\n","Iteration 574: Training Loss = 0.1869895354093884 Training Accuracy = 0.9870650172233582\n","Iteration 575: Training Loss = 0.1847913791645729 Training Accuracy = 0.9872333407402039\n","Iteration 576: Training Loss = 0.18556014855637878 Training Accuracy = 0.9870666861534119\n","Iteration 577: Training Loss = 0.18770378858997258 Training Accuracy = 0.9870083332061768\n","Iteration 578: Training Loss = 0.18932505461287347 Training Accuracy = 0.986573338508606\n","Iteration 579: Training Loss = 0.1883511450982952 Training Accuracy = 0.9869049787521362\n","Iteration 580: Training Loss = 0.1862111562991895 Training Accuracy = 0.9868616461753845\n","Iteration 581: Training Loss = 0.1840464084094802 Training Accuracy = 0.9872699975967407\n","Iteration 582: Training Loss = 0.18341230895227167 Training Accuracy = 0.9873616695404053\n","Iteration 583: Training Loss = 0.184212478289724 Training Accuracy = 0.987071692943573\n","Iteration 584: Training Loss = 0.18512398460823734 Training Accuracy = 0.9871766567230225\n","Iteration 585: Training Loss = 0.18555031282793125 Training Accuracy = 0.9869166612625122\n","Iteration 586: Training Loss = 0.1846294576422046 Training Accuracy = 0.9872599840164185\n","Iteration 587: Training Loss = 0.18342525416293612 Training Accuracy = 0.9871233105659485\n","Iteration 588: Training Loss = 0.18237710328565215 Training Accuracy = 0.9874500036239624\n","Iteration 589: Training Loss = 0.18200162130424705 Training Accuracy = 0.9874500036239624\n","Iteration 590: Training Loss = 0.1821903985315223 Training Accuracy = 0.9873366951942444\n","Iteration 591: Training Loss = 0.18251671507366332 Training Accuracy = 0.987416684627533\n","Iteration 592: Training Loss = 0.18274113389095922 Training Accuracy = 0.9871583580970764\n","Iteration 593: Training Loss = 0.18246940863182726 Training Accuracy = 0.9873999953269958\n","Iteration 594: Training Loss = 0.18203221743137352 Training Accuracy = 0.9872599840164185\n","Iteration 595: Training Loss = 0.18137542790266192 Training Accuracy = 0.9874950051307678\n","Iteration 596: Training Loss = 0.18083255112381114 Training Accuracy = 0.9874749779701233\n","Iteration 597: Training Loss = 0.18047951445121552 Training Accuracy = 0.9875249862670898\n","Iteration 598: Training Loss = 0.18032944723549169 Training Accuracy = 0.9876049757003784\n","Iteration 599: Training Loss = 0.18031706876368284 Training Accuracy = 0.987500011920929\n","Iteration 600: Training Loss = 0.18042777853098346 Training Accuracy = 0.9875966906547546\n","Iteration 601: Training Loss = 0.18070106655631799 Training Accuracy = 0.9874633550643921\n","Iteration 602: Training Loss = 0.18111595202975944 Training Accuracy = 0.9875083565711975\n","Iteration 603: Training Loss = 0.1821581145703772 Training Accuracy = 0.9873483180999756\n","Iteration 604: Training Loss = 0.18343986545477736 Training Accuracy = 0.9872316718101501\n","Iteration 605: Training Loss = 0.18698307608690218 Training Accuracy = 0.9868483543395996\n","Iteration 606: Training Loss = 0.1900324858527699 Training Accuracy = 0.9865566492080688\n","Iteration 607: Training Loss = 0.1968732925832026 Training Accuracy = 0.9859750270843506\n","Iteration 608: Training Loss = 0.19817439536829237 Training Accuracy = 0.9857533574104309\n","Iteration 609: Training Loss = 0.19498185522892303 Training Accuracy = 0.9860983490943909\n","Iteration 610: Training Loss = 0.18869403249150024 Training Accuracy = 0.9865249991416931\n","Iteration 611: Training Loss = 0.18286535339233356 Training Accuracy = 0.9871450066566467\n","Iteration 612: Training Loss = 0.1833374632420042 Training Accuracy = 0.9871799945831299\n","Iteration 613: Training Loss = 0.18438937067246358 Training Accuracy = 0.9870799779891968\n","Iteration 614: Training Loss = 0.184776845819926 Training Accuracy = 0.9871050119400024\n","Iteration 615: Training Loss = 0.18248914837887265 Training Accuracy = 0.9871633052825928\n","Iteration 616: Training Loss = 0.18123936959116926 Training Accuracy = 0.9873766899108887\n","Iteration 617: Training Loss = 0.1818032013372787 Training Accuracy = 0.9871766567230225\n","Iteration 618: Training Loss = 0.18141181119657232 Training Accuracy = 0.9873650074005127\n","Iteration 619: Training Loss = 0.18024181955620716 Training Accuracy = 0.9874399900436401\n","Iteration 620: Training Loss = 0.17858322987649444 Training Accuracy = 0.987613320350647\n","Iteration 621: Training Loss = 0.1784183618920127 Training Accuracy = 0.9876466393470764\n","Iteration 622: Training Loss = 0.1790147811604949 Training Accuracy = 0.987405002117157\n","Iteration 623: Training Loss = 0.17892207349744785 Training Accuracy = 0.9875649809837341\n","Iteration 624: Training Loss = 0.17813356619964155 Training Accuracy = 0.9875916838645935\n","Iteration 625: Training Loss = 0.17705912038656327 Training Accuracy = 0.9877983331680298\n","Iteration 626: Training Loss = 0.17623353886400073 Training Accuracy = 0.9877983331680298\n","Iteration 627: Training Loss = 0.17582807457164296 Training Accuracy = 0.9877883195877075\n","Iteration 628: Training Loss = 0.17583015759982998 Training Accuracy = 0.9878366589546204\n","Iteration 629: Training Loss = 0.17619729940186218 Training Accuracy = 0.9877316951751709\n","Iteration 630: Training Loss = 0.1763384964646876 Training Accuracy = 0.9878583550453186\n","Iteration 631: Training Loss = 0.17590910575598864 Training Accuracy = 0.9877283573150635\n","Iteration 632: Training Loss = 0.17489008285766783 Training Accuracy = 0.9879816770553589\n","Iteration 633: Training Loss = 0.17410838010256188 Training Accuracy = 0.9880216717720032\n","Iteration 634: Training Loss = 0.1740295844816019 Training Accuracy = 0.9880116581916809\n","Iteration 635: Training Loss = 0.17429788592898104 Training Accuracy = 0.9879950284957886\n","Iteration 636: Training Loss = 0.17423547172239526 Training Accuracy = 0.9879283308982849\n","Iteration 637: Training Loss = 0.17357086879558398 Training Accuracy = 0.9880466461181641\n","Iteration 638: Training Loss = 0.17285262074398974 Training Accuracy = 0.9880899786949158\n","Iteration 639: Training Loss = 0.1725907056764315 Training Accuracy = 0.9881150126457214\n","Iteration 640: Training Loss = 0.17274392213224382 Training Accuracy = 0.9881483316421509\n","Iteration 641: Training Loss = 0.17290154503320734 Training Accuracy = 0.9880533218383789\n","Iteration 642: Training Loss = 0.1727928862884516 Training Accuracy = 0.9880866408348083\n","Iteration 643: Training Loss = 0.1725843713063674 Training Accuracy = 0.9880949854850769\n","Iteration 644: Training Loss = 0.17243743953913745 Training Accuracy = 0.9881150126457214\n","Iteration 645: Training Loss = 0.17243773811423757 Training Accuracy = 0.9881183505058289\n","Iteration 646: Training Loss = 0.17247402506171636 Training Accuracy = 0.9880733489990234\n","Iteration 647: Training Loss = 0.1723980500538461 Training Accuracy = 0.9880783557891846\n","Iteration 648: Training Loss = 0.1726741048277285 Training Accuracy = 0.9879733324050903\n","Iteration 649: Training Loss = 0.17293234858398351 Training Accuracy = 0.9880466461181641\n","Iteration 650: Training Loss = 0.17414516400611083 Training Accuracy = 0.9877750277519226\n","Iteration 651: Training Loss = 0.17484745589864167 Training Accuracy = 0.9878183603286743\n","Iteration 652: Training Loss = 0.1768876199058041 Training Accuracy = 0.9874666929244995\n","Iteration 653: Training Loss = 0.17673849359457267 Training Accuracy = 0.9875733256340027\n","Iteration 654: Training Loss = 0.17852672945199133 Training Accuracy = 0.9873499870300293\n","Iteration 655: Training Loss = 0.17599887846780676 Training Accuracy = 0.9876266717910767\n","Iteration 656: Training Loss = 0.17528833721855996 Training Accuracy = 0.9876499772071838\n","Iteration 657: Training Loss = 0.17214698573743778 Training Accuracy = 0.9880666732788086\n","Iteration 658: Training Loss = 0.17030538046538138 Training Accuracy = 0.9882299900054932\n","Iteration 659: Training Loss = 0.16944067409531305 Training Accuracy = 0.9883099794387817\n","Iteration 660: Training Loss = 0.16965097307482593 Training Accuracy = 0.9883933067321777\n","Iteration 661: Training Loss = 0.17050367887605847 Training Accuracy = 0.988099992275238\n","Iteration 662: Training Loss = 0.17106025020117607 Training Accuracy = 0.9882216453552246\n","Iteration 663: Training Loss = 0.17168358160124766 Training Accuracy = 0.9879216551780701\n","Iteration 664: Training Loss = 0.17100683876881478 Training Accuracy = 0.98819500207901\n","Iteration 665: Training Loss = 0.17095937994464971 Training Accuracy = 0.9880549907684326\n","Iteration 666: Training Loss = 0.17020664283811426 Training Accuracy = 0.9881716370582581\n","Iteration 667: Training Loss = 0.17007769160979389 Training Accuracy = 0.9882233142852783\n","Iteration 668: Training Loss = 0.17011562686579357 Training Accuracy = 0.988236665725708\n","Iteration 669: Training Loss = 0.17017077010477535 Training Accuracy = 0.9882749915122986\n","Iteration 670: Training Loss = 0.17085695668313514 Training Accuracy = 0.988183319568634\n","Iteration 671: Training Loss = 0.17075672774059147 Training Accuracy = 0.9881733059883118\n","Iteration 672: Training Loss = 0.1709964569706169 Training Accuracy = 0.9881783127784729\n","Iteration 673: Training Loss = 0.16973817064056698 Training Accuracy = 0.9882716536521912\n","Iteration 674: Training Loss = 0.16846380773699474 Training Accuracy = 0.9883716702461243\n","Iteration 675: Training Loss = 0.16682467006507584 Training Accuracy = 0.9885616898536682\n","Iteration 676: Training Loss = 0.16585774212896 Training Accuracy = 0.9885849952697754\n","Iteration 677: Training Loss = 0.16570418192542238 Training Accuracy = 0.9886149764060974\n","Iteration 678: Training Loss = 0.16614550451017687 Training Accuracy = 0.9886266589164734\n","Iteration 679: Training Loss = 0.16684847203937517 Training Accuracy = 0.9884899854660034\n","Iteration 680: Training Loss = 0.1672636398124477 Training Accuracy = 0.9884516596794128\n","Iteration 681: Training Loss = 0.16766279994543354 Training Accuracy = 0.9884300231933594\n","Iteration 682: Training Loss = 0.16737127106621696 Training Accuracy = 0.9884666800498962\n","Iteration 683: Training Loss = 0.16725863114193917 Training Accuracy = 0.9885016679763794\n","Iteration 684: Training Loss = 0.16672187148152307 Training Accuracy = 0.9885266423225403\n","Iteration 685: Training Loss = 0.16662662832027048 Training Accuracy = 0.9885133504867554\n","Iteration 686: Training Loss = 0.1668535056572564 Training Accuracy = 0.9884099960327148\n","Iteration 687: Training Loss = 0.16756257673731137 Training Accuracy = 0.9883366823196411\n","Iteration 688: Training Loss = 0.1698265340229345 Training Accuracy = 0.9879916906356812\n","Iteration 689: Training Loss = 0.17116582770307273 Training Accuracy = 0.9879066944122314\n","Iteration 690: Training Loss = 0.1759109778576445 Training Accuracy = 0.9874200224876404\n","Iteration 691: Training Loss = 0.17410044225323879 Training Accuracy = 0.9876099824905396\n","Iteration 692: Training Loss = 0.17556653475746375 Training Accuracy = 0.9875016808509827\n","Iteration 693: Training Loss = 0.16898575138519897 Training Accuracy = 0.9881449937820435\n","Iteration 694: Training Loss = 0.16505127048320106 Training Accuracy = 0.9885083436965942\n","Iteration 695: Training Loss = 0.16270765394064643 Training Accuracy = 0.9888783097267151\n","Iteration 696: Training Loss = 0.16334959000761035 Training Accuracy = 0.9888383150100708\n","Iteration 697: Training Loss = 0.1658223926265208 Training Accuracy = 0.9883933067321777\n","Iteration 698: Training Loss = 0.167019337568111 Training Accuracy = 0.9883666634559631\n","Iteration 699: Training Loss = 0.16822828318995958 Training Accuracy = 0.9881399869918823\n","Iteration 700: Training Loss = 0.16580301460606384 Training Accuracy = 0.9884716868400574\n","Iteration 701: Training Loss = 0.16423708686820343 Training Accuracy = 0.9885749816894531\n","Iteration 702: Training Loss = 0.16306375208743806 Training Accuracy = 0.988818347454071\n","Iteration 703: Training Loss = 0.16318267851116725 Training Accuracy = 0.9887599945068359\n","Iteration 704: Training Loss = 0.16426853937144165 Training Accuracy = 0.9886149764060974\n","Iteration 705: Training Loss = 0.16505886837927397 Training Accuracy = 0.988610029220581\n","Iteration 706: Training Loss = 0.1653437102166458 Training Accuracy = 0.9884183406829834\n","Iteration 707: Training Loss = 0.16433088879607763 Training Accuracy = 0.9887183308601379\n","Iteration 708: Training Loss = 0.16293523229984827 Training Accuracy = 0.9887216687202454\n","Iteration 709: Training Loss = 0.1614473519757273 Training Accuracy = 0.9889400005340576\n","Iteration 710: Training Loss = 0.16081579382813085 Training Accuracy = 0.9890316724777222\n","Iteration 711: Training Loss = 0.1609959648015341 Training Accuracy = 0.9888566732406616\n","Iteration 712: Training Loss = 0.16136358102523135 Training Accuracy = 0.9889566898345947\n","Iteration 713: Training Loss = 0.16188037959903984 Training Accuracy = 0.9887466430664062\n","Iteration 714: Training Loss = 0.161741568351084 Training Accuracy = 0.9888866543769836\n","Iteration 715: Training Loss = 0.16135294340423026 Training Accuracy = 0.9887750148773193\n","Iteration 716: Training Loss = 0.16053275440171008 Training Accuracy = 0.9890750050544739\n","Iteration 717: Training Loss = 0.15976468818372347 Training Accuracy = 0.9889983534812927\n","Iteration 718: Training Loss = 0.1591875092303473 Training Accuracy = 0.989133358001709\n","Iteration 719: Training Loss = 0.15899162386825258 Training Accuracy = 0.9891650080680847\n","Iteration 720: Training Loss = 0.15911960941756945 Training Accuracy = 0.9891583323478699\n","Iteration 721: Training Loss = 0.15947194335394052 Training Accuracy = 0.989068329334259\n","Iteration 722: Training Loss = 0.16004645039127213 Training Accuracy = 0.9890116453170776\n","Iteration 723: Training Loss = 0.1604894126280163 Training Accuracy = 0.9889749884605408\n","Iteration 724: Training Loss = 0.1614846817484356 Training Accuracy = 0.9889100193977356\n","Iteration 725: Training Loss = 0.16217675425267467 Training Accuracy = 0.9888049960136414\n","Iteration 726: Training Loss = 0.16464188646142325 Training Accuracy = 0.9885183572769165\n","Iteration 727: Training Loss = 0.16675757468365981 Training Accuracy = 0.9883033037185669\n","Iteration 728: Training Loss = 0.17066499304458976 Training Accuracy = 0.9879183173179626\n","Iteration 729: Training Loss = 0.1733244553945286 Training Accuracy = 0.9875933527946472\n","Iteration 730: Training Loss = 0.17140959569041433 Training Accuracy = 0.9877183437347412\n","Iteration 731: Training Loss = 0.16762937149912172 Training Accuracy = 0.9880116581916809\n","Iteration 732: Training Loss = 0.16092047465326698 Training Accuracy = 0.9888283610343933\n","Iteration 733: Training Loss = 0.1581752101350116 Training Accuracy = 0.989038348197937\n","Iteration 734: Training Loss = 0.15917993401929814 Training Accuracy = 0.9890183210372925\n","Iteration 735: Training Loss = 0.16242414269841357 Training Accuracy = 0.988664984703064\n","Iteration 736: Training Loss = 0.16419992906160394 Training Accuracy = 0.9884483218193054\n","Iteration 737: Training Loss = 0.1633903145574177 Training Accuracy = 0.9885433316230774\n","Iteration 738: Training Loss = 0.16123626032808686 Training Accuracy = 0.9887116551399231\n","Iteration 739: Training Loss = 0.15795800905060853 Training Accuracy = 0.9891683459281921\n","Iteration 740: Training Loss = 0.15673527516235458 Training Accuracy = 0.9891833066940308\n","Iteration 741: Training Loss = 0.15697366652979186 Training Accuracy = 0.989246666431427\n","Iteration 742: Training Loss = 0.15828366818405862 Training Accuracy = 0.9891066551208496\n","Iteration 743: Training Loss = 0.15903853661200557 Training Accuracy = 0.988955020904541\n","Iteration 744: Training Loss = 0.15870737634381624 Training Accuracy = 0.9890166521072388\n","Iteration 745: Training Loss = 0.15758267085262748 Training Accuracy = 0.9891266822814941\n","Iteration 746: Training Loss = 0.1558556989791577 Training Accuracy = 0.9893233180046082\n","Iteration 747: Training Loss = 0.15515889219419274 Training Accuracy = 0.9892966747283936\n","Iteration 748: Training Loss = 0.15522097045619845 Training Accuracy = 0.9893816709518433\n","Iteration 749: Training Loss = 0.1555374293074769 Training Accuracy = 0.9894350171089172\n","Iteration 750: Training Loss = 0.15563636367168104 Training Accuracy = 0.9892849922180176\n","Iteration 751: Training Loss = 0.15562714658883356 Training Accuracy = 0.9893916845321655\n","Iteration 752: Training Loss = 0.15527642147919887 Training Accuracy = 0.9893400073051453\n","Iteration 753: Training Loss = 0.15495927478844362 Training Accuracy = 0.9893916845321655\n","Iteration 754: Training Loss = 0.15440092578599712 Training Accuracy = 0.9894499778747559\n","Iteration 755: Training Loss = 0.15371877836053652 Training Accuracy = 0.9895049929618835\n","Iteration 756: Training Loss = 0.1532390231899345 Training Accuracy = 0.9895049929618835\n","Iteration 757: Training Loss = 0.15306021382419516 Training Accuracy = 0.9896066784858704\n","Iteration 758: Training Loss = 0.1531510043028257 Training Accuracy = 0.9895866513252258\n","Iteration 759: Training Loss = 0.15325869810809695 Training Accuracy = 0.9895116686820984\n","Iteration 760: Training Loss = 0.15347073382278395 Training Accuracy = 0.9895133376121521\n","Iteration 761: Training Loss = 0.15349926120661786 Training Accuracy = 0.9895349740982056\n","Iteration 762: Training Loss = 0.15365922596797266 Training Accuracy = 0.9895049929618835\n","Iteration 763: Training Loss = 0.1533832072750381 Training Accuracy = 0.9895016551017761\n","Iteration 764: Training Loss = 0.15312224974956457 Training Accuracy = 0.9895349740982056\n","Iteration 765: Training Loss = 0.1525482464046959 Training Accuracy = 0.989579975605011\n","Iteration 766: Training Loss = 0.15213325860213614 Training Accuracy = 0.9897000193595886\n","Iteration 767: Training Loss = 0.15171823048295277 Training Accuracy = 0.9896583557128906\n","Iteration 768: Training Loss = 0.15140479576128815 Training Accuracy = 0.9897333383560181\n","Iteration 769: Training Loss = 0.15112115003712928 Training Accuracy = 0.9897516369819641\n","Iteration 770: Training Loss = 0.1508898779961891 Training Accuracy = 0.9897383451461792\n","Iteration 771: Training Loss = 0.15072230354982039 Training Accuracy = 0.9897983074188232\n","Iteration 772: Training Loss = 0.15061415916115148 Training Accuracy = 0.9897333383560181\n","Iteration 773: Training Loss = 0.15055871499737303 Training Accuracy = 0.9897599816322327\n","Iteration 774: Training Loss = 0.15063165384413418 Training Accuracy = 0.9897366762161255\n","Iteration 775: Training Loss = 0.15088034192727215 Training Accuracy = 0.9897116422653198\n","Iteration 776: Training Loss = 0.15173704838747437 Training Accuracy = 0.9895733594894409\n","Iteration 777: Training Loss = 0.15295309471827656 Training Accuracy = 0.9894400238990784\n","Iteration 778: Training Loss = 0.15624709091171032 Training Accuracy = 0.988943338394165\n","Iteration 779: Training Loss = 0.15892569159363404 Training Accuracy = 0.9887300133705139\n","Iteration 780: Training Loss = 0.16696556625957745 Training Accuracy = 0.9879699945449829\n","Iteration 781: Training Loss = 0.16631898723796276 Training Accuracy = 0.9880783557891846\n","Iteration 782: Training Loss = 0.17165402417522121 Training Accuracy = 0.9875900149345398\n","Iteration 783: Training Loss = 0.16152518794131543 Training Accuracy = 0.9885033369064331\n","Iteration 784: Training Loss = 0.15577185262165114 Training Accuracy = 0.9889666438102722\n","Iteration 785: Training Loss = 0.15321773010429582 Training Accuracy = 0.9894800186157227\n","Iteration 786: Training Loss = 0.15735278015380133 Training Accuracy = 0.9889666438102722\n","Iteration 787: Training Loss = 0.16241696667410296 Training Accuracy = 0.988426685333252\n","Iteration 788: Training Loss = 0.161765441754883 Training Accuracy = 0.9884733557701111\n","Iteration 789: Training Loss = 0.1570411939607074 Training Accuracy = 0.988871693611145\n","Iteration 790: Training Loss = 0.14973015352269983 Training Accuracy = 0.9897516369819641\n","Iteration 791: Training Loss = 0.14822404849156295 Training Accuracy = 0.9899783134460449\n","Iteration 792: Training Loss = 0.15188416908979424 Training Accuracy = 0.9894616603851318\n","Iteration 793: Training Loss = 0.15501080347079263 Training Accuracy = 0.9891250133514404\n","Iteration 794: Training Loss = 0.15500293921755323 Training Accuracy = 0.989121675491333\n","Iteration 795: Training Loss = 0.15106340623762785 Training Accuracy = 0.9895883202552795\n","Iteration 796: Training Loss = 0.1481610846237413 Training Accuracy = 0.9899100065231323\n","Iteration 797: Training Loss = 0.14850256432933015 Training Accuracy = 0.9898383617401123\n","Iteration 798: Training Loss = 0.1502546281119935 Training Accuracy = 0.9896866679191589\n","Iteration 799: Training Loss = 0.15158886843560396 Training Accuracy = 0.9894350171089172\n","Iteration 800: Training Loss = 0.1495628401715757 Training Accuracy = 0.9897400140762329\n","Iteration 801: Training Loss = 0.14754515682416722 Training Accuracy = 0.9899666905403137\n","Iteration 802: Training Loss = 0.1466036559043736 Training Accuracy = 0.9901216626167297\n","Iteration 803: Training Loss = 0.1473622699436902 Training Accuracy = 0.9900266528129578\n","Iteration 804: Training Loss = 0.14880589322533377 Training Accuracy = 0.9897016882896423\n","Iteration 805: Training Loss = 0.149144352915965 Training Accuracy = 0.9898166656494141\n","Iteration 806: Training Loss = 0.14851127940867237 Training Accuracy = 0.9897266626358032\n","Iteration 807: Training Loss = 0.14689524111981775 Training Accuracy = 0.9901149868965149\n","Iteration 808: Training Loss = 0.14584970977682118 Training Accuracy = 0.9901566505432129\n","Iteration 809: Training Loss = 0.14572319619980179 Training Accuracy = 0.990149974822998\n","Iteration 810: Training Loss = 0.14618901946186172 Training Accuracy = 0.9901216626167297\n","Iteration 811: Training Loss = 0.14656802381485928 Training Accuracy = 0.9900400042533875\n","Iteration 812: Training Loss = 0.14627294056678444 Training Accuracy = 0.9900333285331726\n","Iteration 813: Training Loss = 0.14557506868761325 Training Accuracy = 0.9901466369628906\n","Iteration 814: Training Loss = 0.1447001100063242 Training Accuracy = 0.9902716875076294\n","Iteration 815: Training Loss = 0.14419813606681792 Training Accuracy = 0.9903249740600586\n","Iteration 816: Training Loss = 0.14414093862939517 Training Accuracy = 0.9902616739273071\n","Iteration 817: Training Loss = 0.14436541068250486 Training Accuracy = 0.9903050065040588\n","Iteration 818: Training Loss = 0.1446039345244007 Training Accuracy = 0.9901800155639648\n","Iteration 819: Training Loss = 0.14457095218644941 Training Accuracy = 0.9902483224868774\n","Iteration 820: Training Loss = 0.14431853501548722 Training Accuracy = 0.9902200102806091\n","Iteration 821: Training Loss = 0.14384630697463768 Training Accuracy = 0.9903550148010254\n","Iteration 822: Training Loss = 0.14339270405008958 Training Accuracy = 0.9903333187103271\n","Iteration 823: Training Loss = 0.1430715663684322 Training Accuracy = 0.9904066920280457\n","Iteration 824: Training Loss = 0.14292041203513012 Training Accuracy = 0.990423321723938\n","Iteration 825: Training Loss = 0.14293277185295922 Training Accuracy = 0.9903383255004883\n","Iteration 826: Training Loss = 0.14306118641888824 Training Accuracy = 0.9904316663742065\n","Iteration 827: Training Loss = 0.14329217323447166 Training Accuracy = 0.9903033375740051\n","Iteration 828: Training Loss = 0.14364984613085768 Training Accuracy = 0.9903533458709717\n","Iteration 829: Training Loss = 0.1443266562827069 Training Accuracy = 0.9901700019836426\n","Iteration 830: Training Loss = 0.14559709471926682 Training Accuracy = 0.990143358707428\n","Iteration 831: Training Loss = 0.1484986587918313 Training Accuracy = 0.9897666573524475\n","Iteration 832: Training Loss = 0.15272573145112217 Training Accuracy = 0.9894766807556152\n","Iteration 833: Training Loss = 0.1616919432937745 Training Accuracy = 0.9886500239372253\n","Iteration 834: Training Loss = 0.16523096670629445 Training Accuracy = 0.9882383346557617\n","Iteration 835: Training Loss = 0.1700855999384245 Training Accuracy = 0.9879416823387146\n","Iteration 836: Training Loss = 0.15393621780927844 Training Accuracy = 0.9891983270645142\n","Iteration 837: Training Loss = 0.14356374879297049 Training Accuracy = 0.9903150200843811\n","Iteration 838: Training Loss = 0.14673467283710717 Training Accuracy = 0.9898899793624878\n","Iteration 839: Training Loss = 0.15341603825534167 Training Accuracy = 0.9895166754722595\n","Iteration 840: Training Loss = 0.15180833013831962 Training Accuracy = 0.9895150065422058\n","Iteration 841: Training Loss = 0.14267631140523104 Training Accuracy = 0.9903716444969177\n","Iteration 842: Training Loss = 0.14347611348054295 Training Accuracy = 0.9903266429901123\n","Iteration 843: Training Loss = 0.14976835626426813 Training Accuracy = 0.9896799921989441\n","Iteration 844: Training Loss = 0.14765496533722042 Training Accuracy = 0.99003666639328\n","Iteration 845: Training Loss = 0.14226754270584482 Training Accuracy = 0.9903616905212402\n","Iteration 846: Training Loss = 0.14131948412719436 Training Accuracy = 0.9904850125312805\n","Iteration 847: Training Loss = 0.14477341609804445 Training Accuracy = 0.9902549982070923\n","Iteration 848: Training Loss = 0.14605544591794015 Training Accuracy = 0.9900000095367432\n","Iteration 849: Training Loss = 0.1421063264826212 Training Accuracy = 0.9904016852378845\n","Iteration 850: Training Loss = 0.13996233423622928 Training Accuracy = 0.9906483292579651\n","Iteration 851: Training Loss = 0.14168754812313328 Training Accuracy = 0.9903683066368103\n","Iteration 852: Training Loss = 0.14295985017834312 Training Accuracy = 0.9903950095176697\n","Iteration 853: Training Loss = 0.14165426543764997 Training Accuracy = 0.990423321723938\n","Iteration 854: Training Loss = 0.13953114099642025 Training Accuracy = 0.9905716776847839\n","Iteration 855: Training Loss = 0.13974296119121807 Training Accuracy = 0.9906466603279114\n","Iteration 856: Training Loss = 0.14101419277179814 Training Accuracy = 0.9903883337974548\n","Iteration 857: Training Loss = 0.14069556037407402 Training Accuracy = 0.9905616641044617\n","Iteration 858: Training Loss = 0.13933991981254606 Training Accuracy = 0.9906483292579651\n","Iteration 859: Training Loss = 0.13865695496707114 Training Accuracy = 0.9907333254814148\n","Iteration 860: Training Loss = 0.13907923967070396 Training Accuracy = 0.9907266497612\n","Iteration 861: Training Loss = 0.13959966766010734 Training Accuracy = 0.9906566739082336\n","Iteration 862: Training Loss = 0.13921113117081652 Training Accuracy = 0.9906816482543945\n","Iteration 863: Training Loss = 0.1386287818394597 Training Accuracy = 0.9907649755477905\n","Iteration 864: Training Loss = 0.1382311640597753 Training Accuracy = 0.9907716512680054\n","Iteration 865: Training Loss = 0.13830343448390361 Training Accuracy = 0.9907900094985962\n","Iteration 866: Training Loss = 0.13871428996473242 Training Accuracy = 0.9906583428382874\n","Iteration 867: Training Loss = 0.13907137977266315 Training Accuracy = 0.9906833171844482\n","Iteration 868: Training Loss = 0.13962066267160128 Training Accuracy = 0.9905683398246765\n","Iteration 869: Training Loss = 0.14013507077096463 Training Accuracy = 0.9904400110244751\n","Iteration 870: Training Loss = 0.1421654628009015 Training Accuracy = 0.9902083277702332\n","Iteration 871: Training Loss = 0.14419061848724776 Training Accuracy = 0.9900000095367432\n","Iteration 872: Training Loss = 0.14929001971431008 Training Accuracy = 0.9894166588783264\n","Iteration 873: Training Loss = 0.15016377296502 Training Accuracy = 0.9893566370010376\n","Iteration 874: Training Loss = 0.15423001476454012 Training Accuracy = 0.9890716671943665\n","Iteration 875: Training Loss = 0.14718255715601783 Training Accuracy = 0.989591658115387\n","Iteration 876: Training Loss = 0.1438603684088102 Training Accuracy = 0.9899299740791321\n","Iteration 877: Training Loss = 0.1392558358358589 Training Accuracy = 0.9905250072479248\n","Iteration 878: Training Loss = 0.13843932259549244 Training Accuracy = 0.9906783103942871\n","Iteration 879: Training Loss = 0.140013445015996 Training Accuracy = 0.9905750155448914\n","Iteration 880: Training Loss = 0.14165847352721983 Training Accuracy = 0.9901999831199646\n","Iteration 881: Training Loss = 0.1428991207227574 Training Accuracy = 0.9900100231170654\n","Iteration 882: Training Loss = 0.14035589629633133 Training Accuracy = 0.9903666377067566\n","Iteration 883: Training Loss = 0.1390663924397746 Training Accuracy = 0.9904900193214417\n","Iteration 884: Training Loss = 0.1380534542493697 Training Accuracy = 0.9906799793243408\n","Iteration 885: Training Loss = 0.13820583146886808 Training Accuracy = 0.9906449913978577\n","Iteration 886: Training Loss = 0.1381566931470692 Training Accuracy = 0.9906949996948242\n","Iteration 887: Training Loss = 0.13753626063707106 Training Accuracy = 0.9907983541488647\n","Iteration 888: Training Loss = 0.13658213184966783 Training Accuracy = 0.9907816648483276\n","Iteration 889: Training Loss = 0.13595453802648372 Training Accuracy = 0.9908966422080994\n","Iteration 890: Training Loss = 0.13621461019328898 Training Accuracy = 0.9907799959182739\n","Iteration 891: Training Loss = 0.13665108588657363 Training Accuracy = 0.9907816648483276\n","Iteration 892: Training Loss = 0.13671192242597327 Training Accuracy = 0.9907833337783813\n","Iteration 893: Training Loss = 0.1363582582483685 Training Accuracy = 0.9908266663551331\n","Iteration 894: Training Loss = 0.13557537875167922 Training Accuracy = 0.9909150004386902\n","Iteration 895: Training Loss = 0.13525823786011068 Training Accuracy = 0.9909716844558716\n","Iteration 896: Training Loss = 0.1348537092312686 Training Accuracy = 0.9910333156585693\n","Iteration 897: Training Loss = 0.13444870666778763 Training Accuracy = 0.9909800291061401\n","Iteration 898: Training Loss = 0.13405577092204451 Training Accuracy = 0.9910566806793213\n","Iteration 899: Training Loss = 0.133859736366758 Training Accuracy = 0.9911016821861267\n","Iteration 900: Training Loss = 0.1339270211791576 Training Accuracy = 0.9910683035850525\n","Iteration 901: Training Loss = 0.13386628433620007 Training Accuracy = 0.9910899996757507\n","Iteration 902: Training Loss = 0.13355586489309168 Training Accuracy = 0.9911366701126099\n","Iteration 903: Training Loss = 0.13303287118342982 Training Accuracy = 0.9911516904830933\n","Iteration 904: Training Loss = 0.1327200308854454 Training Accuracy = 0.9911966919898987\n","Iteration 905: Training Loss = 0.13271745461456022 Training Accuracy = 0.9911933541297913\n","Iteration 906: Training Loss = 0.1329018171236033 Training Accuracy = 0.9911749958992004\n","Iteration 907: Training Loss = 0.13307464630445492 Training Accuracy = 0.9911350011825562\n","Iteration 908: Training Loss = 0.13308229339585895 Training Accuracy = 0.9910983443260193\n","Iteration 909: Training Loss = 0.13298537331866786 Training Accuracy = 0.991100013256073\n","Iteration 910: Training Loss = 0.13293233036374957 Training Accuracy = 0.9911133050918579\n","Iteration 911: Training Loss = 0.13334573710253356 Training Accuracy = 0.9910983443260193\n","Iteration 912: Training Loss = 0.13412515481417744 Training Accuracy = 0.9909600019454956\n","Iteration 913: Training Loss = 0.13599691347975415 Training Accuracy = 0.9907199740409851\n","Iteration 914: Training Loss = 0.13823653097048538 Training Accuracy = 0.9905433058738708\n","Iteration 915: Training Loss = 0.1422819840096813 Training Accuracy = 0.9899700284004211\n","Iteration 916: Training Loss = 0.14514815304078096 Training Accuracy = 0.9896766543388367\n","Iteration 917: Training Loss = 0.1493927843132151 Training Accuracy = 0.9892666935920715\n","Iteration 918: Training Loss = 0.1462594639643775 Training Accuracy = 0.9895316958427429\n","Iteration 919: Training Loss = 0.1424834131229723 Training Accuracy = 0.9899616837501526\n","Iteration 920: Training Loss = 0.13504770431676016 Training Accuracy = 0.990838348865509\n","Iteration 921: Training Loss = 0.13166789114637228 Training Accuracy = 0.9912183284759521\n","Iteration 922: Training Loss = 0.13260990617834842 Training Accuracy = 0.9911199808120728\n","Iteration 923: Training Loss = 0.1360763024852863 Training Accuracy = 0.9907366633415222\n","Iteration 924: Training Loss = 0.1388675652734389 Training Accuracy = 0.990363359451294\n","Iteration 925: Training Loss = 0.13830427568793288 Training Accuracy = 0.9904333353042603\n","Iteration 926: Training Loss = 0.13629750734066653 Training Accuracy = 0.9906333088874817\n","Iteration 927: Training Loss = 0.13247094934690654 Training Accuracy = 0.9911100268363953\n","Iteration 928: Training Loss = 0.13071500884940268 Training Accuracy = 0.991296648979187\n","Iteration 929: Training Loss = 0.1310020501748822 Training Accuracy = 0.9912466406822205\n","Iteration 930: Training Loss = 0.1323537188715513 Training Accuracy = 0.9910849928855896\n","Iteration 931: Training Loss = 0.133142117195248 Training Accuracy = 0.9910133481025696\n","Iteration 932: Training Loss = 0.13288590211281923 Training Accuracy = 0.990993320941925\n","Iteration 933: Training Loss = 0.13225119355481701 Training Accuracy = 0.9910966753959656\n","Iteration 934: Training Loss = 0.13102987931995036 Training Accuracy = 0.9911699891090393\n","Iteration 935: Training Loss = 0.13053569403424956 Training Accuracy = 0.9912866950035095\n","Iteration 936: Training Loss = 0.13008042492588262 Training Accuracy = 0.9913216829299927\n","Iteration 937: Training Loss = 0.1298493675475847 Training Accuracy = 0.9913600087165833\n","Iteration 938: Training Loss = 0.12976182513358833 Training Accuracy = 0.9913866519927979\n","Iteration 939: Training Loss = 0.12998659547644126 Training Accuracy = 0.9913449883460999\n","Iteration 940: Training Loss = 0.13008356202885465 Training Accuracy = 0.9913333058357239\n","Iteration 941: Training Loss = 0.13009898134691203 Training Accuracy = 0.9912949800491333\n","Iteration 942: Training Loss = 0.12976406864002438 Training Accuracy = 0.9913866519927979\n","Iteration 943: Training Loss = 0.1290733151003494 Training Accuracy = 0.9913933277130127\n","Iteration 944: Training Loss = 0.12855049803103807 Training Accuracy = 0.9915016889572144\n","Iteration 945: Training Loss = 0.12821414650316337 Training Accuracy = 0.9915249943733215\n","Iteration 946: Training Loss = 0.12821418021489678 Training Accuracy = 0.9915033578872681\n","Iteration 947: Training Loss = 0.1283387518685353 Training Accuracy = 0.9914666414260864\n","Iteration 948: Training Loss = 0.12848379983950753 Training Accuracy = 0.9914233088493347\n","Iteration 949: Training Loss = 0.12838933856737095 Training Accuracy = 0.9914583563804626\n","Iteration 950: Training Loss = 0.12835185739245503 Training Accuracy = 0.991433322429657\n","Iteration 951: Training Loss = 0.12811993617360762 Training Accuracy = 0.9915149807929993\n","Iteration 952: Training Loss = 0.1279993724406607 Training Accuracy = 0.9915333390235901\n","Iteration 953: Training Loss = 0.12772869317938818 Training Accuracy = 0.9915599822998047\n","Iteration 954: Training Loss = 0.12750087084560882 Training Accuracy = 0.9915300011634827\n","Iteration 955: Training Loss = 0.12715707375720922 Training Accuracy = 0.9916549921035767\n","Iteration 956: Training Loss = 0.1268376427121946 Training Accuracy = 0.9916099905967712\n","Iteration 957: Training Loss = 0.1265264256000326 Training Accuracy = 0.9916566610336304\n","Iteration 958: Training Loss = 0.12631167592461906 Training Accuracy = 0.9915783405303955\n","Iteration 959: Training Loss = 0.1261927075052998 Training Accuracy = 0.9917200207710266\n","Iteration 960: Training Loss = 0.12617326379360722 Training Accuracy = 0.9916566610336304\n","Iteration 961: Training Loss = 0.12628920860010973 Training Accuracy = 0.9916483163833618\n","Iteration 962: Training Loss = 0.1265169000400594 Training Accuracy = 0.9916300177574158\n","Iteration 963: Training Loss = 0.1270792224082597 Training Accuracy = 0.9915516376495361\n","Iteration 964: Training Loss = 0.12780556545208233 Training Accuracy = 0.9915233254432678\n","Iteration 965: Training Loss = 0.1295728189843698 Training Accuracy = 0.9912566542625427\n","Iteration 966: Training Loss = 0.13156373084203418 Training Accuracy = 0.9911383390426636\n","Iteration 967: Training Loss = 0.13652036731426737 Training Accuracy = 0.9904366731643677\n","Iteration 968: Training Loss = 0.13976613238212815 Training Accuracy = 0.9903616905212402\n","Iteration 969: Training Loss = 0.1480715577184341 Training Accuracy = 0.9893316626548767\n","Iteration 970: Training Loss = 0.14560641723695722 Training Accuracy = 0.9897616505622864\n","Iteration 971: Training Loss = 0.14467199237864187 Training Accuracy = 0.9896249771118164\n","Iteration 972: Training Loss = 0.132370624954352 Training Accuracy = 0.990993320941925\n","Iteration 973: Training Loss = 0.1253623355309719 Training Accuracy = 0.9916583299636841\n","Iteration 974: Training Loss = 0.12657513980126608 Training Accuracy = 0.9915316700935364\n","Iteration 975: Training Loss = 0.13184003114547285 Training Accuracy = 0.9910783171653748\n","Iteration 976: Training Loss = 0.13583356338973887 Training Accuracy = 0.9904749989509583\n","Iteration 977: Training Loss = 0.13123387318383156 Training Accuracy = 0.9911333322525024\n","Iteration 978: Training Loss = 0.12625862133446925 Training Accuracy = 0.9915533065795898\n","Iteration 979: Training Loss = 0.1245918914953879 Training Accuracy = 0.991736650466919\n","Iteration 980: Training Loss = 0.1272772264419323 Training Accuracy = 0.9915333390235901\n","Iteration 981: Training Loss = 0.1301350941638876 Training Accuracy = 0.9911133050918579\n","Iteration 982: Training Loss = 0.12896623303677332 Training Accuracy = 0.9913849830627441\n","Iteration 983: Training Loss = 0.1257948085356695 Training Accuracy = 0.9916166663169861\n","Iteration 984: Training Loss = 0.12356300620896503 Training Accuracy = 0.9918416738510132\n","Iteration 985: Training Loss = 0.1243170436440187 Training Accuracy = 0.9917916655540466\n","Iteration 986: Training Loss = 0.1264225730510989 Training Accuracy = 0.9915566444396973\n","Iteration 987: Training Loss = 0.12659012119608526 Training Accuracy = 0.9915616512298584\n","Iteration 988: Training Loss = 0.12525621067764792 Training Accuracy = 0.9916616678237915\n","Iteration 989: Training Loss = 0.12327328965106168 Training Accuracy = 0.9918649792671204\n","Iteration 990: Training Loss = 0.12274596024671988 Training Accuracy = 0.9919083118438721\n","Iteration 991: Training Loss = 0.12360156538582248 Training Accuracy = 0.9918416738510132\n","Iteration 992: Training Loss = 0.12452986976651524 Training Accuracy = 0.9917583465576172\n","Iteration 993: Training Loss = 0.12462850082221234 Training Accuracy = 0.9917299747467041\n","Iteration 994: Training Loss = 0.12373192402656905 Training Accuracy = 0.9918416738510132\n"],"name":"stdout"}]}]}